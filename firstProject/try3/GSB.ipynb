{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import th.optim as optim\n",
    "import th.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(y, x) -> th.Tensor:\n",
    "    grad = th.autograd.grad(\n",
    "        outputs=y,\n",
    "        inputs=x,\n",
    "        grad_outputs=th.ones_like(y),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptronClass(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "    MultiLayerPerceptron\n",
    "    \n",
    "    This class is a simple implementation of a multi-layer perceptron (MLP) using PyTorch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        name       = 'mlp',\n",
    "        x_dim      = 784,\n",
    "        h_dim_list = [256,256],\n",
    "        y_dim      = 10,\n",
    "        actv       = nn.ReLU(),\n",
    "        p_drop     = 0.2,\n",
    "        batch_norm = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MLP\n",
    "        \"\"\"\n",
    "        super(MultiLayerPerceptronClass,self).__init__()\n",
    "        self.name       = name\n",
    "        self.x_dim      = x_dim\n",
    "        self.h_dim_list = h_dim_list\n",
    "        self.y_dim      = y_dim\n",
    "        self.actv       = actv\n",
    "        self.p_drop     = p_drop\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        # Declare layers\n",
    "        self.layer_list = []\n",
    "        h_dim_prev = self.x_dim\n",
    "        for h_dim in self.h_dim_list:\n",
    "            # dense -> batchnorm -> actv -> dropout\n",
    "            self.layer_list.append(nn.Linear(h_dim_prev,h_dim))\n",
    "            if self.batch_norm: self.layer_list.append(nn.BatchNorm1d(num_features=h_dim))\n",
    "            self.layer_list.append(self.actv)\n",
    "            self.layer_list.append(nn.Dropout1d(p=self.p_drop))\n",
    "            h_dim_prev = h_dim\n",
    "        self.layer_list.append(nn.Linear(h_dim_prev,self.y_dim))\n",
    "        \n",
    "        # Define net\n",
    "        self.net = nn.Sequential()\n",
    "        self.layer_names = []\n",
    "        for l_idx,layer in enumerate(self.layer_list):\n",
    "            layer_name = \"%s_%02d\"%(type(layer).__name__.lower(),l_idx)\n",
    "            self.layer_names.append(layer_name)\n",
    "            self.net.add_module(layer_name,layer)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.init_param(VERBOSE=False)\n",
    "        \n",
    "    def init_param(self,VERBOSE=False):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        for m_idx,m in enumerate(self.modules()):\n",
    "            if VERBOSE:\n",
    "                print (\"[%02d]\"%(m_idx))\n",
    "            if isinstance(m,nn.BatchNorm1d): # init BN\n",
    "                nn.init.constant_(m.weight,1.0)\n",
    "                nn.init.constant_(m.bias,0.0)\n",
    "            elif isinstance(m,nn.Linear): # lnit dense\n",
    "                nn.init.kaiming_normal_(m.weight,nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        intermediate_output_list = []\n",
    "        for idx, layer in enumerate(self.net):\n",
    "            x = layer(x)\n",
    "            intermediate_output_list.append(x)\n",
    "        # Final output\n",
    "        final_output = x\n",
    "        return final_output,intermediate_output_list\n",
    "    \n",
    "def compute_pde_loss(model, inputs, nu):\n",
    "    inputs.requires_grad_(True)\n",
    "    u_v, p = model(inputs)\n",
    "    u = u_v[:, 0].unsqueeze(1)\n",
    "    v = u_v[:, 1].unsqueeze(1)\n",
    "    \n",
    "    # Compute gradients\n",
    "    ones = th.ones_like(u)\n",
    "    u_x = th.autograd.grad(u, inputs, grad_outputs=ones, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_y = th.autograd.grad(u, inputs, grad_outputs=ones, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    u_t = th.autograd.grad(u, inputs, grad_outputs=ones, create_graph=True)[0][:, 2].unsqueeze(1)\n",
    "    \n",
    "    v_x = th.autograd.grad(v, inputs, grad_outputs=ones, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_y = th.autograd.grad(v, inputs, grad_outputs=ones, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_t = th.autograd.grad(v, inputs, grad_outputs=ones, create_graph=True)[0][:, 2].unsqueeze(1)\n",
    "    \n",
    "    p_x = th.autograd.grad(p, inputs, grad_outputs=ones, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    p_y = th.autograd.grad(p, inputs, grad_outputs=ones, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    u_xx = th.autograd.grad(u_x, inputs, grad_outputs=ones, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    u_yy = th.autograd.grad(u_y, inputs, grad_outputs=ones, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    v_xx = th.autograd.grad(v_x, inputs, grad_outputs=ones, create_graph=True)[0][:, 0].unsqueeze(1)\n",
    "    v_yy = th.autograd.grad(v_y, inputs, grad_outputs=ones, create_graph=True)[0][:, 1].unsqueeze(1)\n",
    "    \n",
    "    # Navier-Stokes equations\n",
    "    continuity_eq = u_x + v_y\n",
    "    u_momentum_eq = u_t + u*u_x + v*u_y + p_x - nu*( + )\n",
    "    v_momentum_eq = v_t + u*v_x + v*v_y + p_y - nu*( + )\n",
    "    \n",
    "    # PDE loss\n",
    "    pde_loss = torch.mean(continuity_eq**2) + torch.mean(u_momentum_eq**2) + torch.mean(v_momentum_eq**2)\n",
    "    return pde_loss\n",
    "\n",
    "\n",
    "\n",
    "class GradientLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom layer to compute derivatives for the steady Navier-Stokes equation using PyTorch.\n",
    "    # model = Create_Model()\n",
    "    # gradient_layer = GradientLayer(model)\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super(GradientLayer, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Computing derivatives for the steady Navier-Stokes equation.\n",
    "        Args:\n",
    "            xy: input variable.\n",
    "        Returns:\n",
    "            psi: stream function.\n",
    "            p_grads: pressure and its gradients.\n",
    "            u_grads: u and its gradients.\n",
    "            v_grads: v and its gradients.\n",
    "        \"\"\"\n",
    "        x, y = xyt[..., 0], xyt[..., 1]\n",
    "        x.requires_grad_(True)\n",
    "        y.requires_grad_(True)\n",
    "        # Combine x and y and predict u, v, p\n",
    "        u_v_p, _ = self.model(th.stack([x, y], dim=-1))\n",
    "        u, v, p = u_v_p[..., 0], u_v_p[..., 1], u_v_p[..., 2]\n",
    "        \n",
    "        # First derivatives\n",
    "        u_x = calc_grad(u, x)\n",
    "        u_y = calc_grad(u, y)\n",
    "        v_x = calc_grad(v, x)\n",
    "        v_y = calc_grad(v, y)\n",
    "        p_x = calc_grad(p, x)\n",
    "        p_y = calc_grad(p, y)\n",
    "        # u_x = th.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "        # v_x = th.autograd.grad(v.sum(), x, create_graph=True)[0]\n",
    "        # u_y = th.autograd.grad(u.sum(), y, create_graph=True)[0]\n",
    "        # v_y = th.autograd.grad(v.sum(), y, create_graph=True)[0]\n",
    "        # p_x = th.autograd.grad(p.sum(), x, create_graph=True)[0]\n",
    "        # p_y = th.autograd.grad(p.sum(), y, create_graph=True)[0]\n",
    "\n",
    "        # Second derivatives\n",
    "        u_xx = calc_grad(u_x, x)\n",
    "        u_yy = calc_grad(u_y, y)\n",
    "        v_xx = calc_grad(v_x, x)\n",
    "        v_yy = calc_grad(v_y, y)\n",
    "        # u_xx = th.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
    "        # u_yy = th.autograd.grad(u_y.sum(), y, create_graph=True)[0]\n",
    "        # v_xx = th.autograd.grad(v_x.sum(), x, create_graph=True)[0]\n",
    "        # v_yy = th.autograd.grad(v_y.sum(), y, create_graph=True)[0]\n",
    "\n",
    "        p_grads = (p, p_x, p_y)\n",
    "        u_grads = (u, u_x, u_y, u_xx, u_yy)\n",
    "        v_grads = (v, v_x, v_y, v_xx, v_yy)\n",
    "\n",
    "        return p_grads, u_grads, v_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, network, rho=1.0, mu=0.01):\n",
    "        super(PINN, self).__init__()\n",
    "        self.network = network\n",
    "        self.rho = rho\n",
    "        self.mu = mu\n",
    "        self.grads = GradientLayer(self.network)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Unpack inputs\n",
    "        # Equation input: xy_eqn\n",
    "        # Boundary Condition: xy_in, xy_out, xy_w1, xy_w2, xy_circle\n",
    "        xy_eqn, xy_in, xy_out, xy_w1, xy_w2, xy_circle = inputs\n",
    "        \n",
    "        # compute gradients relative to equation\n",
    "        p_grads, u_grads, v_grads = self.grads(xy_eqn)\n",
    "        _, p_x, p_y = p_grads\n",
    "        u, u_x, u_y, u_xx, u_yy = u_grads\n",
    "        v, v_x, v_y, v_xx, v_yy = v_grads\n",
    "        \n",
    "        # compute equation loss\n",
    "        u_eqn =  u*u_x + v*u_y + p_x/self.rho - self.mu*(u_xx + u_yy) / self.rho\n",
    "        v_eqn =  u*v_x + v*v_y + p_y/self.rho - self.mu*(v_xx + v_yy) / self.rho\n",
    "        uv_eqn = u_x + v_y\n",
    "        \n",
    "        u_eqn = u_eqn.unsqueeze(-1)\n",
    "        v_eqn = v_eqn.unsqueeze(-1)\n",
    "        uv_eqn = uv_eqn.unsqueeze(-1)\n",
    "        uv_eqn = th.cat([u_eqn, v_eqn, uv_eqn], dim=1)\n",
    "        \n",
    "        # compute gradients relative to boundary condition\n",
    "        p_r, u_grads_r, v_grads_r = self.grads(xy_out)\n",
    "        uv_out = th.cat([p_r[0].unsqueeze(-1), p_r[0].unsqueeze(-1), p_r[0].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        p_l, u_grads_l, v_grads_l = self.grads(xy_w1)\n",
    "        uv_w1 = th.cat([u_grads_l[0].unsqueeze(-1), v_grads_l[0].unsqueeze(-1), p_l[2].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        p_l, u_grads_l, v_grads_l = self.grads(xy_w2)\n",
    "        uv_w2 = th.cat([u_grads_l[0].unsqueeze(-1), v_grads_l[0].unsqueeze(-1), p_l[2].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        p_l, u_grads_l, v_grads_l = self.grads(xy_circle)\n",
    "        uv_circle = th.cat([u_grads_l[0].unsqueeze(-1), v_grads_l[0].unsqueeze(-1), u_grads_l[0].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        p_inn, u_inn, v_inn = self.grads(xy_in)\n",
    "        uv_in = th.cat([u_inn[0].unsqueeze(-1), v_inn[0].unsqueeze(-1), u_inn[0].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        # build the PINN model for the steady Navier-Stokes equation\n",
    "        return uv_eqn, uv_in, uv_out, uv_w1, uv_w2, uv_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_cons(network, xy):\n",
    "    \"\"\"\n",
    "    Compute u_x and v_y in PyTorch.\n",
    "    Args:\n",
    "        network: PyTorch model for computing u, v, p from inputs.\n",
    "        xy: network input variables as tensor.\n",
    "    Returns:\n",
    "        (u_x, v_y) as tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    xy = th.tensor(xy, requires_grad=True)\n",
    "    x, y = xy[..., 0].unsqueeze(-1), xy[..., 1].unsqueeze(-1)\n",
    "\n",
    "    def compute_u_v(xy):\n",
    "        u_v_p = network(xy)\n",
    "        u = u_v_p[..., 0].unsqueeze(-1)\n",
    "        v = u_v_p[..., 1].unsqueeze(-1)\n",
    "        return u, v\n",
    "\n",
    "    # Calculate the jacobian with respect to inputs\n",
    "    u, v = compute_u_v(xy)\n",
    "    u_x = calc_grad(u, x)\n",
    "    v_y = calc_grad(v, y)\n",
    "\n",
    "    return u_x, v_y\n",
    "\n",
    "def u_0(xy):\n",
    "    \"\"\"\n",
    "    Initial wave form in PyTorch.\n",
    "    Args:\n",
    "        xy: variables (x, y) as torch.Tensor.\n",
    "    Returns:\n",
    "        u(x, y) as torch.Tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = xy[..., 0].unsqueeze(-1)\n",
    "    y = xy[..., 1].unsqueeze(-1)\n",
    "\n",
    "    return 4 * y * (1 - y)\n",
    "\n",
    "def contour(x, y, z, title, levels=100):\n",
    "    \"\"\"\n",
    "    Contour plot.\n",
    "    Args:\n",
    "        x: x-array.\n",
    "        y: y-array.\n",
    "        z: z-array.\n",
    "        title: title string.\n",
    "        levels: number of contour lines.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the value range\n",
    "    vmin = np.min(z)\n",
    "    vmax = np.max(z)\n",
    "\n",
    "    # plot a contour\n",
    "    font1 = {'family':'serif','size':20}\n",
    "    plt.contour(x, y, z, colors='k', linewidths=0.2, levels=levels)\n",
    "    contour_filled = plt.contourf(x, y, z, cmap='rainbow', levels=levels, norm=Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "    # Add the circle patch to the current axes without altering the axes limits\n",
    "    circle = plt.Circle((0, 0), 0.5, color='black')\n",
    "    plt.gca().add_patch(circle)\n",
    "\n",
    "    plt.title(title, fontdict=font1)\n",
    "    plt.xlabel(\"X\", fontdict=font1)\n",
    "    plt.ylabel(\"Y\", fontdict=font1)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contour_filled, pad=0.03, aspect=25, format='%.0e')\n",
    "    cbar.mappable.set_clim(vmin, vmax)\n",
    "    cbar.ax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_2332\\249722964.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  xyt_eqn[i, 0] = (x_f - x_ini) * np.random.rand(1, 1) + x_ini\n",
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_2332\\249722964.py:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  xyt_eqn[i, 1] = (y_f - y_ini) * np.random.rand(1, 1) + y_ini\n"
     ]
    }
   ],
   "source": [
    "# Task parameters\n",
    "u0 = 1 # inlet flow velocity\n",
    "rho = 1000 # density\n",
    "mu = 1 # viscosity\n",
    "\n",
    "# Samples\n",
    "num_train_samples = 10000 # number of training samples\n",
    "num_test_samples = 500 # number of test samples\n",
    "\n",
    "# Data boundary\n",
    "# Domain\n",
    "x_f =16\n",
    "x_ini=-4\n",
    "y_f= 4\n",
    "y_ini= -4\n",
    "# Time\n",
    "T = 20  # total time in seconds\n",
    "Delta_t = 0.1  # time step in seconds\n",
    "num_time_steps = int(T / Delta_t)  # number of time steps\n",
    "# Circle\n",
    "Cx = 0\n",
    "Cy = 0\n",
    "a = 0.5\n",
    "b = 0.5\n",
    "\n",
    "# create training input\n",
    "# Eqauation collocation points\n",
    "xyt_eqn = np.random.rand(num_train_samples, 2)\n",
    "xyt_eqn[...,0] = (x_f - x_ini)*xyt_eqn[...,0] + x_ini\n",
    "xyt_eqn[...,1] = (y_f - y_ini)*xyt_eqn[...,1] + y_ini\n",
    "\n",
    "for i in range(num_train_samples):\n",
    "    while (xyt_eqn[i, 0] - Cx)**2/a**2 + (xyt_eqn[i, 1] - Cy)**2/b**2 < 1:\n",
    "        xyt_eqn[i, 0] = (x_f - x_ini) * np.random.rand(1, 1) + x_ini\n",
    "        xyt_eqn[i, 1] = (y_f - y_ini) * np.random.rand(1, 1) + y_ini\n",
    "\n",
    "# Boundary collocation points\n",
    "# Circle\n",
    "xyt_circle = np.random.rand(num_train_samples, 2)\n",
    "xyt_circle[...,0] = 2*(a)*xyt_circle[...,0] +(Cx-a)\n",
    "xyt_circle[0:num_train_samples//2,1] = b*(1 - (xyt_circle[0:num_train_samples//2,0]-Cx)**2 / a**2)**0.5 + Cy\n",
    "xyt_circle[num_train_samples//2:,1] = -b*(1 - (xyt_circle[num_train_samples//2:,0]-Cx)**2 / a**2)**0.5 + Cy\n",
    "\n",
    "# Wall top and bottom\n",
    "xyt_w1 = np.random.rand(num_train_samples, 2)  # top-bottom boundaries\n",
    "xyt_w1[..., 0] = (x_f - x_ini)*xyt_w1[...,0] + x_ini\n",
    "xyt_w1[..., 1] =  y_ini          # y-position is 0 or 1\n",
    "\n",
    "xyt_w2 = np.random.rand(num_train_samples, 2)  # top-bottom boundaries\n",
    "xyt_w2[..., 0] = (x_f - x_ini)*xyt_w2[...,0] + x_ini\n",
    "xyt_w2[..., 1] =  y_f\n",
    "\n",
    "# Inlet and outlet\n",
    "xyt_in = np.random.rand(num_train_samples, 2)\n",
    "xyt_in[...,0] = x_ini\n",
    "\n",
    "xyt_out = np.random.rand(num_train_samples, 2)  # left-right boundaries\n",
    "xyt_out[..., 0] = x_f\n",
    "\n",
    "x_train = [xyt_eqn, xyt_in, xyt_out, xyt_w1, xyt_w2, xyt_circle]\n",
    "\n",
    "# create training output\n",
    "zeros = np.zeros((num_train_samples, 3))\n",
    "\n",
    "a = u_0(th.tensor(xyt_in, dtype=th.float32)).detach().numpy()\n",
    "b = np.zeros((num_train_samples, 1))\n",
    "onze = np.random.permutation(np.concatenate([a,b,a],axis = -1))\n",
    "\n",
    "# [uv_eqn, uv_in, uv_out, uv_w1, uv_w2, uv_circle]\n",
    "y_train = [zeros, onze, zeros, zeros, zeros, zeros]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MultiLayerPerceptronClass(\n",
    "    x_dim=2, y_dim=3,\n",
    "    h_dim_list=[48,48,48,48],\n",
    "    actv= th.nn.Tanh(),\n",
    "    p_drop=0.0,\n",
    "    batch_norm=False\n",
    ")\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "MLP = MLP.to(device)\n",
    "pinn = PINN(network=MLP, rho=rho, mu=mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_BFGS_B:\n",
    "    \"\"\"\n",
    "    Optimize the PyTorch model using L-BFGS-B algorithm.\n",
    "    Attributes:\n",
    "        model: optimization target model.\n",
    "        samples: training samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: optimization target model.\n",
    "            x_train: training input samples as tensors.\n",
    "            y_train: training target samples as tensors.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "        self.optimizer = optim.LBFGS(\n",
    "            params = self.model.parameters(),\n",
    "            lr = 1,\n",
    "            max_iter = 10000,\n",
    "            max_eval = 10000,\n",
    "            tolerance_grad = 1e-5,\n",
    "            tolerance_change = 0.5 * th.finfo(float).eps,\n",
    "            history_size=20,\n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "            )\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradients for the model.\n",
    "        Returns:\n",
    "            loss: the loss as a scalar tensor.\n",
    "        \"\"\"\n",
    "        def closure():\n",
    "            if th.is_grad_enabled():\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "            outputs = self.model(self.x_train)\n",
    "            \n",
    "            # 각 output과 y_train의 해당 타겟 사이의 손실을 계산\n",
    "            # uv_eqn, uv_in, uv_out, uv_w1, uv_w2, uv_circle\n",
    "            \n",
    "            ############################################################################################################\n",
    "            # # Neumann boundary condition\n",
    "            # criterion = nn.MSELoss()\n",
    "            # loss_eqn = criterion(outputs[0], self.y_train[0,:])\n",
    "            # loss_in = criterion(outputs[1], self.y_train[1,:])\n",
    "            # loss_w1 = criterion(outputs[3], self.y_train[3,:])\n",
    "            # loss_w2 = criterion(outputs[4], self.y_train[4,:])\n",
    "            # loss_circle = criterion(outputs[5], self.y_train[5,:])\n",
    "            \n",
    "            # total_loss = loss_eqn + ( loss_in + loss_w1 + loss_w2 + loss_circle ) / 4\n",
    "            ############################################################################################################\n",
    "            # Periodic boundary condition\n",
    "            criterion = nn.MSELoss()\n",
    "            loss_eqn = criterion(outputs[0], self.y_train[0,:])\n",
    "            loss_in = criterion(outputs[1], self.y_train[1,:])\n",
    "            loss_periodic = criterion(outputs[3], outputs[4])\n",
    "            loss_circle = criterion(outputs[5], self.y_train[5,:])\n",
    "            \n",
    "            total_loss = loss_eqn + ( loss_in + loss_periodic + loss_circle ) / 3\n",
    "            ############################################################################################################\n",
    "            # loss_out = criterion(outputs[2], self.y_train[2,:]) # 자유 스트림 조건\n",
    "            \n",
    "            if total_loss.requires_grad:\n",
    "                total_loss.backward()\n",
    "            return total_loss\n",
    "\n",
    "        return closure\n",
    "\n",
    "    def fit(self, max_iter=20, log_interval=1):\n",
    "        \"\"\"\n",
    "        Train the model using L-BFGS-B algorithm.\n",
    "        Args:\n",
    "            max_iter: Maximum number of iterations.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        # Optimize\n",
    "        for iter in range(max_iter):\n",
    "            loss = self.optimizer.step(self.evaluate())\n",
    "            \n",
    "            print(f\"{iter}:{loss}\")   \n",
    "        print('Optimization finished.')\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict using the trained model.\n",
    "        Args:\n",
    "            x: Input data for prediction.\n",
    "        Returns:\n",
    "            predictions: Predicted values by the model.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # 모델을 평가 모드로 설정\n",
    "        with th.no_grad():  # 그라디언트 계산 비활성화\n",
    "            predictions = self.model(x)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_2332\\1191769340.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  x_train = th.tensor(x_train, dtype=th.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.43863725662231445\n",
      "1:0.019912058487534523\n",
      "2:0.019898150116205215\n",
      "3:0.019896259531378746\n",
      "4:0.019893433898687363\n",
      "5:0.01988842338323593\n",
      "6:0.019885610789060593\n",
      "7:0.019885310903191566\n",
      "8:0.01988505944609642\n",
      "9:0.019884957000613213\n",
      "10:0.019884617999196053\n",
      "11:0.019884617999196053\n",
      "12:0.01988399028778076\n",
      "13:0.019883830100297928\n",
      "14:0.019883830100297928\n",
      "15:0.019883830100297928\n",
      "16:0.019883830100297928\n",
      "17:0.019883830100297928\n",
      "18:0.019883830100297928\n",
      "19:0.019883830100297928\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "x_train = th.tensor(x_train, dtype=th.float32)\n",
    "y_train = th.tensor(y_train, dtype=th.float32)\n",
    "# 데이터를 GPU로 이동\n",
    "x_train = x_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "lbfgs = L_BFGS_B(model=pinn, x_train=x_train, y_train=y_train)\n",
    "lbfgs.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(MLP.state_dict(), \"./model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptronClass(\n",
       "  (actv): Tanh()\n",
       "  (net): Sequential(\n",
       "    (linear_00): Linear(in_features=2, out_features=48, bias=True)\n",
       "    (tanh_01): Tanh()\n",
       "    (dropout1d_02): Dropout1d(p=0.0, inplace=False)\n",
       "    (linear_03): Linear(in_features=48, out_features=48, bias=True)\n",
       "    (tanh_04): Tanh()\n",
       "    (dropout1d_05): Dropout1d(p=0.0, inplace=False)\n",
       "    (linear_06): Linear(in_features=48, out_features=48, bias=True)\n",
       "    (tanh_07): Tanh()\n",
       "    (dropout1d_08): Dropout1d(p=0.0, inplace=False)\n",
       "    (linear_09): Linear(in_features=48, out_features=48, bias=True)\n",
       "    (tanh_10): Tanh()\n",
       "    (dropout1d_11): Dropout1d(p=0.0, inplace=False)\n",
       "    (linear_12): Linear(in_features=48, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = MultiLayerPerceptronClass(\n",
    "    x_dim=2, y_dim=3,\n",
    "    h_dim_list=[48,48,48,48],\n",
    "    actv= th.nn.Tanh(),\n",
    "    p_drop=0.0,\n",
    "    batch_norm=False\n",
    ")\n",
    "\n",
    "MLP.load_state_dict(th.load(\"./model_state_dict.pth\"))\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "MLP = MLP.to(device)\n",
    "MLP.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
