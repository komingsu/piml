{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from NN import *\n",
    "from th_operator import calc_grad\n",
    "from utils import print_model_layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task parameters\n",
    "u0 = 0.1 # inlet flow velocity\n",
    "rho = 5000 # density\n",
    "mu = 0.5 # viscosity\n",
    "\n",
    "# Samples\n",
    "num_points_per_step = 10000  # number of spatial points per time step\n",
    "num_BC_points_per_step = 1000  # number of boundary condition points per time step\n",
    "num_IC_points = 50000  # number of initial condition points\n",
    "\n",
    "# Data boundary\n",
    "## Domain\n",
    "x_ini, x_f, y_ini, y_f = -4, 16, -4, 4\n",
    "\n",
    "## Time\n",
    "T = 20  # total time in seconds\n",
    "Delta_t = 0.1  # time step in seconds\n",
    "num_time_steps = int(T / Delta_t)  # number of time steps\n",
    "time_steps = np.linspace(0, T, num=num_time_steps)\n",
    "\n",
    "## Circle\n",
    "Cx, Cy, r = 0, 0, 0.5\n",
    "\n",
    "\n",
    "def generate_domain_points(num_points, x_range, y_range, time_steps, circle_center, circle_radius):\n",
    "    \"\"\"\n",
    "    Generate spatial-temporal points within specified domain excluding a circular obstacle, for given time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - num_points: Number of points to generate for each time step.\n",
    "    - x_range: Tuple of (min_x, max_x) for x-coordinate range.\n",
    "    - y_range: Tuple of (min_y, max_y) for y-coordinate range.\n",
    "    - time_steps: Array of time steps.\n",
    "    - circle_center: Tuple of (Cx, Cy), the center of the circular obstacle.\n",
    "    - circle_radius: Radius of the circular obstacle.\n",
    "\n",
    "    Returns:\n",
    "    - Numpy array of spatial-temporal points with shape (num_points * len(time_steps), 3), where each row represents (x, y, t),\n",
    "      excluding points inside the circular obstacle.\n",
    "    \"\"\"\n",
    "    Cx, Cy = circle_center\n",
    "    xyt_points = []\n",
    "\n",
    "    for t in time_steps:\n",
    "        for _ in range(num_points):\n",
    "            while True:\n",
    "                x = np.random.uniform(x_range[0], x_range[1])\n",
    "                y = np.random.uniform(y_range[0], y_range[1])\n",
    "                # Check if the point is outside the circular obstacle\n",
    "                if (x - Cx)**2 + (y - Cy)**2 >= circle_radius**2:\n",
    "                    xyt_points.append([x, y, t])\n",
    "                    break\n",
    "\n",
    "    return np.array(xyt_points, dtype=np.float32)\n",
    "\n",
    "def generate_boundary_points(num_points, boundary_func, time_steps):\n",
    "    xyt_points = []\n",
    "    for t in time_steps:\n",
    "        points = boundary_func(num_points)\n",
    "        t_col = np.full((points.shape[0], 1), t)\n",
    "        xyt = np.hstack((points, t_col))\n",
    "        xyt_points.append(xyt)\n",
    "    return np.vstack(xyt_points)\n",
    "\n",
    "def circle_boundary(num_points):\n",
    "    # Generate points for the circle boundary\n",
    "    theta = np.random.uniform(0, 2*np.pi, num_points)\n",
    "    x = Cx + 2*r * np.cos(theta)\n",
    "    y = Cy + 2*r * np.sin(theta)\n",
    "    return np.vstack((x, y)).T\n",
    "\n",
    "def wall_boundary(num_points, x_range, y_value):\n",
    "    # Generate points for wall boundaries (top or bottom)\n",
    "    x = np.random.uniform(x_range[0], x_range[1], num_points)\n",
    "    y = np.full(num_points, y_value)\n",
    "    return np.vstack((x, y)).T\n",
    "\n",
    "def inlet_outlet_boundary(num_points, y_range, x_value):\n",
    "    # Generate points for inlet or outlet boundaries\n",
    "    y = np.random.uniform(y_range[0], y_range[1], num_points)\n",
    "    x = np.full(num_points, x_value)\n",
    "    return np.vstack((x, y)).T\n",
    "\n",
    "def generate_initial_conditions(num_points, x_range, y_range):\n",
    "    x_initial = np.random.uniform(x_range[0], x_range[1], num_points)\n",
    "    y_initial = np.random.uniform(y_range[0], y_range[1], num_points)\n",
    "    t_initial = np.zeros(num_points)\n",
    "    xyt_initial = np.stack((x_initial, y_initial, t_initial), axis=-1)\n",
    "    return xyt_initial\n",
    "\n",
    "def train_dataset():\n",
    "    # Generating boundary points for each boundary condition and time step\n",
    "    xyt_eqn = generate_domain_points(num_points_per_step, (x_ini, x_f), (y_ini, y_f), time_steps, (0, 0), 0.5)\n",
    "    xyt_circle = generate_boundary_points(num_BC_points_per_step, circle_boundary, time_steps)\n",
    "    xyt_w1 = generate_boundary_points(num_BC_points_per_step, lambda num_points: wall_boundary(num_points, (x_ini, x_f), y_ini), time_steps)\n",
    "    xyt_w2 = generate_boundary_points(num_BC_points_per_step, lambda num_points: wall_boundary(num_points, (x_ini, x_f), y_f), time_steps)\n",
    "    xyt_in = generate_boundary_points(num_BC_points_per_step, lambda num_points: inlet_outlet_boundary(num_points, (y_ini, y_f), x_ini), time_steps)\n",
    "    xyt_out = generate_boundary_points(num_BC_points_per_step, lambda num_points: inlet_outlet_boundary(num_points, (y_ini, y_f), x_f), time_steps)\n",
    "    xyt_initial = generate_initial_conditions(num_IC_points, (x_ini, x_f), (y_ini, y_f))\n",
    "\n",
    "    # Combine all training points\n",
    "    return {\n",
    "        \"eqn\": xyt_eqn,\n",
    "        \"circle\": xyt_circle,\n",
    "        \"w1\": xyt_w1,\n",
    "        \"w2\": xyt_w2,\n",
    "        \"in\": xyt_in,\n",
    "        \"out\": xyt_out,\n",
    "        \"initial\": xyt_initial,\n",
    "    }\n",
    "\n",
    "x_train = train_dataset()\n",
    "# # Save the training data\n",
    "# mu_str = '{:04.1f}'.format(mu)\n",
    "# file_path = f\"./trainX__rho_{rho:04d}__mu_{mu_str}__.mat\"\n",
    "# sio.savemat(file_path, {\n",
    "#     \"eqn\": xyt_eqn,\n",
    "#     \"circle\": xyt_circle,\n",
    "#     \"w1\": xyt_w1,\n",
    "#     \"w2\": xyt_w2,\n",
    "#     \"in\": xyt_in,\n",
    "#     \"out\": xyt_out,\n",
    "#     \"initial\": xyt_initial,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eqn shape:  (2000000, 3)\n",
      "circle shape:  (200000, 3)\n",
      "w1 shape:  (200000, 3)\n",
      "w2 shape:  (200000, 3)\n",
      "in shape:  (200000, 3)\n",
      "out shape:  (200000, 3)\n",
      "initial shape:  (50000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"eqn shape: \", x_train[\"eqn\"].shape)\n",
    "print(\"circle shape: \", x_train[\"circle\"].shape)\n",
    "print(\"w1 shape: \", x_train[\"w1\"].shape)\n",
    "print(\"w2 shape: \", x_train[\"w2\"].shape)\n",
    "print(\"in shape: \", x_train[\"in\"].shape)\n",
    "print(\"out shape: \", x_train[\"out\"].shape)\n",
    "print(\"initial shape: \", x_train[\"initial\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치학습을 위한 데이터 로더 함수를 정의합니다.\n",
    "def create_dataloader(x_data, batch_size, shuffle):\n",
    "    dataset = TensorDataset(th.tensor(x_data, dtype=th.float32))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "def generate_combined_batch(dataloaders_iterators, batch_size_per_loader):\n",
    "    batch_parts = {}\n",
    "    for key, iterator in dataloaders_iterators.items():\n",
    "        try:\n",
    "            # 각 DataLoader의 iterator로부터 데이터 배치를 가져옴\n",
    "            data, = next(iterator)\n",
    "            batch_parts[key] = data[:batch_size_per_loader]\n",
    "        except StopIteration:\n",
    "            # 현재 DataLoader의 데이터가 끝에 도달했을 경우, iterator를 다시 시작\n",
    "            dataloaders_iterators[key] = iter(dataloaders[key])\n",
    "            data, = next(dataloaders_iterators[key])\n",
    "            batch_parts[key] = data[:batch_size_per_loader]\n",
    "    return batch_parts\n",
    "\n",
    "dataloaders = {key: create_dataloader(x_train[key], 10000, True) for key in x_train}\n",
    "dataloaders_iterators = {key: iter(loader) for key, loader in dataloaders.items()}\n",
    "\n",
    "\n",
    "# for _ in range(200):  # 200번의 iteration으로 1 epoch을 수행\n",
    "#     print(\"---\")\n",
    "#     combined_batch = generate_combined_batch(dataloaders_iterators, batch_size_per_loader=10000)\n",
    "#     for k, v in combined_batch.items():\n",
    "#         print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_5608\\4233571621.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  xyt_eqn[i, 0] = (x_f - x_ini) * np.random.rand(1, 1) + x_ini\n",
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_5608\\4233571621.py:35: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  xyt_eqn[i, 1] = (y_f - y_ini) * np.random.rand(1, 1) + y_ini\n"
     ]
    }
   ],
   "source": [
    "# Task parameters\n",
    "u0 = 0.1 # inlet flow velocity\n",
    "rho = 5000 # density\n",
    "mu = 0.5 # viscosity\n",
    "\n",
    "# Samples\n",
    "num_train_samples = 20000 # number of training samples\n",
    "num_test_samples = 100 # number of test samples\n",
    "\n",
    "# Data boundary\n",
    "# Domain\n",
    "x_f =16\n",
    "x_ini=-4\n",
    "y_f= 4\n",
    "y_ini= -4\n",
    "# Time\n",
    "T = 20  # total time in seconds\n",
    "Delta_t = 0.1  # time step in seconds\n",
    "num_time_steps = int(T / Delta_t)  # number of time steps\n",
    "# Circle\n",
    "Cx = 0\n",
    "Cy = 0\n",
    "a = 1\n",
    "b = 1\n",
    "\n",
    "# create training input\n",
    "# Eqauation collocation points\n",
    "xyt_eqn = np.random.rand(num_train_samples, 2)\n",
    "xyt_eqn[...,0] = (x_f - x_ini)*xyt_eqn[...,0] + x_ini\n",
    "xyt_eqn[...,1] = (y_f - y_ini)*xyt_eqn[...,1] + y_ini\n",
    "\n",
    "for i in range(num_train_samples):\n",
    "    while (xyt_eqn[i, 0] - Cx)**2/a**2 + (xyt_eqn[i, 1] - Cy)**2/b**2 < 1:\n",
    "        xyt_eqn[i, 0] = (x_f - x_ini) * np.random.rand(1, 1) + x_ini\n",
    "        xyt_eqn[i, 1] = (y_f - y_ini) * np.random.rand(1, 1) + y_ini\n",
    "\n",
    "# Boundary collocation points\n",
    "# Circle\n",
    "xyt_circle = np.random.rand(num_train_samples, 2)\n",
    "xyt_circle[...,0] = 2*(a)*xyt_circle[...,0] +(Cx-a)\n",
    "xyt_circle[0:num_train_samples//2,1] = b*(1 - (xyt_circle[0:num_train_samples//2,0]-Cx)**2 / a**2)**0.5 + Cy\n",
    "xyt_circle[num_train_samples//2:,1] = -b*(1 - (xyt_circle[num_train_samples//2:,0]-Cx)**2 / a**2)**0.5 + Cy\n",
    "\n",
    "# Wall top and bottom\n",
    "xyt_w1 = np.random.rand(num_train_samples, 2)  # top-bottom boundaries\n",
    "xyt_w1[..., 0] = (x_f - x_ini)*xyt_w1[...,0] + x_ini\n",
    "xyt_w1[..., 1] =  y_ini          # y-position is 0 or 1\n",
    "\n",
    "xyt_w2 = np.random.rand(num_train_samples, 2)  # top-bottom boundaries\n",
    "xyt_w2[..., 0] = (x_f - x_ini)*xyt_w2[...,0] + x_ini\n",
    "xyt_w2[..., 1] =  y_f\n",
    "\n",
    "# Inlet and outlet\n",
    "xyt_in = np.random.rand(num_train_samples, 2)\n",
    "xyt_in[...,0] = x_ini\n",
    "\n",
    "xyt_out = np.random.rand(num_train_samples, 2)  # left-right boundaries\n",
    "xyt_out[..., 0] = x_f\n",
    "\n",
    "x_train = [xyt_eqn, xyt_in, xyt_out, xyt_w1, xyt_w2, xyt_circle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:[16]\n",
      "[  ] layer:[          input] size:[          16x3]\n",
      "[ 0] layer:[      linear_00] size:[         16x48] numel:[       768]\n",
      "[ 1] layer:[        tanh_01] size:[         16x48] numel:[       768]\n",
      "[ 2] layer:[      linear_02] size:[         16x48] numel:[       768]\n",
      "[ 3] layer:[        tanh_03] size:[         16x48] numel:[       768]\n",
      "[ 4] layer:[      linear_04] size:[         16x48] numel:[       768]\n",
      "[ 5] layer:[        tanh_05] size:[         16x48] numel:[       768]\n",
      "[ 6] layer:[      linear_06] size:[         16x48] numel:[       768]\n",
      "[ 7] layer:[        tanh_07] size:[         16x48] numel:[       768]\n",
      "[ 8] layer:[      linear_08] size:[          16x3] numel:[        48]\n"
     ]
    }
   ],
   "source": [
    "MLP = MultiLayerPerceptronClass(\n",
    "    x_dim=3, y_dim=3,\n",
    "    h_dim_list=[48,48,48,48],\n",
    "    actv= th.nn.Tanh(),\n",
    "    p_drop=0.0,\n",
    "    batch_norm=False\n",
    ")\n",
    "print_model_layers(model=MLP, x_torch=th.rand(size=[16,3], dtype=th.float32))\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "MLP = MLP.to(device)\n",
    "# MLP = th.nn.DataParallel(MLP)\n",
    "pinn = PINN(network=MLP, rho=rho, mu=mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_BFGS_B:\n",
    "    \"\"\"\n",
    "    Optimize the PyTorch model using L-BFGS-B algorithm.\n",
    "    Attributes:\n",
    "        model: optimization target model.\n",
    "        samples: training samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, dataloaders_iterators, generate_func, batch_size=10000, epochs=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: optimization target model.\n",
    "            x_train: training input samples as tensors.\n",
    "            y_train: training target samples as tensors.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataloaders_iterators = dataloaders_iterators\n",
    "        self.generate_combined_batch = generate_func\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.optimizer = optim.LBFGS(\n",
    "            params = self.model.parameters(),\n",
    "            lr = 0.5,\n",
    "            max_iter = 1000,\n",
    "            max_eval = 1000,\n",
    "            tolerance_grad = 1e-5,\n",
    "            tolerance_change = th.finfo(float).eps,\n",
    "            history_size=50,\n",
    "            line_search_fn=\"strong_wolfe\",\n",
    "            )\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradients for the model.\n",
    "        Returns:\n",
    "            loss: the loss as a scalar tensor.\n",
    "        \"\"\"\n",
    "        def closure():\n",
    "            \n",
    "            \n",
    "            if th.is_grad_enabled():\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            combined_batch = self.generate_combined_batch(self.dataloaders_iterators, self.batch_size)\n",
    "            for key in combined_batch:\n",
    "                combined_batch[key] = combined_batch[key].to(device)\n",
    "            \n",
    "            PDE, BC, IC = self.model(combined_batch)\n",
    "            \n",
    "            # PDE: u_eqn, v_eqn\n",
    "            # BC: uv_in, uv_w1, uv_w2, uv_circle # uv_out is not used\n",
    "            # IC: u_initial, v_initial\n",
    "            \n",
    "            inlet = BC[0][..., 1].unsqueeze(-1) # y' = y - ymin / ymax - ymin\n",
    "            inlet = 4*inlet*(1-inlet)\n",
    "            zeros = th.zeros((self.batch_size, 1))\n",
    "            zeros = zeros.to(device)\n",
    "            inlet = th.cat([inlet, zeros], dim = -1)\n",
    "            \n",
    "            # LOSS\n",
    "            criterion = nn.MSELoss()\n",
    "            loss_eqn = criterion(PDE, th.zeros_like(PDE))\n",
    "            loss_in = criterion(BC[0], inlet)\n",
    "            loss_periodic = criterion(BC[1], BC[2])\n",
    "            loss_circle = criterion(BC[3], th.zeros_like(BC[3]))\n",
    "            loss_ic = criterion(IC, th.zeros_like(IC))\n",
    "            total_loss = loss_eqn + ( loss_in + loss_periodic + loss_circle + loss_ic) / 4\n",
    "\n",
    "            if total_loss.requires_grad:\n",
    "                total_loss.backward()\n",
    "            \n",
    "\n",
    "            return total_loss\n",
    "        \n",
    "        return closure\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the model using L-BFGS-B algorithm.\n",
    "        Args:\n",
    "            max_iter: Maximum number of iterations.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        # Optimize\n",
    "        for epoch in range(self.epochs):\n",
    "            for iter in range(200):\n",
    "                loss = self.optimizer.step(self.evaluate())\n",
    "                print(f\"{epoch:02d}:{iter:03d}:{loss}\")   \n",
    "        print('Optimization finished.')\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict using the trained model.\n",
    "        Args:\n",
    "            x: Input data for prediction.\n",
    "        Returns:\n",
    "            predictions: Predicted values by the model.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # 모델을 평가 모드로 설정\n",
    "        with th.no_grad():  # 그라디언트 계산 비활성화\n",
    "            predictions = self.model(x)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:000:0.00590389221906662\n",
      "00:001:0.0060229552909731865\n",
      "00:002:0.0061650583520531654\n",
      "00:003:0.005948071368038654\n",
      "00:004:0.0060457224026322365\n",
      "00:005:0.005891289561986923\n",
      "00:006:0.006130452733486891\n",
      "00:007:0.006213209126144648\n",
      "00:008:0.005910563748329878\n",
      "00:009:0.006130942143499851\n",
      "00:010:0.005759900435805321\n",
      "00:011:0.005914062261581421\n",
      "00:012:0.005897027440369129\n",
      "00:013:0.006111622788012028\n",
      "00:014:0.006087162997573614\n",
      "00:015:0.0060966163873672485\n",
      "00:016:0.005811913870275021\n",
      "00:017:0.005841623060405254\n",
      "00:018:0.005793738178908825\n",
      "00:019:0.0057824840769171715\n",
      "00:020:0.006016789469867945\n",
      "00:021:0.005846207961440086\n",
      "00:022:0.005848351866006851\n",
      "00:023:0.005942717660218477\n",
      "00:024:0.005894653033465147\n",
      "00:025:0.005881688557565212\n",
      "00:026:0.005664526484906673\n",
      "00:027:0.00579722598195076\n",
      "00:028:0.0061312150210142136\n",
      "00:029:0.005791706498712301\n",
      "00:030:0.005940423347055912\n",
      "00:031:0.005971744190901518\n",
      "00:032:0.006002441048622131\n",
      "00:033:0.005775209981948137\n",
      "00:034:0.0058482130989432335\n",
      "00:035:0.006077520549297333\n",
      "00:036:0.00601545162498951\n",
      "00:037:0.005743682384490967\n",
      "00:038:0.005778566002845764\n",
      "00:039:0.005707621108740568\n",
      "00:040:0.005752576515078545\n",
      "00:041:0.005789986811578274\n",
      "00:042:0.005887720733880997\n",
      "00:043:0.005725998431444168\n",
      "00:044:0.005639970302581787\n",
      "00:045:0.006030953023582697\n",
      "00:046:0.0060227662324905396\n",
      "00:047:0.005778512917459011\n",
      "00:048:0.005795433185994625\n",
      "00:049:0.005804040934890509\n",
      "00:050:0.005780560430139303\n",
      "00:051:0.005891413427889347\n",
      "00:052:0.005791352596133947\n",
      "00:053:0.005549796391278505\n",
      "00:054:0.005852077156305313\n",
      "00:055:0.005851867608726025\n",
      "00:056:0.005899603478610516\n",
      "00:057:0.005873134359717369\n",
      "00:058:0.005681154318153858\n",
      "00:059:0.005857744254171848\n",
      "00:060:0.005792178679257631\n",
      "00:061:0.005757401697337627\n",
      "00:062:0.00565843190997839\n",
      "00:063:0.005715664476156235\n",
      "00:064:0.005779958795756102\n",
      "00:065:0.005702255293726921\n",
      "00:066:0.005926460959017277\n",
      "00:067:0.005753912031650543\n",
      "00:068:0.005694717168807983\n",
      "00:069:0.0057915328070521355\n",
      "00:070:0.005787244997918606\n",
      "00:071:0.005727250128984451\n",
      "00:072:0.0056849634274840355\n",
      "00:073:0.005936887115240097\n",
      "00:074:0.0056847380474209785\n",
      "00:075:0.005875117145478725\n",
      "00:076:0.005811034701764584\n",
      "00:077:0.005871144123375416\n",
      "00:078:0.005878503900021315\n",
      "00:079:0.005749897100031376\n",
      "00:080:0.005769500508904457\n",
      "00:081:0.005663473159074783\n",
      "00:082:0.0057148621417582035\n",
      "00:083:0.005751060321927071\n",
      "00:084:0.005732703022658825\n",
      "00:085:0.005705517716705799\n",
      "00:086:0.005621026735752821\n",
      "00:087:0.005897887982428074\n",
      "00:088:0.005659207701683044\n",
      "00:089:0.005909697152674198\n",
      "00:090:0.00574076920747757\n",
      "00:091:0.0055158706381917\n",
      "00:092:0.005666668526828289\n",
      "00:093:0.0056932102888822556\n",
      "00:094:0.0055336118675768375\n",
      "00:095:0.005411657504737377\n",
      "00:096:0.005752682685852051\n",
      "00:097:0.005624874494969845\n",
      "00:098:0.005720647983253002\n",
      "00:099:0.005786849185824394\n",
      "00:100:0.005587052553892136\n",
      "00:101:0.005704253911972046\n",
      "00:102:0.005562144331634045\n",
      "00:103:0.005444630980491638\n",
      "00:104:0.00564420223236084\n",
      "00:105:0.005610549822449684\n",
      "00:106:0.005536332726478577\n",
      "00:107:0.005440265405923128\n",
      "00:108:0.005709969438612461\n",
      "00:109:0.005642371252179146\n",
      "00:110:0.005672049708664417\n",
      "00:111:0.005579533986747265\n",
      "00:112:0.005674571264535189\n",
      "00:113:0.005658532027155161\n",
      "00:114:0.00556569779291749\n",
      "00:115:0.005378861911594868\n",
      "00:116:0.005572898779064417\n",
      "00:117:0.005555956624448299\n",
      "00:118:0.005564193241298199\n",
      "00:119:0.005386032164096832\n",
      "00:120:0.005380787421017885\n",
      "00:121:0.005583514459431171\n",
      "00:122:0.005495776422321796\n",
      "00:123:0.005574243143200874\n",
      "00:124:0.0056553492322564125\n",
      "00:125:0.005423958878964186\n",
      "00:126:0.005459626205265522\n",
      "00:127:0.005322259850800037\n",
      "00:128:0.00535199511796236\n",
      "00:129:0.005362619645893574\n",
      "00:130:0.005553520284593105\n",
      "00:131:0.005472871474921703\n",
      "00:132:0.005422522313892841\n",
      "00:133:0.0055467672646045685\n",
      "00:134:0.00528345163911581\n",
      "00:135:0.005461685359477997\n",
      "00:136:0.005397707223892212\n",
      "00:137:0.0054697999730706215\n",
      "00:138:0.0051374249160289764\n",
      "00:139:0.005235230550169945\n",
      "00:140:0.005354986060410738\n",
      "00:141:0.005215841345489025\n",
      "00:142:0.005319950170814991\n",
      "00:143:0.005402819253504276\n",
      "00:144:0.005380303133279085\n",
      "00:145:0.005384442396461964\n",
      "00:146:0.005294426344335079\n",
      "00:147:0.005476593039929867\n",
      "00:148:0.005205427296459675\n",
      "00:149:0.005229615606367588\n",
      "00:150:0.005405953619629145\n",
      "00:151:0.00527959456667304\n",
      "00:152:0.0055337329395115376\n",
      "00:153:0.005384313873946667\n",
      "00:154:0.005193463992327452\n",
      "00:155:0.005325150676071644\n",
      "00:156:0.005289793014526367\n",
      "00:157:0.00535168033093214\n",
      "00:158:0.005510618910193443\n",
      "00:159:0.00550985150039196\n",
      "00:160:0.005161565728485584\n",
      "00:161:0.005294003523886204\n",
      "00:162:0.005375209730118513\n",
      "00:163:0.005304860882461071\n",
      "00:164:0.005431877449154854\n",
      "00:165:0.005457976367324591\n",
      "00:166:0.005276599898934364\n",
      "00:167:0.005278709344565868\n",
      "00:168:0.005353823769837618\n",
      "00:169:0.0052284495905041695\n",
      "00:170:0.0052366359159350395\n",
      "00:171:0.005186283960938454\n",
      "00:172:0.005189957097172737\n",
      "00:173:0.00515777338296175\n",
      "00:174:0.005280204117298126\n",
      "00:175:0.005326221697032452\n",
      "00:176:0.0051472969353199005\n",
      "00:177:0.00518687441945076\n",
      "00:178:0.005294133443385363\n",
      "00:179:0.005201558582484722\n",
      "00:180:0.005232393741607666\n",
      "00:181:0.005117228254675865\n",
      "00:182:0.005086615215986967\n",
      "00:183:0.004999789875000715\n",
      "00:184:0.005091020837426186\n",
      "00:185:0.005160965491086245\n",
      "00:186:0.0052617876790463924\n",
      "00:187:0.00532684288918972\n",
      "00:188:0.005255099385976791\n",
      "00:189:0.005019095726311207\n",
      "00:190:0.004886561073362827\n",
      "00:191:0.005298653617501259\n",
      "00:192:0.005123727954924107\n",
      "00:193:0.00515627209097147\n",
      "00:194:0.005214503966271877\n",
      "00:195:0.005173006094992161\n",
      "00:196:0.005150923505425453\n",
      "00:197:0.005080471746623516\n",
      "00:198:0.005278469994664192\n",
      "00:199:0.005267789121717215\n",
      "01:000:0.004891437478363514\n",
      "01:001:0.005001965910196304\n",
      "01:002:0.0050427853129804134\n",
      "01:003:0.005040498450398445\n",
      "01:004:0.005143525078892708\n",
      "01:005:0.005053956527262926\n",
      "01:006:0.0049581630155444145\n",
      "01:007:0.005254590883851051\n",
      "01:008:0.0051335301250219345\n",
      "01:009:0.0051163178868591785\n",
      "01:010:0.0049881720915436745\n",
      "01:011:0.005127723328769207\n",
      "01:012:0.005056831985712051\n",
      "01:013:0.005114520899951458\n",
      "01:014:0.004998326301574707\n",
      "01:015:0.0049943882040679455\n",
      "01:016:0.005056525580585003\n",
      "01:017:0.005102441646158695\n",
      "01:018:0.005135961342602968\n",
      "01:019:0.005051632411777973\n",
      "01:020:0.005037921480834484\n",
      "01:021:0.005049876868724823\n",
      "01:022:0.005079835653305054\n",
      "01:023:0.005055151414126158\n",
      "01:024:0.005069679580628872\n",
      "01:025:0.0050066327676177025\n",
      "01:026:0.0051434701308608055\n",
      "01:027:0.0050778985023498535\n",
      "01:028:0.004817035980522633\n",
      "01:029:0.0051134079694747925\n",
      "01:030:0.005110542289912701\n",
      "01:031:0.004949719645082951\n",
      "01:032:0.004879387095570564\n",
      "01:033:0.00477776350453496\n",
      "01:034:0.00497528538107872\n",
      "01:035:0.004999929573386908\n",
      "01:036:0.004940071143209934\n",
      "01:037:0.005127224139869213\n",
      "01:038:0.005153799429535866\n",
      "01:039:0.00498289056122303\n",
      "01:040:0.004968394059687853\n",
      "01:041:0.004920456558465958\n",
      "01:042:0.004914996214210987\n",
      "01:043:0.005070772022008896\n",
      "01:044:0.005057642236351967\n",
      "01:045:0.004997570998966694\n",
      "01:046:0.004870742559432983\n",
      "01:047:0.0048966603353619576\n",
      "01:048:0.005055159330368042\n",
      "01:049:0.005011348053812981\n",
      "01:050:0.004992964677512646\n",
      "01:051:0.004889543633908033\n",
      "01:052:0.004930499941110611\n",
      "01:053:0.004833810031414032\n",
      "01:054:0.004716528579592705\n",
      "01:055:0.005078161135315895\n",
      "01:056:0.004885856993496418\n",
      "01:057:0.004910760559141636\n",
      "01:058:0.005071026273071766\n",
      "01:059:0.004963151179254055\n",
      "01:060:0.004924262408167124\n",
      "01:061:0.004957378841936588\n",
      "01:062:0.004921069368720055\n",
      "01:063:0.005076509900391102\n",
      "01:064:0.004872302524745464\n",
      "01:065:0.004878164269030094\n",
      "01:066:0.0048894803039729595\n",
      "01:067:0.004869340918958187\n",
      "01:068:0.004814033396542072\n",
      "01:069:0.004963619634509087\n",
      "01:070:0.004963777028024197\n",
      "01:071:0.004763471428304911\n",
      "01:072:0.004891395568847656\n",
      "01:073:0.004893583245575428\n",
      "01:074:0.00504100089892745\n",
      "01:075:0.004921613726764917\n",
      "01:076:0.00479238061234355\n",
      "01:077:0.004750254563987255\n",
      "01:078:0.004876777529716492\n",
      "01:079:0.004777027294039726\n",
      "01:080:0.004869095981121063\n",
      "01:081:0.004679219331592321\n",
      "01:082:0.004805079661309719\n",
      "01:083:0.004814782179892063\n",
      "01:084:0.004643912427127361\n",
      "01:085:0.0047589074820280075\n",
      "01:086:0.004869251511991024\n",
      "01:087:0.004580679349601269\n",
      "01:088:0.004840908572077751\n",
      "01:089:0.004917442798614502\n",
      "01:090:0.004795538727194071\n",
      "01:091:0.004617620259523392\n",
      "01:092:0.004834773018956184\n",
      "01:093:0.004658214747905731\n",
      "01:094:0.0045950631611049175\n",
      "01:095:0.00467807799577713\n",
      "01:096:0.004769209772348404\n",
      "01:097:0.004775821231305599\n",
      "01:098:0.00482874596491456\n",
      "01:099:0.004745869431644678\n",
      "01:100:0.0045177629217505455\n",
      "01:101:0.0047560445964336395\n",
      "01:102:0.004822202958166599\n",
      "01:103:0.004760207608342171\n",
      "01:104:0.004973030649125576\n",
      "01:105:0.004753275774419308\n",
      "01:106:0.004897735081613064\n",
      "01:107:0.00470252800732851\n",
      "01:108:0.00486052967607975\n",
      "01:109:0.004641417879611254\n",
      "01:110:0.004900637082755566\n",
      "01:111:0.004995313938707113\n",
      "01:112:0.00479918671771884\n",
      "01:113:0.004845314659178257\n",
      "01:114:0.004815048538148403\n",
      "01:115:0.004656920209527016\n",
      "01:116:0.0046091931872069836\n",
      "01:117:0.004486804362386465\n",
      "01:118:0.004640142433345318\n",
      "01:119:0.00451210793107748\n",
      "01:120:0.00461889524012804\n",
      "01:121:0.004466511774808168\n",
      "01:122:0.004713888745754957\n",
      "01:123:0.004787946119904518\n",
      "01:124:0.0047224038280546665\n",
      "01:125:0.004687578417360783\n",
      "01:126:0.004503925330936909\n",
      "01:127:0.004694117233157158\n",
      "01:128:0.004521788097918034\n",
      "01:129:0.004737670533359051\n",
      "01:130:0.0046813515946269035\n",
      "01:131:0.004830250516533852\n",
      "01:132:0.0045689186081290245\n",
      "01:133:0.004623735323548317\n",
      "01:134:0.004603381268680096\n",
      "01:135:0.004499560222029686\n",
      "01:136:0.004476284142583609\n",
      "01:137:0.004593010991811752\n",
      "01:138:0.004726793617010117\n",
      "01:139:0.004563044756650925\n",
      "01:140:0.004561813548207283\n",
      "01:141:0.00460183247923851\n",
      "01:142:0.004614529199898243\n",
      "01:143:0.004615166690200567\n",
      "01:144:0.00440953578799963\n",
      "01:145:0.00444012600928545\n",
      "01:146:0.0046259453520178795\n",
      "01:147:0.004779195412993431\n",
      "01:148:0.004616857971996069\n",
      "01:149:0.004651919938623905\n",
      "01:150:0.004313432611525059\n",
      "01:151:0.0045235054567456245\n",
      "01:152:0.0046762870624661446\n",
      "01:153:0.004495957866311073\n",
      "01:154:0.004487551748752594\n",
      "01:155:0.004469561390578747\n",
      "01:156:0.004574480466544628\n",
      "01:157:0.004575604572892189\n",
      "01:158:0.004663458559662104\n",
      "01:159:0.004649074748158455\n",
      "01:160:0.0043883598409593105\n",
      "01:161:0.004551497288048267\n",
      "01:162:0.004457375034689903\n",
      "01:163:0.004621060099452734\n",
      "01:164:0.004465622361749411\n",
      "01:165:0.0046460554003715515\n",
      "01:166:0.004497542046010494\n",
      "01:167:0.004431739449501038\n",
      "01:168:0.004475720692425966\n",
      "01:169:0.004528637509793043\n",
      "01:170:0.004851894918829203\n",
      "01:171:0.004470773972570896\n",
      "01:172:0.004611042328178883\n",
      "01:173:0.004529191181063652\n",
      "01:174:0.004220724105834961\n",
      "01:175:0.004522908944636583\n",
      "01:176:0.0047414470463991165\n",
      "01:177:0.004520111717283726\n",
      "01:178:0.004619724117219448\n",
      "01:179:0.004469179082661867\n",
      "01:180:0.004573727026581764\n",
      "01:181:0.004457798786461353\n",
      "01:182:0.004678955301642418\n",
      "01:183:0.004616179969161749\n",
      "01:184:0.004586062394082546\n",
      "01:185:0.004440689459443092\n",
      "01:186:0.004466243553906679\n",
      "01:187:0.004632378928363323\n",
      "01:188:0.004445746541023254\n",
      "01:189:0.004601216875016689\n",
      "01:190:0.004446434788405895\n",
      "01:191:0.004506823606789112\n",
      "01:192:0.004456215538084507\n",
      "01:193:0.0044728415086865425\n",
      "01:194:0.004344763699918985\n",
      "01:195:0.004455508664250374\n",
      "01:196:0.004355406854301691\n",
      "01:197:0.004544652998447418\n",
      "01:198:0.004604109562933445\n",
      "01:199:0.004426712170243263\n",
      "02:000:0.004554194398224354\n",
      "02:001:0.004547957330942154\n",
      "02:002:0.004374298732727766\n",
      "02:003:0.00461299205198884\n",
      "02:004:0.0045339204370975494\n",
      "02:005:0.004385758191347122\n",
      "02:006:0.0044030481949448586\n",
      "02:007:0.004490168299525976\n",
      "02:008:0.004471755586564541\n",
      "02:009:0.004367622546851635\n",
      "02:010:0.0044682808220386505\n",
      "02:011:0.004341841675341129\n",
      "02:012:0.00436605978757143\n",
      "02:013:0.004410235211253166\n",
      "02:014:0.004391210153698921\n",
      "02:015:0.0043769823387265205\n",
      "02:016:0.004419587552547455\n",
      "02:017:0.004424407612532377\n",
      "02:018:0.004537299741059542\n",
      "02:019:0.00452599860727787\n",
      "02:020:0.004579074680805206\n",
      "02:021:0.004447014071047306\n",
      "02:022:0.004628249444067478\n",
      "02:023:0.004244739189743996\n",
      "02:024:0.004544137045741081\n",
      "02:025:0.004428300075232983\n",
      "02:026:0.004430354107171297\n",
      "02:027:0.0048094242811203\n",
      "02:028:0.004374464508146048\n",
      "02:029:0.004422886297106743\n",
      "02:030:0.004411463625729084\n",
      "02:031:0.00455507542937994\n",
      "02:032:0.00445551285520196\n",
      "02:033:0.004535100422799587\n",
      "02:034:0.004415758885443211\n",
      "02:035:0.004688385874032974\n",
      "02:036:0.0043815141543745995\n",
      "02:037:0.004198299255222082\n",
      "02:038:0.004206028766930103\n",
      "02:039:0.004482762888073921\n",
      "02:040:0.0044304924085736275\n",
      "02:041:0.004373534582555294\n",
      "02:042:0.004480461124330759\n",
      "02:043:0.004543835297226906\n",
      "02:044:0.004505266435444355\n",
      "02:045:0.004347734618932009\n",
      "02:046:0.004363849759101868\n",
      "02:047:0.004419112578034401\n",
      "02:048:0.004411063157021999\n",
      "02:049:0.004447360523045063\n",
      "02:050:0.004388805478811264\n",
      "02:051:0.004372068680822849\n",
      "02:052:0.004366355948150158\n",
      "02:053:0.004314294550567865\n",
      "02:054:0.00439167395234108\n",
      "02:055:0.004252808168530464\n",
      "02:056:0.004431215114891529\n",
      "02:057:0.004335477948188782\n",
      "02:058:0.004405270330607891\n",
      "02:059:0.004454623907804489\n",
      "02:060:0.004489511251449585\n",
      "02:061:0.004435529001057148\n",
      "02:062:0.004394448362290859\n",
      "02:063:0.004464465659111738\n",
      "02:064:0.004278925713151693\n",
      "02:065:0.004390645306557417\n",
      "02:066:0.004127921536564827\n",
      "02:067:0.004259352572262287\n",
      "02:068:0.0043401336297392845\n",
      "02:069:0.004559873603284359\n",
      "02:070:0.004259341396391392\n",
      "02:071:0.004492057953029871\n",
      "02:072:0.00424962630495429\n",
      "02:073:0.004331502132117748\n",
      "02:074:0.0042546712793409824\n",
      "02:075:0.0042157890275120735\n",
      "02:076:0.004303107038140297\n",
      "02:077:0.004340621177107096\n",
      "02:078:0.0044011399149894714\n",
      "02:079:0.0042471857741475105\n",
      "02:080:0.0042717792093753815\n",
      "02:081:0.004404515027999878\n",
      "02:082:0.0043408372439444065\n",
      "02:083:0.004370862618088722\n",
      "02:084:0.004129796754568815\n",
      "02:085:0.00440576858818531\n",
      "02:086:0.004379602149128914\n",
      "02:087:0.004300275351852179\n",
      "02:088:0.0043338267132639885\n",
      "02:089:0.004366646055132151\n",
      "02:090:0.004269083496183157\n",
      "02:091:0.004059209953993559\n",
      "02:092:0.0043197935447096825\n",
      "02:093:0.004349551163613796\n",
      "02:094:0.004262843169271946\n",
      "02:095:0.004427585285156965\n",
      "02:096:0.00410783477127552\n",
      "02:097:0.004299903754144907\n",
      "02:098:0.004026479087769985\n",
      "02:099:0.004279452376067638\n",
      "02:100:0.0043930783867836\n",
      "02:101:0.004357711877673864\n",
      "02:102:0.00441654771566391\n",
      "02:103:0.004239195957779884\n",
      "02:104:0.004288418218493462\n",
      "02:105:0.004373311996459961\n",
      "02:106:0.004377200733870268\n",
      "02:107:0.004157525487244129\n",
      "02:108:0.004288860596716404\n",
      "02:109:0.004537362605333328\n",
      "02:110:0.004289533942937851\n",
      "02:111:0.004072743467986584\n",
      "02:112:0.003995206207036972\n",
      "02:113:0.004297896288335323\n",
      "02:114:0.004336138255894184\n",
      "02:115:0.004283548332750797\n",
      "02:116:0.004407260566949844\n",
      "02:117:0.00431209709495306\n",
      "02:118:0.004191354848444462\n",
      "02:119:0.004098124336451292\n",
      "02:120:0.004044170491397381\n",
      "02:121:0.004255606327205896\n",
      "02:122:0.004030467942357063\n",
      "02:123:0.004130898043513298\n",
      "02:124:0.004208602011203766\n",
      "02:125:0.004056225996464491\n",
      "02:126:0.004218516871333122\n",
      "02:127:0.0043431855738162994\n",
      "02:128:0.004181590862572193\n",
      "02:129:0.004357588477432728\n",
      "02:130:0.0041829971596598625\n",
      "02:131:0.004190969280898571\n",
      "02:132:0.0043806517496705055\n",
      "02:133:0.004163144156336784\n",
      "02:134:0.0043286485597491264\n",
      "02:135:0.004115364979952574\n",
      "02:136:0.004395563621073961\n",
      "02:137:0.004133915528655052\n",
      "02:138:0.00419744523242116\n",
      "02:139:0.004376114346086979\n",
      "02:140:0.004244030453264713\n",
      "02:141:0.004251554608345032\n",
      "02:142:0.004007083363831043\n",
      "02:143:0.004268762189894915\n",
      "02:144:0.004241551272571087\n",
      "02:145:0.0041184378787875175\n",
      "02:146:0.004176767077296972\n",
      "02:147:0.004281311761587858\n",
      "02:148:0.0041206637397408485\n",
      "02:149:0.004116185009479523\n",
      "02:150:0.00427730567753315\n",
      "02:151:0.004312507808208466\n",
      "02:152:0.004170019179582596\n",
      "02:153:0.004085404798388481\n",
      "02:154:0.0042600734159350395\n",
      "02:155:0.004170370288193226\n",
      "02:156:0.00420067273080349\n",
      "02:157:0.004328502807766199\n",
      "02:158:0.004109999164938927\n",
      "02:159:0.004200855270028114\n",
      "02:160:0.004146834835410118\n",
      "02:161:0.004341132007539272\n",
      "02:162:0.004089427180588245\n",
      "02:163:0.004205634817481041\n",
      "02:164:0.0041051870211958885\n",
      "02:165:0.004151592031121254\n",
      "02:166:0.004186602309346199\n",
      "02:167:0.004264244809746742\n",
      "02:168:0.0040888008661568165\n",
      "02:169:0.0040315077640116215\n",
      "02:170:0.004085744731128216\n",
      "02:171:0.0042634159326553345\n",
      "02:172:0.004309631418436766\n",
      "02:173:0.004299040883779526\n",
      "02:174:0.004113065078854561\n",
      "02:175:0.003918750677257776\n",
      "02:176:0.004194939509034157\n",
      "02:177:0.0041734809055924416\n",
      "02:178:0.004117488861083984\n",
      "02:179:0.004086336120963097\n",
      "02:180:0.004164739511907101\n",
      "02:181:0.004185006953775883\n",
      "02:182:0.004197621252387762\n",
      "02:183:0.004231927916407585\n",
      "02:184:0.004155966453254223\n",
      "02:185:0.00399345438927412\n",
      "02:186:0.004166536033153534\n",
      "02:187:0.004109814763069153\n",
      "02:188:0.0042456514202058315\n",
      "02:189:0.004024087451398373\n",
      "02:190:0.00424516387283802\n",
      "02:191:0.004142302088439465\n",
      "02:192:0.004051848314702511\n",
      "02:193:0.004069936461746693\n",
      "02:194:0.00426477612927556\n",
      "02:195:0.004221774637699127\n",
      "02:196:0.004184095188975334\n",
      "02:197:0.004217426758259535\n",
      "02:198:0.003960893023759127\n",
      "02:199:0.00399791169911623\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "lbfgs = L_BFGS_B(model=pinn, dataloaders_iterators=dataloaders_iterators, generate_func=generate_combined_batch, batch_size=10000, epochs=3)\n",
    "lbfgs.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.save(MLP.state_dict(), \"./model_state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch as th\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.io as sio\n",
    "\n",
    "# from NN import *\n",
    "# from th_operator import calc_grad\n",
    "# from utils import print_model_layers\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import Normalize\n",
    "\n",
    "def contour(x, y, z, title, levels=100):\n",
    "    \"\"\"\n",
    "    Contour plot.\n",
    "    Args:\n",
    "        x: x-array.\n",
    "        y: y-array.\n",
    "        z: z-array.\n",
    "        title: title string.\n",
    "        levels: number of contour lines.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the value range\n",
    "    vmin = np.min(z)\n",
    "    vmax = np.max(z)\n",
    "\n",
    "    # plot a contour\n",
    "    font1 = {'family':'serif','size':20}\n",
    "    plt.contour(x, y, z, colors='k', linewidths=0.2, levels=levels)\n",
    "    contour_filled = plt.contourf(x, y, z, cmap='rainbow', levels=levels, norm=Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "    # Add the circle patch to the current axes without altering the axes limits\n",
    "    circle = plt.Circle((0, 0), 1, color='black')\n",
    "    plt.gca().add_patch(circle)\n",
    "\n",
    "    plt.title(title, fontdict=font1)\n",
    "    plt.xlabel(\"X\", fontdict=font1)\n",
    "    plt.ylabel(\"Y\", fontdict=font1)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(contour_filled, pad=0.03, aspect=25, format='%.0e')\n",
    "    cbar.mappable.set_clim(vmin, vmax)\n",
    "    cbar.ax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2010000, 3])\n"
     ]
    }
   ],
   "source": [
    "# 도메인 및 시간 설정\n",
    "x_min, x_max = -4, 16\n",
    "y_min, y_max = -4, 4\n",
    "t_min, t_max = 0, 20  # 테스트할 시간 범위\n",
    "delta_t = 0.1  # 시간 단위\n",
    "\n",
    "# 공간 그리드 포인트 수\n",
    "num_x, num_y = 100, 100 # 공간 및 시간 축에 대한 포인트 수\n",
    "\n",
    "# 공간 및 시간 축을 위한 그리드 생성\n",
    "x = np.linspace(x_min, x_max, num_x)\n",
    "y = np.linspace(y_min, y_max, num_y)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "test_shape = X.shape\n",
    "time_steps = np.arange(t_min, t_max + delta_t, delta_t)\n",
    "\n",
    "# 초기화\n",
    "test_xyt = np.empty((0, 3), dtype=np.float32)\n",
    "\n",
    "# 각 시간 스텝에 대해 x, y 그리드와 t 값을 결합\n",
    "for t in time_steps:\n",
    "    T = np.full(X.shape, t)\n",
    "    xyt = np.stack([X.ravel(), Y.ravel(), T.ravel()], axis=-1)\n",
    "    test_xyt = np.vstack([test_xyt, xyt])\n",
    "    \n",
    "# PyTorch 텐서로 변환 및 디바이스 할당\n",
    "test_xyt = th.tensor(test_xyt, dtype=th.float32)\n",
    "\n",
    "print(test_xyt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_5608\\2733593040.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xyt = th.tensor(xyt, dtype=th.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "for time_steps, xyt in enumerate(test_xyt.split(10000)):\n",
    "    xx = xyt[..., 0].unsqueeze(-1).reshape(test_shape).numpy()\n",
    "    yy = xyt[..., 1].unsqueeze(-1).reshape(test_shape).numpy()\n",
    "    xyt = th.tensor(xyt, dtype=th.float32).to(device)\n",
    "    MLP.eval()\n",
    "    u_v_p, _ = MLP(xyt)\n",
    "    u, v, p = [ u_v_p[..., i].reshape(test_shape) for i in range(u_v_p.shape[-1]) ]\n",
    "    u = u.detach().cpu().numpy()\n",
    "    v = v.detach().cpu().numpy()\n",
    "    p = p.detach().cpu().numpy()\n",
    "    # compute (u, v)\n",
    "    u = u.reshape(test_shape)\n",
    "    v = v.reshape(test_shape)\n",
    "    p = p.reshape(test_shape)\n",
    "\n",
    "    # plot test results\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    contour(xx, yy, p, f'p_{time_steps:03d}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./p/Pressure_timestep_{time_steps:03d}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    contour(xx, yy, u, f'u_{time_steps:03d}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./u/Vx_timestep_{time_steps:03d}.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    contour(xx, yy, v, f'v_{time_steps:03d}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./v/Vy_timestep_{time_steps:03d}.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "frames = sorted(glob(\"./p/./*.png\"))\n",
    "frame_one = frames[0]\n",
    "frame_one.save(\"figures/convergence.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def make_gif(folder_path, title):\n",
    "    frames = [Image.open(image) for image in sorted(glob(folder_path))]\n",
    "    frame_one = frames[0]\n",
    "    frame_one.save(f\"./{title}.gif\", format=\"GIF\", append_images=frames, save_all=True, duration=100, loop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_gif(\"./p/*.png\", \"p\")\n",
    "make_gif(\"./u/*.png\", \"u\")\n",
    "make_gif(\"./v/*.png\", \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
