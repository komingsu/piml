{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a054aa-6266-482c-98bf-539b1a0e8b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import pyDOE\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43d046d-9c0f-4e83-9843-9ee7cfc4a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# CUDA 사용 가능한지 확인합니다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c9b920-c1df-4dbb-9cc0-6ee06e2dfdac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/train_0.882821876481688_0.6111822228604451_0.7286163354114135_.npy',\n",
       " './data/train_0.9926610113369381_1.0044676920990128_1.0638761184602317_.npy',\n",
       " './data/train_1.0600509553624107_1.80363778828612_0.8063141203855428_.npy',\n",
       " './data/train_0.8056793994939285_1.667088624340879_0.6558453062090482_.npy',\n",
       " './data/train_1.1142205550249682_1.551663418732249_1.8841221939248038_.npy',\n",
       " './data/train_1.0113005341300303_1.1042978763708164_0.9825942665038673_.npy',\n",
       " './data/train_0.7807992483360096_1.2503325654877329_0.6434790402413546_.npy',\n",
       " './data/train_0.7170707652711635_1.2481880765132585_1.5748870026040955_.npy',\n",
       " './data/train_0.564789328106174_1.5367085495967512_1.9498958056687643_.npy',\n",
       " './data/train_1.4414725726351811_0.9122516777264776_1.405664910808274_.npy',\n",
       " './data/train_0.6964663563181073_0.8714126073073516_0.8858933764210435_.npy',\n",
       " './data/train_0.5286829503741566_1.9697624738125876_1.8280660150877082_.npy',\n",
       " './data/train_1.1550933504346717_1.3479038854802354_1.2269056767996454_.npy',\n",
       " './data/train_1.215064271879887_1.033489549337249_1.319033954472093_.npy',\n",
       " './data/train_1.465259158983664_1.8509863151745332_1.7394468677552821_.npy',\n",
       " './data/train_1.2516291665726755_0.6855972644708601_1.1553484825375557_.npy',\n",
       " './data/train_1.3793156162463651_1.4235602716612665_0.5256465964473_.npy',\n",
       " './data/train_0.6461435586370163_0.7519880200471808_1.3459413475493847_.npy',\n",
       " './data/train_0.9481901383571353_1.736580824767281_1.633252245395024_.npy',\n",
       " './data/train_1.3027367182262846_0.5521091050706922_1.4942715403090903_.npy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = glob(\"./data/*.npy\")\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97de1ad7-a190-4f71-a25e-f23aeb317e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with A:0.882821876481688 B:0.6111822228604451 C:0.7286163354114135\n"
     ]
    }
   ],
   "source": [
    "ABC = tasks[0].split(\"_\")[1:4]\n",
    "print(f\"training with A:{ABC[0]} B:{ABC[1]} C:{ABC[2]}\")\n",
    "A = torch.tensor(float(ABC[0]), dtype=torch.float)\n",
    "B = torch.tensor(float(ABC[1]), dtype=torch.float)\n",
    "C = torch.tensor(float(ABC[2]), dtype=torch.float)\n",
    "\n",
    "# 데이터를 준비합니다.\n",
    "with open(tasks[0], 'rb') as f:\n",
    "    data = np.load(f,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c985062e-f61a-4fef-9513-67f276774886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8828],\n",
       "        [0.8828],\n",
       "        [0.8828],\n",
       "        ...,\n",
       "        [0.8828],\n",
       "        [0.8828],\n",
       "        [0.8828]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AA.reshape(-1,1).repeat(5000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f615f4-794a-452f-bcec-c398f672e2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_derivatives(model, xy_data):\n",
    "    xy_data = xy_data.to(device)\n",
    "    # Ensure that xy_data has gradient information.\n",
    "    xy_data.requires_grad_(True)\n",
    "    \n",
    "    # Get the model prediction.\n",
    "    f_pred = model(xy_data)\n",
    "    \n",
    "    # Create a tensor of ones with the same shape as f_pred to be used for gradient computation.\n",
    "    # Reshape the ones tensor to match the shape of f_pred.\n",
    "    ones = torch.ones(f_pred.shape, device=device, requires_grad=False)\n",
    "    \n",
    "    # Compute the first derivatives.\n",
    "    f_x = torch.autograd.grad(f_pred, xy_data, grad_outputs=ones, create_graph=True)[0][:, 0]\n",
    "    f_y = torch.autograd.grad(f_pred, xy_data, grad_outputs=ones, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    # Compute the second derivatives.\n",
    "    f_xx = torch.autograd.grad(f_x, xy_data, grad_outputs=ones[:, 0], create_graph=True)[0][:, 0]\n",
    "    f_yy = torch.autograd.grad(f_y, xy_data, grad_outputs=ones[:, 0], create_graph=True)[0][:, 1]\n",
    "    \n",
    "    return f_xx, f_yy\n",
    "\n",
    "# 손실 함수를 정의합니다.\n",
    "def pinn_loss(model, criterion, xy_data, f_data, alpha=0.1, A=1,B=1,C=1):\n",
    "    f_pred = model(xy_data)\n",
    "    data_loss = criterion(f_pred, f_data)\n",
    "    f_xx, f_yy = compute_derivatives(model, xy_data)\n",
    "    pde_loss = criterion(f_xx + f_yy, -(B*B+C*C)*f_pred.squeeze())\n",
    "    return data_loss + alpha * pde_loss\n",
    "\n",
    "# 배치학습을 위한 데이터 로더 함수를 정의합니다.\n",
    "def create_dataloader(x_data, y_data, batch_size, shuffle):\n",
    "    dataset = TensorDataset(x_data, y_data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader\n",
    "\n",
    "def train_model(model, epochs, A, B, C):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = 0.0\n",
    "        batch_data_loss = 0.0  # To record data loss\n",
    "        batch_pde_loss = 0.0   # To record pde loss\n",
    "\n",
    "        for batch_xy, batch_f in loader:\n",
    "            batch_xy, batch_f = batch_xy.to(device), batch_f.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the losses\n",
    "            f_pred = model(batch_xy)\n",
    "            data_loss = criterion(f_pred, batch_f)\n",
    "            f_xx, f_yy = compute_derivatives(model, batch_xy)\n",
    "            pde_loss = criterion(f_xx + f_yy, -(B*B+C*C)*f_pred.squeeze())\n",
    "\n",
    "            # Combine the losses\n",
    "            loss = data_loss + alpha * pde_loss\n",
    "\n",
    "            # Backpropagate and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record losses\n",
    "            batch_loss += loss.item()\n",
    "            batch_data_loss += data_loss.item()\n",
    "            batch_pde_loss += pde_loss.item()\n",
    "\n",
    "        avg_loss = batch_loss / len(loader)\n",
    "        avg_data_loss = batch_data_loss / len(loader)\n",
    "        avg_pde_loss = batch_pde_loss / len(loader)\n",
    "\n",
    "        # Append the average losses for this epoch to the history\n",
    "        loss_history.append(avg_loss)\n",
    "        data_loss_history.append(avg_data_loss)\n",
    "        pde_loss_history.append(avg_pde_loss)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Print the losses every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch}/{epochs}, Total Loss: {avg_loss}, Data Loss: {avg_data_loss}, PDE Loss: {avg_pde_loss}, LR: {current_lr}')\n",
    "\n",
    "    # 손실값 그래프를 그립니다.\n",
    "    print(\"total loss : \",loss_history[-1])\n",
    "    print(\"DATA  loss : \",data_loss_history[-1])\n",
    "    print(\"PDE   loss : \",pde_loss_history[-1])\n",
    "    plt.semilogy(loss_history, label=\"Total\")\n",
    "    plt.semilogy(data_loss_history, label=\"Data\")\n",
    "    plt.semilogy(pde_loss_history, label=\"PDE\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Epoch vs loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def test_inference(model, test_data_path):\n",
    "    with open(test_data_path, 'rb') as f:\n",
    "        data = np.load(f,allow_pickle=True)\n",
    "    x = data.T[0]\n",
    "    y = data.T[1]\n",
    "    X = data[:,:2]\n",
    "    f_true = data.T[2]\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    X = X.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        f_pred = model(X).cpu().numpy().squeeze()\n",
    "\n",
    "    loss = np.mean(np.sqrt(np.abs(np.square(f_true.reshape(-1))-np.square(f_pred.reshape(-1)))))\n",
    "    print(\"Test MSE Loss : \", loss)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\n",
    "    # grid\n",
    "    xi, yi = np.mgrid[x.min():x.max():500j, y.min():y.max():500j]\n",
    "\n",
    "    # f_true figure\n",
    "    rbf = scipy.interpolate.Rbf(x, y, f_true)\n",
    "    org = rbf(xi, yi)\n",
    "    img = ax[0].imshow(org.T, origin='lower',extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "    ax[0].scatter(x, y, c=f_true)\n",
    "    ax[0].set(xlabel='X', ylabel='Y')\n",
    "    fig.colorbar(img,shrink=0.9)\n",
    "\n",
    "    # f_pred figure\n",
    "    rbf = scipy.interpolate.Rbf(x, y, f_pred)\n",
    "    org = rbf(xi, yi)\n",
    "    img = ax[1].imshow(org.T, origin='lower',extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "    ax[1].scatter(x, y, c=f_pred)\n",
    "    ax[1].set(xlabel='X', ylabel='Y')\n",
    "    fig.colorbar(img,shrink=0.9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d333f-8bff-44b9-9e5c-e68002799559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe30b0-0f98-44a3-afbf-4bfadd336e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd802b-b8e8-4954-b510-69e154e82af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piad2d(args):\n",
    "    \"\"\"pretraining and reconstruction process\"\"\"\n",
    "    config = preprocess_config(args)\n",
    "    train_dataset = create_random_dataset(config)\n",
    "    train_dataset = train_dataset.create_dataset(batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                                                 prebatched_data=True, drop_remainder=True)\n",
    "    epoch_steps = len(train_dataset)\n",
    "    print(\"check train dataset size: \", len(train_dataset))\n",
    "    # load ckpt\n",
    "    if config.get(\"load_ckpt\", False):\n",
    "        param_dict = load_checkpoint(config[\"load_ckpt_path\"])\n",
    "        if args.mode == \"pretrain\":\n",
    "            loaded_ckpt_dict = param_dict\n",
    "        else:\n",
    "            loaded_ckpt_dict, latent_vector_ckpt = {}, 0\n",
    "            for name in param_dict:\n",
    "                if name == \"model.latent_vector\":\n",
    "                    latent_vector_ckpt = param_dict[name].data.asnumpy()\n",
    "                elif \"network\" in name and \"moment\" not in name:\n",
    "                    loaded_ckpt_dict[name] = param_dict[name]\n",
    "    # initialize latent vector\n",
    "    num_scenarios, latent_size = config[\"num_scenarios\"], config[\"latent_vector_size\"]\n",
    "    latent_vector = calc_latent_init(latent_size, latent_vector_ckpt, args.mode, num_scenarios)\n",
    "    network = MultiScaleFCCell(config[\"input_size\"], config[\"output_size\"],\n",
    "                               layers=config[\"layers\"], neurons=config[\"neurons\"], residual=config[\"residual\"],\n",
    "                               weight_init=HeUniform(negative_slope=math.sqrt(5)), act=\"sin\",\n",
    "                               num_scales=config[\"num_scales\"], amp_factor=config[\"amp_factor\"],\n",
    "                               scale_factor=config[\"scale_factor\"], input_scale=config[\"input_scale\"],\n",
    "                               input_center=config[\"input_center\"], latent_vector=latent_vector)\n",
    "    network = network.to_float(ms.float16)\n",
    "    network.input_scale.to_float(ms.float32)\n",
    "    mtl_cell = MTLWeightedLossCell(num_losses=train_dataset.num_dataset) if config.get(\"enable_mtl\", True) else None\n",
    "    # define problem\n",
    "    train_prob = {}\n",
    "    for dataset in train_dataset.all_datasets:\n",
    "        train_prob[dataset.name] = Maxwell2DMur(network=network, config=config, domain_column=dataset.name + \"_points\",\n",
    "                                                ic_column=dataset.name + \"_points\", bc_column=dataset.name + \"_points\")\n",
    "    print(\"check problem: \", train_prob)\n",
    "    train_constraints = Constraints(train_dataset, train_prob)\n",
    "    # optimizer\n",
    "    params = load_net(args, config, loaded_ckpt_dict, mtl_cell, network)\n",
    "    lr_scheduler = MultiStepLR(config[\"lr\"], config[\"milestones\"], config[\"lr_gamma\"], epoch_steps,\n",
    "                               config[\"train_epoch\"])\n",
    "    optimizer = nn.Adam(params, learning_rate=Tensor(lr_scheduler.get_lr()))\n",
    "    # problem solver\n",
    "    solver = Solver(network, optimizer=optimizer, mode=\"PINNs\", train_constraints=train_constraints,\n",
    "                    test_constraints=None, metrics={'l2': L2(), 'distance': nn.MAE()}, loss_fn='smooth_l1_loss',\n",
    "                    loss_scale_manager=DynamicLossScaleManager(), mtl_weighted_cell=mtl_cell,\n",
    "                    latent_vector=latent_vector, latent_reg=config[\"latent_reg\"])\n",
    "    callbacks = get_callbacks(args, config, epoch_steps, network)\n",
    "    solver.train(config[\"train_epoch\"], train_dataset, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
