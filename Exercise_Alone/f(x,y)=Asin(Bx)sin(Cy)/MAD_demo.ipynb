{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a054aa-6266-482c-98bf-539b1a0e8b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import pyDOE\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43d046d-9c0f-4e83-9843-9ee7cfc4a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# CUDA 사용 가능한지 확인합니다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4bc0d84-51f6-4ab6-8e94-26c04818d342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터를 준비합니다.\n",
    "with open(\"./data/trainXYFABC.npy\", 'rb') as f:\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f615f4-794a-452f-bcec-c398f672e2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_derivatives(model, xy_data):\n",
    "    xy_data = xy_data.to(device)\n",
    "    # Ensure that xy_data has gradient information.\n",
    "    xy_data.requires_grad_(True)\n",
    "    \n",
    "    # Get the model prediction.\n",
    "    f_pred = model(xy_data)\n",
    "    \n",
    "    # Create a tensor of ones with the same shape as f_pred to be used for gradient computation.\n",
    "    # Reshape the ones tensor to match the shape of f_pred.\n",
    "    ones = torch.ones(f_pred.shape, device=device, requires_grad=False)\n",
    "    \n",
    "    # Compute the first derivatives.\n",
    "    f_x = torch.autograd.grad(f_pred, xy_data, grad_outputs=ones, create_graph=True)[0][:, 0]\n",
    "    f_y = torch.autograd.grad(f_pred, xy_data, grad_outputs=ones, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    # Compute the second derivatives.\n",
    "    f_xx = torch.autograd.grad(f_x, xy_data, grad_outputs=ones[:, 0], create_graph=True)[0][:, 0]\n",
    "    f_yy = torch.autograd.grad(f_y, xy_data, grad_outputs=ones[:, 0], create_graph=True)[0][:, 1]\n",
    "    \n",
    "    return f_xx, f_yy\n",
    "\n",
    "# 손실 함수를 정의합니다.\n",
    "def pinn_loss(model, criterion, xy_data, f_data, alpha=0.1, A=1,B=1,C=1):\n",
    "    f_pred = model(xy_data)\n",
    "    data_loss = criterion(f_pred, f_data)\n",
    "    f_xx, f_yy = compute_derivatives(model, xy_data)\n",
    "    pde_loss = criterion(f_xx + f_yy, -(B*B+C*C)*f_pred.squeeze())\n",
    "    return data_loss + alpha * pde_loss\n",
    "\n",
    "# 배치학습을 위한 데이터 로더 함수를 정의합니다.\n",
    "def create_dataloader(x_data, y_data, batch_size, shuffle):\n",
    "    dataset = TensorDataset(x_data, y_data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return tqdm(loader, desc=\"Training\", leave=False)\n",
    "\n",
    "def train_model(model, epochs, A, B, C):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = 0.0\n",
    "        batch_data_loss = 0.0  # To record data loss\n",
    "        batch_pde_loss = 0.0   # To record pde loss\n",
    "\n",
    "        for batch_xy, batch_f in loader:\n",
    "            batch_xy, batch_f = batch_xy.to(device), batch_f.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the losses\n",
    "            f_pred = model(batch_xy)\n",
    "            data_loss = criterion(f_pred, batch_f)\n",
    "            f_xx, f_yy = compute_derivatives(model, batch_xy)\n",
    "            pde_loss = criterion(f_xx + f_yy, -(B*B+C*C)*f_pred.squeeze())\n",
    "\n",
    "            # Combine the losses\n",
    "            loss = data_loss + alpha * pde_loss\n",
    "\n",
    "            # Backpropagate and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record losses\n",
    "            batch_loss += loss.item()\n",
    "            batch_data_loss += data_loss.item()\n",
    "            batch_pde_loss += pde_loss.item()\n",
    "\n",
    "        avg_loss = batch_loss / len(loader)\n",
    "        avg_data_loss = batch_data_loss / len(loader)\n",
    "        avg_pde_loss = batch_pde_loss / len(loader)\n",
    "\n",
    "        # Append the average losses for this epoch to the history\n",
    "        loss_history.append(avg_loss)\n",
    "        data_loss_history.append(avg_data_loss)\n",
    "        pde_loss_history.append(avg_pde_loss)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Print the losses every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch}/{epochs}, Total Loss: {avg_loss}, Data Loss: {avg_data_loss}, PDE Loss: {avg_pde_loss}, LR: {current_lr}')\n",
    "\n",
    "    # 손실값 그래프를 그립니다.\n",
    "    print(\"total loss : \",loss_history[-1])\n",
    "    print(\"DATA  loss : \",data_loss_history[-1])\n",
    "    print(\"PDE   loss : \",pde_loss_history[-1])\n",
    "    plt.semilogy(loss_history, label=\"Total\")\n",
    "    plt.semilogy(data_loss_history, label=\"Data\")\n",
    "    plt.semilogy(pde_loss_history, label=\"PDE\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Epoch vs loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def test_inference(model, test_data_path):\n",
    "    with open(test_data_path, 'rb') as f:\n",
    "        data = np.load(f,allow_pickle=True)\n",
    "    x = data.T[0]\n",
    "    y = data.T[1]\n",
    "    X = data[:,:2]\n",
    "    f_true = data.T[2]\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    X = X.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        f_pred = model(X).cpu().numpy().squeeze()\n",
    "\n",
    "    loss = np.mean(np.sqrt(np.abs(np.square(f_true.reshape(-1))-np.square(f_pred.reshape(-1)))))\n",
    "    print(\"Test MSE Loss : \", loss)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\n",
    "    # grid\n",
    "    xi, yi = np.mgrid[x.min():x.max():500j, y.min():y.max():500j]\n",
    "\n",
    "    # f_true figure\n",
    "    rbf = scipy.interpolate.Rbf(x, y, f_true)\n",
    "    org = rbf(xi, yi)\n",
    "    img = ax[0].imshow(org.T, origin='lower',extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "    ax[0].scatter(x, y, c=f_true)\n",
    "    ax[0].set(xlabel='X', ylabel='Y')\n",
    "    fig.colorbar(img,shrink=0.9)\n",
    "\n",
    "    # f_pred figure\n",
    "    rbf = scipy.interpolate.Rbf(x, y, f_pred)\n",
    "    org = rbf(xi, yi)\n",
    "    img = ax[1].imshow(org.T, origin='lower',extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "    ax[1].scatter(x, y, c=f_pred)\n",
    "    ax[1].set(xlabel='X', ylabel='Y')\n",
    "    fig.colorbar(img,shrink=0.9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c22b241-b235-43b6-95e4-4719e17a1371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "batch_size = 5000\n",
    "shuffle = True # 데이터 셔플\n",
    "epochs = 2000   # 훈련 epoch\n",
    "alpha=0.5      # Total Loss = Data Loss + alpha * PDE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c81e58-b095-4a91-9689-69f680f37c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 데이터를 준비합니다.\n",
    "with open(\"./data/trainXYFABC.npy\", 'rb') as f:\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "f.close()\n",
    "\n",
    "x_data = data.T[0]\n",
    "y_data = data.T[1]\n",
    "f_data = data.T[2]\n",
    "AA = data.T[3]\n",
    "BB = data.T[4]\n",
    "CC = data.T[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1081ac17-1827-4c0a-ac2e-896ca62e195b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([x_data,y_data], axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d333f-8bff-44b9-9e5c-e68002799559",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_data = np.stack([x_data.ravel(), y_data.ravel()], axis=-1)\n",
    "xy_data = torch.tensor(xy_data, dtype=torch.float)\n",
    "f_data = torch.tensor(f_data.ravel(), dtype=torch.float).view(-1, 1)\n",
    "\n",
    "A = A.to(device)\n",
    "B = B.to(device)\n",
    "C = C.to(device)\n",
    "# 데이터 로더를 생성합니다.\n",
    "loader = create_dataloader(xy_data, f_data, batch_size, shuffle)\n",
    "\n",
    "# 모델을 GPU로 이동합니다(만약 사용 가능하다면).\n",
    "model = NN(mask=True)\n",
    "model.load_state_dict(torch.load(\"./data/model.pt\"))\n",
    "model = model.to(device)\n",
    "# loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10, verbose=False)\n",
    "\n",
    "# init\n",
    "loss_history = []\n",
    "data_loss_history = []\n",
    "pde_loss_history = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "#### 훈련 시작 ####\n",
    "\n",
    "model = train_model(model, epochs, A=A, B=B, C=C)\n",
    "print(\"Train end\\n\\n\")\n",
    "loss = test_inference(model, task)\n",
    "if loss <= 0.2:\n",
    "    torch.save(model.state_dict(), f\"./data/model_{ABC[0]}_{ABC[1]}_{ABC[2]}_.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe30b0-0f98-44a3-afbf-4bfadd336e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd802b-b8e8-4954-b510-69e154e82af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piad2d(args):\n",
    "    \"\"\"pretraining and reconstruction process\"\"\"\n",
    "    config = preprocess_config(args)\n",
    "    train_dataset = create_random_dataset(config)\n",
    "    train_dataset = train_dataset.create_dataset(batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                                                 prebatched_data=True, drop_remainder=True)\n",
    "    epoch_steps = len(train_dataset)\n",
    "    print(\"check train dataset size: \", len(train_dataset))\n",
    "    # load ckpt\n",
    "    if config.get(\"load_ckpt\", False):\n",
    "        param_dict = load_checkpoint(config[\"load_ckpt_path\"])\n",
    "        if args.mode == \"pretrain\":\n",
    "            loaded_ckpt_dict = param_dict\n",
    "        else:\n",
    "            loaded_ckpt_dict, latent_vector_ckpt = {}, 0\n",
    "            for name in param_dict:\n",
    "                if name == \"model.latent_vector\":\n",
    "                    latent_vector_ckpt = param_dict[name].data.asnumpy()\n",
    "                elif \"network\" in name and \"moment\" not in name:\n",
    "                    loaded_ckpt_dict[name] = param_dict[name]\n",
    "    # initialize latent vector\n",
    "    num_scenarios, latent_size = config[\"num_scenarios\"], config[\"latent_vector_size\"]\n",
    "    latent_vector = calc_latent_init(latent_size, latent_vector_ckpt, args.mode, num_scenarios)\n",
    "    network = MultiScaleFCCell(config[\"input_size\"], config[\"output_size\"],\n",
    "                               layers=config[\"layers\"], neurons=config[\"neurons\"], residual=config[\"residual\"],\n",
    "                               weight_init=HeUniform(negative_slope=math.sqrt(5)), act=\"sin\",\n",
    "                               num_scales=config[\"num_scales\"], amp_factor=config[\"amp_factor\"],\n",
    "                               scale_factor=config[\"scale_factor\"], input_scale=config[\"input_scale\"],\n",
    "                               input_center=config[\"input_center\"], latent_vector=latent_vector)\n",
    "    network = network.to_float(ms.float16)\n",
    "    network.input_scale.to_float(ms.float32)\n",
    "    mtl_cell = MTLWeightedLossCell(num_losses=train_dataset.num_dataset) if config.get(\"enable_mtl\", True) else None\n",
    "    # define problem\n",
    "    train_prob = {}\n",
    "    for dataset in train_dataset.all_datasets:\n",
    "        train_prob[dataset.name] = Maxwell2DMur(network=network, config=config, domain_column=dataset.name + \"_points\",\n",
    "                                                ic_column=dataset.name + \"_points\", bc_column=dataset.name + \"_points\")\n",
    "    print(\"check problem: \", train_prob)\n",
    "    train_constraints = Constraints(train_dataset, train_prob)\n",
    "    # optimizer\n",
    "    params = load_net(args, config, loaded_ckpt_dict, mtl_cell, network)\n",
    "    lr_scheduler = MultiStepLR(config[\"lr\"], config[\"milestones\"], config[\"lr_gamma\"], epoch_steps,\n",
    "                               config[\"train_epoch\"])\n",
    "    optimizer = nn.Adam(params, learning_rate=Tensor(lr_scheduler.get_lr()))\n",
    "    # problem solver\n",
    "    solver = Solver(network, optimizer=optimizer, mode=\"PINNs\", train_constraints=train_constraints,\n",
    "                    test_constraints=None, metrics={'l2': L2(), 'distance': nn.MAE()}, loss_fn='smooth_l1_loss',\n",
    "                    loss_scale_manager=DynamicLossScaleManager(), mtl_weighted_cell=mtl_cell,\n",
    "                    latent_vector=latent_vector, latent_reg=config[\"latent_reg\"])\n",
    "    callbacks = get_callbacks(args, config, epoch_steps, network)\n",
    "    solver.train(config[\"train_epoch\"], train_dataset, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
