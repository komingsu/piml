{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca40ce5-eb3a-4fb2-b9c6-71d424c68b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29b5b40-2b98-43d3-b878-d85fb063c1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize LayerPerceptron\n",
    "        \"\"\"\n",
    "        super(LayerPerceptron,self).__init__()\n",
    "        self.dense      = nn.Linear(in_features=in_dim,\n",
    "                                    out_features=out_dim,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "        self.activation = act\n",
    "        # Initialize parameters\n",
    "        self.init_param(w_init, b_init)\n",
    "\n",
    "    def init_param(self, w_init, b_init):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        if w_init:\n",
    "            nn.init.constant_(self.dense.weight, w_init)\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.dense.weight,a=math.sqrt(5))\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(self.dense(x))\n",
    "        else:\n",
    "            out = self.dense(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8199aa-c22b-4d8a-a5f2-a1fd52a9e25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize ResidualLayerPerceptron\n",
    "        \"\"\"\n",
    "        super(ResidualLayerPerceptron,self).__init__()\n",
    "\n",
    "        if in_dim != out_dim:\n",
    "            raise ValueError(\"in_dim of ResBlock should be equal of out_dim, but got in_dim: {}, \"\n",
    "                             \"out_dim: {}\".format(in_dim, out_dim))\n",
    "\n",
    "        self.dense      = nn.Linear(in_features=in_dim,\n",
    "                                    out_features=out_dim,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "        self.activation = act\n",
    "        # Initialize parameters\n",
    "        self.init_param(w_init, b_init)\n",
    "\n",
    "    def init_param(self, w_init, b_init):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        if w_init:\n",
    "            nn.init.constant_(self.dense.weight, w_init)\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.dense.weight,a=math.sqrt(5))\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = self.activation(self.dense(x)+x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55761f49-bee9-4169-a1e2-0983bb8005d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPSequential(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        layers     = None,\n",
    "        neurons    = None,\n",
    "        residual   = False,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MLPSequential\n",
    "        \"\"\"\n",
    "        super(MLPSequential,self).__init__()\n",
    "        if layers < 3:\n",
    "            raise ValueError(\"MLPSequential have at least 3 layers, but got layers: {}\".format(layers))\n",
    "\n",
    "        # Define net\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add_module(\"input\", LayerPerceptron(in_dim=in_dim,\n",
    "                                                           out_dim=neurons,\n",
    "                                                           w_init=w_init,\n",
    "                                                           b_init=b_init,\n",
    "                                                           act=act,\n",
    "                                                           ))\n",
    "        for idx in range(layers - 2):\n",
    "            if residual:\n",
    "                self.net.add_module(f\"res_{idx+1:02d}\", ResidualLayerPerceptron(in_dim=neurons,\n",
    "                                                                                     out_dim=neurons,\n",
    "                                                                                     w_init=w_init,\n",
    "                                                                                     b_init=b_init,\n",
    "                                                                                     act=act,\n",
    "                                                                                     ))\n",
    "            else:\n",
    "                self.net.add_module(f\"mlp_{idx+1:02d}\", LayerPerceptron(in_dim=neurons,\n",
    "                                                                        out_dim=neurons,\n",
    "                                                                        w_init=w_init,\n",
    "                                                                        b_init=b_init,\n",
    "                                                                        act=act,\n",
    "                                                                        ))\n",
    "\n",
    "        self.net.add_module(\"output\", LayerPerceptron(in_dim=neurons,\n",
    "                                                      out_dim=out_dim,\n",
    "                                                      w_init=w_init,\n",
    "                                                      b_init=b_init,\n",
    "                                                      act=None,\n",
    "                                                      ))       \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56b667b-dba1-41b3-ab06-e91b288ac0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputScaleNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scales     = [],\n",
    "        centers    = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize InputScaleNet\n",
    "        \"\"\"\n",
    "        super(InputScaleNet,self).__init__()\n",
    "        self.scales     = torch.from_numpy(np.array(scales)).type(torch.float)\n",
    "        if centers is None:\n",
    "            self.centers = torch.zeros_like(self.scales, dtype=torch.float)\n",
    "        else:\n",
    "            self.centers = torch.from_numpy(np.array(centers)).type(torch.float)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = torch.mul(x - self.centers, self.scales)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e255b3f-19b0-4665-864b-b96adbc12e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleMLPSequential(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        layers     = None,\n",
    "        neurons    = None,\n",
    "        residual   = False, # use residual | [True, False]\n",
    "        w_init     = False, # constant std | (ex 0.1, 0.01)\n",
    "        b_init     = False, # use bias | [True, False]\n",
    "        act        = nn.Tanh(),\n",
    "        subnets    = None,  # subnet(multi scale) number\n",
    "        amp        = 1.0,   # amplification factor of input\n",
    "        base_scale = 2.0,   # base scale factor\n",
    "        in_scale   = None,  # scale factor of input (ex [x,y,t])\n",
    "        in_center  = None,  # Center position of coordinate translation (ex [x,y,t])\n",
    "        latent_vec = None,  # latent vector (ex Tensor[shape=(4,16)] )\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MultiScaleMLPSequential\n",
    "        \"\"\"\n",
    "        super(MultiScaleMLPSequential,self).__init__()\n",
    "        self.subnets = subnets\n",
    "        self.scale_coef = [amp * (base_scale**i) for i in range(self.subnets)]\n",
    "        self.latent_vec = latent_vec\n",
    "        \n",
    "        if self.latent_vec is not None:\n",
    "            self.num_scenarios = latent_vec.shape[0]\n",
    "            self.latent_size = latent_vec.shape[1]\n",
    "            in_dim += self.latent_size\n",
    "        else:\n",
    "            self.num_scenarios = 1\n",
    "            self.latent_size = 0\n",
    "\n",
    "        # Define MultiScaleMLP\n",
    "        self.msnet = nn.Sequential()\n",
    "        for i in range(self.subnets):\n",
    "            self.msnet.add_module(f\"Scale_{i+1}_Net\",MLPSequential(in_dim=in_dim,\n",
    "                                                               out_dim=out_dim,\n",
    "                                                               layers=layers,\n",
    "                                                               neurons=neurons,\n",
    "                                                               residual=residual,\n",
    "                                                               w_init=w_init,\n",
    "                                                               b_init=b_init,\n",
    "                                                               act=act,\n",
    "                                                               ))\n",
    "        if in_scale:\n",
    "            self.in_scale = InputScaleNet(in_scale, in_center)\n",
    "        else:\n",
    "            self.in_scale = torch.nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        x = self.in_scale(x)\n",
    "        if self.latent_vec is not None:\n",
    "            batch_size = x.shape[0]\n",
    "            latent_vectors = self.latent_vec.view(self.num_scenarios, 1, self.latent_size).repeat(1,batch_size//self.num_scenarios,1).view(-1,self.latent_size)\n",
    "            x = torch.concat([x, latent_vectors], axis=1)\n",
    "        \n",
    "        out = 0\n",
    "        for i in range(self.subnets):\n",
    "            x_s = x * self.scale_coef[i]\n",
    "            out = out + self.msnet[i](x_s)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480da4e7-d519-4b57-b0ed-8c7de7a8d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L1 = LayerPerceptron(in_dim=3,\n",
    "               out_dim=64)\n",
    "L2 = ResidualLayerPerceptron(in_dim=64,\n",
    "                            out_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07d5534-d614-4f0c-8020-8f76911edeca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 0.3000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.from_numpy(np.array([0.0, 0.5, 0.3])).type(torch.float)\n",
    "input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b019dd-fc94-4a4a-bc75-870723f6fe58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1224,  0.0877, -0.0410,  0.2476, -0.0109,  0.2787,  0.3345,  0.1550,\n",
      "         0.1737,  0.3972, -0.0446, -0.0885,  0.2446, -0.2994,  0.2292, -0.0351,\n",
      "        -0.0681,  0.2415,  0.0074,  0.0759,  0.1249, -0.3604, -0.1365, -0.1692,\n",
      "         0.1675,  0.0378, -0.1753,  0.1410, -0.0211,  0.1218,  0.0643, -0.0088,\n",
      "         0.1628,  0.2645, -0.1146, -0.1899, -0.1624,  0.4539,  0.0374,  0.0473,\n",
      "         0.0303, -0.0399,  0.1481, -0.0997,  0.0590, -0.1565,  0.1017,  0.1456,\n",
      "         0.4167, -0.0896,  0.2215, -0.1190, -0.0468, -0.1906, -0.1803, -0.2644,\n",
      "        -0.0810, -0.1746, -0.4509,  0.1829, -0.0760,  0.3074, -0.0942, -0.1054],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "tensor([-0.2390, -0.0173, -0.2266,  0.1401,  0.1874,  0.4809,  0.4530, -0.0850,\n",
      "         0.2399,  0.5614,  0.1005, -0.0180,  0.3894, -0.2739,  0.2445,  0.0274,\n",
      "        -0.2036,  0.1456,  0.0085,  0.1595,  0.2854, -0.4788, -0.1331, -0.3290,\n",
      "        -0.0808,  0.0808, -0.0932,  0.2495, -0.0165,  0.2511, -0.0500, -0.1102,\n",
      "         0.0896,  0.2501, -0.0972, -0.2860, -0.0404,  0.4554, -0.0108,  0.2709,\n",
      "         0.0871, -0.0576,  0.1545,  0.1592,  0.0861, -0.2355,  0.1999, -0.0550,\n",
      "         0.4153, -0.2650,  0.2873,  0.0050, -0.0321, -0.2477, -0.2173, -0.3279,\n",
      "        -0.0847, -0.1424, -0.5900,  0.4300,  0.0142,  0.3138, -0.1256, -0.1497],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "L1_result = L1(input_sample)\n",
    "L2_result = L2(L1_result)\n",
    "\n",
    "print(L1_result)\n",
    "print(L2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d179fd74-6242-413c-b863-4b44ff5e1c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MLPSequential(\n",
    "        in_dim     = 3,\n",
    "        out_dim    = 1,\n",
    "        layers     = 5,\n",
    "        neurons    = 64,\n",
    "        residual   = False,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a82f24-1706-455a-9095-46983d5c306b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPSequential(\n",
       "  (net): Sequential(\n",
       "    (input): LayerPerceptron(\n",
       "      (dense): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_01): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_02): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_03): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (output): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e434a1a-6ff7-495d-9d16-82318a751bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0006], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b788db-782d-4178-adee-8ca9dc1e4440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_norm = InputScaleNet(scales=[2.0],\n",
    "                          centers=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da37dd5f-c628-4ebf-969f-74496ece217a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0000, -1.0000, -1.4000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_norm(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "057f0613-7667-4517-83eb-c64f26fa57a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MultiScaleMLPSequential(\n",
    "    in_dim=3,\n",
    "    out_dim=1,\n",
    "    layers=7,\n",
    "    neurons=32,\n",
    "    residual=True,\n",
    "    subnets=3,\n",
    "    in_scale= [1/5.0,1/20.0,1/100.0],\n",
    "    in_center=[3.0, 10.0, -15.0],\n",
    "    latent_vec= torch.from_numpy(np.random.randn(4, 12) / np.sqrt(12)).type(torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b051d771-1b24-4239-90a3-8c5ed9e6b679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Scale_1_Net): MLPSequential(\n",
       "    (net): Sequential(\n",
       "      (input): LayerPerceptron(\n",
       "        (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_01): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_02): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_03): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_04): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_05): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (output): LayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Scale_2_Net): MLPSequential(\n",
       "    (net): Sequential(\n",
       "      (input): LayerPerceptron(\n",
       "        (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_01): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_02): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_03): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_04): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_05): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (output): LayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Scale_3_Net): MLPSequential(\n",
       "    (net): Sequential(\n",
       "      (input): LayerPerceptron(\n",
       "        (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_01): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_02): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_03): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_04): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (res_05): ResidualLayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "      (output): LayerPerceptron(\n",
       "        (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.msnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48527c5e-f9b6-4d02-a0c6-8da126f47baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.rand(size=(16,3), dtype=torch.float)\n",
    "input_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8142e0b5-2554-43d5-88f2-040ea6638eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0119],\n",
       "        [ 0.0034],\n",
       "        [-0.0125],\n",
       "        [-0.0151],\n",
       "        [-0.5644],\n",
       "        [-0.5729],\n",
       "        [-0.5750],\n",
       "        [-0.5479],\n",
       "        [-0.1439],\n",
       "        [-0.1499],\n",
       "        [-0.1491],\n",
       "        [-0.1408],\n",
       "        [-0.4426],\n",
       "        [-0.3898],\n",
       "        [-0.4611],\n",
       "        [-0.4404]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe46c33-054f-4817-862b-973312b43db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
