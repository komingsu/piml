{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca40ce5-eb3a-4fb2-b9c6-71d424c68b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29b5b40-2b98-43d3-b878-d85fb063c1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize LayerPerceptron\n",
    "        \"\"\"\n",
    "        super(LayerPerceptron,self).__init__()\n",
    "        self.dense      = nn.Linear(in_features=in_dim,\n",
    "                                    out_features=out_dim,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "        self.activation = act\n",
    "        # Initialize parameters\n",
    "        self.init_param(w_init, b_init)\n",
    "\n",
    "    def init_param(self, w_init, b_init):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        if w_init:\n",
    "            nn.init.constant_(self.dense.weight, w_init)\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.dense.weight,a=math.sqrt(5))\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(self.dense(x))\n",
    "        else:\n",
    "            out = self.dense(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8199aa-c22b-4d8a-a5f2-a1fd52a9e25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize ResidualLayerPerceptron\n",
    "        \"\"\"\n",
    "        super(ResidualLayerPerceptron,self).__init__()\n",
    "\n",
    "        if in_dim != out_dim:\n",
    "            raise ValueError(\"in_dim of ResBlock should be equal of out_dim, but got in_dim: {}, \"\n",
    "                             \"out_dim: {}\".format(in_dim, out_dim))\n",
    "\n",
    "        self.dense      = nn.Linear(in_features=in_dim,\n",
    "                                    out_features=out_dim,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "        self.activation = act\n",
    "        # Initialize parameters\n",
    "        self.init_param(w_init, b_init)\n",
    "\n",
    "    def init_param(self, w_init, b_init):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        if w_init:\n",
    "            nn.init.constant_(self.dense.weight, w_init)\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.dense.weight,a=math.sqrt(5))\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = self.activation(self.dense(x)+x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55761f49-bee9-4169-a1e2-0983bb8005d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPSequential(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        layers     = None,\n",
    "        neurons    = None,\n",
    "        residual   = False,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MLPSequential\n",
    "        \"\"\"\n",
    "        super(MLPSequential,self).__init__()\n",
    "        if layers < 3:\n",
    "            raise ValueError(\"MLPSequential have at least 3 layers, but got layers: {}\".format(layers))\n",
    "\n",
    "        # Define net\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add_module(\"input\", LayerPerceptron(in_dim=in_dim,\n",
    "                                                           out_dim=neurons,\n",
    "                                                           w_init=w_init,\n",
    "                                                           b_init=b_init,\n",
    "                                                           act=act,\n",
    "                                                           ))\n",
    "        for idx in range(layers - 2):\n",
    "            if residual:\n",
    "                self.net.add_module(f\"res_{idx+1:02d}\", ResidualLayerPerceptron(in_dim=neurons,\n",
    "                                                                                     out_dim=neurons,\n",
    "                                                                                     w_init=w_init,\n",
    "                                                                                     b_init=b_init,\n",
    "                                                                                     act=act,\n",
    "                                                                                     ))\n",
    "            else:\n",
    "                self.net.add_module(f\"mlp_{idx+1:02d}\", LayerPerceptron(in_dim=neurons,\n",
    "                                                                        out_dim=neurons,\n",
    "                                                                        w_init=w_init,\n",
    "                                                                        b_init=b_init,\n",
    "                                                                        act=act,\n",
    "                                                                        ))\n",
    "\n",
    "        self.net.add_module(\"output\", LayerPerceptron(in_dim=neurons,\n",
    "                                                      out_dim=out_dim,\n",
    "                                                      w_init=w_init,\n",
    "                                                      b_init=b_init,\n",
    "                                                      act=None,\n",
    "                                                      ))       \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56b667b-dba1-41b3-ab06-e91b288ac0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputScaleNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scales     = [],\n",
    "        centers    = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize InputScaleNet\n",
    "        \"\"\"\n",
    "        super(InputScaleNet,self).__init__()\n",
    "        self.scales     = torch.from_numpy(np.array(scales)).type(torch.float)\n",
    "        if centers is None:\n",
    "            self.centers = torch.zeros_like(self.scales, dtype=torch.float)\n",
    "        else:\n",
    "            self.centers = torch.from_numpy(np.array(centers)).type(torch.float)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = torch.mul(x - self.centers, self.scales)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e255b3f-19b0-4665-864b-b96adbc12e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleMLPSequential(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        layers     = None,\n",
    "        neurons    = None,\n",
    "        residual   = False, # use residual | [True, False]\n",
    "        w_init     = False, # constant std | (ex 0.1, 0.01)\n",
    "        b_init     = False, # use bias | [True, False]\n",
    "        act        = nn.Tanh(),\n",
    "        subnets    = None,  # subnet(multi scale) number\n",
    "        amp        = 1.0,   # amplification factor of input\n",
    "        base_scale = 2.0,   # base scale factor\n",
    "        in_scale   = None,  # scale factor of input (ex [x,y,t])\n",
    "        in_center  = None,  # Center position of coordinate translation (ex [x,y,t])\n",
    "        latent_vec = None,  # latent vector (ex Tensor[shape=(4,16)] )\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MultiScaleMLPSequential\n",
    "        \"\"\"\n",
    "        super(MultiScaleMLPSequential,self).__init__()\n",
    "        self.subnets = subnets\n",
    "        self.scale_coef = [amp * (base_scale**i) for i in range(self.subnets)]\n",
    "        self.latent_vec = latent_vec\n",
    "        \n",
    "        if self.latent_vec is not None:\n",
    "            self.num_scenarios = latent_vec.shape[0]\n",
    "            self.latent_size = latent_vec.shape[1]\n",
    "            in_dim += self.latent_size\n",
    "        else:\n",
    "            self.num_scenarios = 1\n",
    "            self.latent_size = 0\n",
    "\n",
    "        # Define MultiScaleMLP\n",
    "        self.msnet = nn.Sequential()\n",
    "        for i in range(self.subnets):\n",
    "            self.msnet.add_module(f\"Scale_{i+1}_Net\",MLPSequential(in_dim=in_dim,\n",
    "                                                               out_dim=out_dim,\n",
    "                                                               layers=layers,\n",
    "                                                               neurons=neurons,\n",
    "                                                               residual=residual,\n",
    "                                                               w_init=w_init,\n",
    "                                                               b_init=b_init,\n",
    "                                                               act=act,\n",
    "                                                               ))\n",
    "        if in_scale:\n",
    "            self.in_scale = InputScaleNet(in_scale, in_center)\n",
    "        else:\n",
    "            self.in_scale = torch.nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        x = self.in_scale(x)\n",
    "        if self.latent_vec is not None:\n",
    "            batch_size = x.shape[0]\n",
    "            latent_vectors = self.latent_vec.view(self.num_scenarios, 1, self.latent_size).repeat(1,batch_size//self.num_scenarios,1).view(-1,self.latent_size)\n",
    "            x = torch.concat([x, latent_vectors], axis=1)\n",
    "        \n",
    "        out = 0\n",
    "        for i in range(self.subnets):\n",
    "            x_s = x * self.scale_coef[i]\n",
    "            out = out + self.msnet[i](x_s)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480da4e7-d519-4b57-b0ed-8c7de7a8d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L1 = LayerPerceptron(in_dim=3,\n",
    "               out_dim=64)\n",
    "L2 = ResidualLayerPerceptron(in_dim=64,\n",
    "                            out_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07d5534-d614-4f0c-8020-8f76911edeca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 0.3000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.from_numpy(np.array([0.0, 0.5, 0.3])).type(torch.float)\n",
    "input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35b019dd-fc94-4a4a-bc75-870723f6fe58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3263, -0.1804, -0.1387,  0.0530, -0.0340, -0.2157,  0.0689,  0.1298,\n",
      "         0.4480, -0.1369,  0.0566,  0.3173, -0.0293,  0.3445,  0.0063, -0.0822,\n",
      "         0.0357,  0.0155,  0.1019, -0.0159, -0.0153,  0.3974,  0.2818,  0.0052,\n",
      "         0.3499, -0.2286,  0.0164,  0.0375, -0.2222, -0.2019, -0.2208, -0.0569,\n",
      "         0.1109,  0.3312,  0.1465,  0.0721, -0.1736,  0.1410,  0.0813, -0.0855,\n",
      "        -0.4368,  0.2499,  0.1756, -0.1532,  0.1458, -0.1716, -0.2216, -0.1384,\n",
      "        -0.1852, -0.1002,  0.0178,  0.0211, -0.0838, -0.3288,  0.2068, -0.1214,\n",
      "        -0.0883, -0.2694,  0.2504,  0.2042,  0.0522, -0.1123, -0.2715, -0.2856],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "tensor([-0.2701, -0.0713, -0.2914, -0.0648, -0.1171, -0.0431,  0.0511,  0.2614,\n",
      "         0.4559, -0.1941,  0.1841,  0.2839,  0.0339,  0.4836, -0.0315, -0.1717,\n",
      "        -0.0533, -0.0383,  0.0340,  0.0217, -0.1201,  0.3498,  0.2873,  0.1268,\n",
      "         0.4080, -0.2005, -0.0681,  0.3122, -0.1523, -0.1747, -0.2480, -0.1388,\n",
      "         0.0415,  0.1645, -0.0550,  0.1424, -0.2606,  0.0979, -0.1532, -0.2024,\n",
      "        -0.3540,  0.3381,  0.0416, -0.1930,  0.2338, -0.1950, -0.1349, -0.2216,\n",
      "        -0.1404, -0.1461,  0.1946,  0.2131, -0.2152, -0.3911,  0.2968, -0.0748,\n",
      "        -0.2296, -0.2577,  0.0345,  0.2286,  0.0617, -0.1904, -0.2818, -0.3688],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "L1_result = L1(input_sample)\n",
    "L2_result = L2(L1_result)\n",
    "\n",
    "print(L1_result)\n",
    "print(L2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d179fd74-6242-413c-b863-4b44ff5e1c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MLPSequential(\n",
    "        in_dim     = 3,\n",
    "        out_dim    = 1,\n",
    "        layers     = 5,\n",
    "        neurons    = 64,\n",
    "        residual   = False,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a82f24-1706-455a-9095-46983d5c306b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPSequential(\n",
       "  (net): Sequential(\n",
       "    (input): LayerPerceptron(\n",
       "      (dense): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_01): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_02): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_03): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (output): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e434a1a-6ff7-495d-9d16-82318a751bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0233], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b788db-782d-4178-adee-8ca9dc1e4440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_norm = InputScaleNet(scales=[2.0],\n",
    "                          centers=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da37dd5f-c628-4ebf-969f-74496ece217a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0000, -1.0000, -1.4000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_norm(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "057f0613-7667-4517-83eb-c64f26fa57a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MultiScaleMLPSequential(\n",
    "    in_dim=3,\n",
    "    out_dim=1,\n",
    "    layers=7,\n",
    "    neurons=32,\n",
    "    residual=True,\n",
    "    subnets=5,\n",
    "    in_scale= [1/5.0,1/20.0,1/100.0],\n",
    "    in_center=[3.0, 10.0, -15.0],\n",
    "    latent_vec= torch.from_numpy(np.random.randn(4, 12) / np.sqrt(12)).type(torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b051d771-1b24-4239-90a3-8c5ed9e6b679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiScaleMLPSequential(\n",
       "  (msnet): Sequential(\n",
       "    (Scale_1_Net): MLPSequential(\n",
       "      (net): Sequential(\n",
       "        (input): LayerPerceptron(\n",
       "          (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_01): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_02): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_03): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_04): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_05): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (output): LayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Scale_2_Net): MLPSequential(\n",
       "      (net): Sequential(\n",
       "        (input): LayerPerceptron(\n",
       "          (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_01): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_02): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_03): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_04): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_05): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (output): LayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Scale_3_Net): MLPSequential(\n",
       "      (net): Sequential(\n",
       "        (input): LayerPerceptron(\n",
       "          (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_01): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_02): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_03): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_04): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_05): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (output): LayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Scale_4_Net): MLPSequential(\n",
       "      (net): Sequential(\n",
       "        (input): LayerPerceptron(\n",
       "          (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_01): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_02): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_03): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_04): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_05): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (output): LayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Scale_5_Net): MLPSequential(\n",
       "      (net): Sequential(\n",
       "        (input): LayerPerceptron(\n",
       "          (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_01): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_02): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_03): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_04): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (res_05): ResidualLayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "        (output): LayerPerceptron(\n",
       "          (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (in_scale): InputScaleNet()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48527c5e-f9b6-4d02-a0c6-8da126f47baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.rand(size=(16,3), dtype=torch.float)\n",
    "input_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8142e0b5-2554-43d5-88f2-040ea6638eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5678],\n",
       "        [ 0.5959],\n",
       "        [ 0.6317],\n",
       "        [ 0.6023],\n",
       "        [-0.0263],\n",
       "        [-0.0142],\n",
       "        [-0.0133],\n",
       "        [-0.0242],\n",
       "        [ 1.0024],\n",
       "        [ 1.0171],\n",
       "        [ 1.0151],\n",
       "        [ 1.1046],\n",
       "        [ 0.2033],\n",
       "        [ 0.2142],\n",
       "        [ 0.2206],\n",
       "        [ 0.2448]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe46c33-054f-4817-862b-973312b43db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
