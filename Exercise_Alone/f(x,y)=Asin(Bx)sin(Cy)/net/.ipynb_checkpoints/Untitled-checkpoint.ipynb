{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca40ce5-eb3a-4fb2-b9c6-71d424c68b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e29b5b40-2b98-43d3-b878-d85fb063c1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize LayerPerceptron\n",
    "        \"\"\"\n",
    "        super(LayerPerceptron,self).__init__()\n",
    "        self.dense      = nn.Linear(in_features=in_dim,\n",
    "                                    out_features=out_dim,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "        self.activation = act\n",
    "        # Initialize parameters\n",
    "        self.init_param(w_init, b_init)\n",
    "\n",
    "    def init_param(self, w_init, b_init):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        if w_init:\n",
    "            nn.init.constant_(self.dense.weight, w_init)\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.dense.weight,a=math.sqrt(5))\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        if self.activation is not None:\n",
    "            out = self.activation(self.dense(x))\n",
    "        else:\n",
    "            out = self.dense(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee8199aa-c22b-4d8a-a5f2-a1fd52a9e25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize ResidualLayerPerceptron\n",
    "        \"\"\"\n",
    "        super(ResidualLayerPerceptron,self).__init__()\n",
    "\n",
    "        if in_dim != out_dim:\n",
    "            raise ValueError(\"in_dim of ResBlock should be equal of out_dim, but got in_dim: {}, \"\n",
    "                             \"out_dim: {}\".format(in_dim, out_dim))\n",
    "\n",
    "        self.dense      = nn.Linear(in_features=in_dim,\n",
    "                                    out_features=out_dim,\n",
    "                                    dtype=torch.float)\n",
    "\n",
    "        self.activation = act\n",
    "        # Initialize parameters\n",
    "        self.init_param(w_init, b_init)\n",
    "\n",
    "    def init_param(self, w_init, b_init):\n",
    "        \"\"\"\n",
    "            Initialize parameters\n",
    "        \"\"\"\n",
    "        if w_init:\n",
    "            nn.init.constant_(self.dense.weight, w_init)\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "        else:\n",
    "            nn.init.kaiming_normal_(self.dense.weight,a=math.sqrt(5))\n",
    "            if b_init:\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.dense.weight)\n",
    "                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                nn.init.zeros_(self.dense.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = self.activation(self.dense(x)+x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55761f49-bee9-4169-a1e2-0983bb8005d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPSequential(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        layers     = None,\n",
    "        neurons    = None,\n",
    "        residual   = False,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MLPSequential\n",
    "        \"\"\"\n",
    "        super(MLPSequential,self).__init__()\n",
    "        if layers < 3:\n",
    "            raise ValueError(\"MLPSequential have at least 3 layers, but got layers: {}\".format(layers))\n",
    "\n",
    "        # Define net\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add_module(\"input\", LayerPerceptron(in_dim=in_dim,\n",
    "                                                           out_dim=neurons,\n",
    "                                                           w_init=w_init,\n",
    "                                                           b_init=b_init,\n",
    "                                                           act=act,\n",
    "                                                           ))\n",
    "        for idx in range(layers - 2):\n",
    "            if residual:\n",
    "                self.net.add_module(f\"res_{idx+1:02d}\", ResidualLayerPerceptron(in_dim=neurons,\n",
    "                                                                                     out_dim=neurons,\n",
    "                                                                                     w_init=w_init,\n",
    "                                                                                     b_init=b_init,\n",
    "                                                                                     act=act,\n",
    "                                                                                     ))\n",
    "            else:\n",
    "                self.net.add_module(f\"mlp_{idx+1:02d}\", LayerPerceptron(in_dim=neurons,\n",
    "                                                                        out_dim=neurons,\n",
    "                                                                        w_init=w_init,\n",
    "                                                                        b_init=b_init,\n",
    "                                                                        act=act,\n",
    "                                                                        ))\n",
    "\n",
    "        self.net.add_module(\"output\", LayerPerceptron(in_dim=neurons,\n",
    "                                                      out_dim=out_dim,\n",
    "                                                      w_init=w_init,\n",
    "                                                      b_init=b_init,\n",
    "                                                      act=None,\n",
    "                                                      ))       \n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b56b667b-dba1-41b3-ab06-e91b288ac0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputScaleNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scales     = [],\n",
    "        centers    = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize InputScaleNet\n",
    "        \"\"\"\n",
    "        super(InputScaleNet,self).__init__()\n",
    "        self.scales     = torch.from_numpy(np.array(scales)).type(torch.float)\n",
    "        if centers is None:\n",
    "            self.centers = torch.zeros_like(self.scales, dtype=torch.float)\n",
    "        else:\n",
    "            self.centers = torch.from_numpy(np.array(centers)).type(torch.float)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        out = torch.mul(x - self.centers, self.scales)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4e255b3f-19b0-4665-864b-b96adbc12e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiScaleMLPSequential(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim     = None,\n",
    "        out_dim    = None,\n",
    "        layers     = None,\n",
    "        neurons    = None,\n",
    "        residual   = False, # use residual | [True, False]\n",
    "        w_init     = False, # constant std | (ex 0.1, 0.01)\n",
    "        b_init     = False, # use bias | [True, False]\n",
    "        act        = nn.Tanh(),\n",
    "        subnets    = None,  # subnet(multi scale) number\n",
    "        amp        = 1.0,   # amplification factor of input\n",
    "        base_scale = 2.0,   # base scale factor\n",
    "        in_scale   = None,  # scale factor of input (ex [x,y,t])\n",
    "        in_center  = None,  # Center position of coordinate translation (ex [x,y,t])\n",
    "        latent_vec = None,  # latent vector (ex Tensor[shape=(4,16)] )\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize MultiScaleMLPSequential\n",
    "        \"\"\"\n",
    "        super(MultiScaleMLPSequential,self).__init__()\n",
    "        self.subnets = subnets\n",
    "        self.scale_coef = [amp * (base_scale**i) for i in range(self.subnets)]\n",
    "        self.latent_vec = latent_vec\n",
    "        \n",
    "        if self.latent_vec is not None:\n",
    "            self.num_scenarios = latent_vec.shape[0]\n",
    "            self.latent_size = latent_vec.shape[1]\n",
    "            in_dim += self.latent_size\n",
    "        else:\n",
    "            self.num_scenarios = 1\n",
    "            self.latent_size = 0\n",
    "\n",
    "        # Define MultiScaleMLP\n",
    "        self.msnet = nn.Sequential()\n",
    "        for i in range(self.subnets):\n",
    "            self.msnet.add_module(f\"Scale_{i+1}_Net\",MLPSequential(in_dim=in_dim,\n",
    "                                                               out_dim=out_dim,\n",
    "                                                               layers=layers,\n",
    "                                                               neurons=neurons,\n",
    "                                                               residual=residual,\n",
    "                                                               w_init=w_init,\n",
    "                                                               b_init=b_init,\n",
    "                                                               act=act,\n",
    "                                                               ))\n",
    "        if in_scale:\n",
    "            self.in_scale = InputScaleNet(in_scale, in_center)\n",
    "        else:\n",
    "            self.in_scale = torch.nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            Forward propagate\n",
    "        \"\"\"\n",
    "        x = self.in_scale(x)\n",
    "        if self.latent_vec is not None:\n",
    "            batch_size = x.shape[0]\n",
    "            latent_vectors = self.latent_vec.view(self.num_scenarios, 1, self.latent_size).repeat(1,batch_size//self.num_scenarios,1).view(-1,self.latent_size)\n",
    "            x = torch.concat([x, latent_vectors], axis=1)\n",
    "        \n",
    "        out = 0\n",
    "        for i in range(self.subnets):\n",
    "            x_s = x * self.scale_coef[i]\n",
    "            out = out + self.msnet[i](x_s)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "480da4e7-d519-4b57-b0ed-8c7de7a8d47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "L1 = LayerPerceptron(in_dim=3,\n",
    "               out_dim=64)\n",
    "L2 = ResidualLayerPerceptron(in_dim=64,\n",
    "                            out_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a07d5534-d614-4f0c-8020-8f76911edeca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 0.3000])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.from_numpy(np.array([0.0, 0.5, 0.3])).type(torch.float)\n",
    "input_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "35b019dd-fc94-4a4a-bc75-870723f6fe58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1282,  0.0256, -0.2003,  0.3692,  0.3741,  0.0310,  0.0847,  0.2083,\n",
      "        -0.0165,  0.2475,  0.1940,  0.2290,  0.1718, -0.0896,  0.1503, -0.0067,\n",
      "         0.1715,  0.0633, -0.1786,  0.1040, -0.2078,  0.2289,  0.1062, -0.0383,\n",
      "        -0.2619,  0.1027,  0.1535,  0.0044,  0.0018, -0.3223, -0.1883,  0.2138,\n",
      "         0.0862, -0.1019, -0.1016, -0.2471, -0.0405,  0.0485, -0.0236, -0.0983,\n",
      "         0.1831, -0.0736, -0.2003,  0.1810, -0.1072,  0.0969,  0.1250, -0.1792,\n",
      "        -0.0718, -0.0333, -0.3354, -0.1539,  0.4142, -0.2296,  0.2455,  0.2373,\n",
      "         0.0882, -0.2062,  0.4431,  0.2342, -0.1151, -0.2988,  0.4287, -0.1166],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "tensor([ 0.0191, -0.0354, -0.1971,  0.3699,  0.3556,  0.0490, -0.0100,  0.3530,\n",
      "         0.1180,  0.1009,  0.2671,  0.2121,  0.1149, -0.0849,  0.0886, -0.1250,\n",
      "        -0.0131,  0.3430, -0.1629,  0.0501, -0.1943,  0.2777,  0.0266, -0.0213,\n",
      "        -0.2530,  0.1224,  0.3085, -0.0211, -0.1696, -0.2420, -0.1503,  0.2234,\n",
      "        -0.0624, -0.0212, -0.1451, -0.1980, -0.1357,  0.0510,  0.0416, -0.2503,\n",
      "         0.2074,  0.0503, -0.1932,  0.0743,  0.0231, -0.0680,  0.0505, -0.0463,\n",
      "         0.1735, -0.0450, -0.0901, -0.1693,  0.5571, -0.3050,  0.2714, -0.0252,\n",
      "         0.0053, -0.0119,  0.4181,  0.4299, -0.0048, -0.2711,  0.3652, -0.0716],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "L1_result = L1(input_sample)\n",
    "L2_result = L2(L1_result)\n",
    "\n",
    "print(L1_result)\n",
    "print(L2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d179fd74-6242-413c-b863-4b44ff5e1c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MLPSequential(\n",
    "        in_dim     = 3,\n",
    "        out_dim    = 1,\n",
    "        layers     = 5,\n",
    "        neurons    = 64,\n",
    "        residual   = False,\n",
    "        w_init     = False,\n",
    "        b_init     = False,\n",
    "        act        = nn.Tanh(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f6a82f24-1706-455a-9095-46983d5c306b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPSequential(\n",
       "  (net): Sequential(\n",
       "    (input): LayerPerceptron(\n",
       "      (dense): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_01): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_02): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (mlp_03): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (output): LayerPerceptron(\n",
       "      (dense): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6e434a1a-6ff7-495d-9d16-82318a751bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0321], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "20b788db-782d-4178-adee-8ca9dc1e4440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_norm = InputScaleNet(scales=[2.0],\n",
    "                          centers=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "da37dd5f-c628-4ebf-969f-74496ece217a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0000, -1.0000, -1.4000])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_norm(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "057f0613-7667-4517-83eb-c64f26fa57a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MultiScaleMLPSequential(\n",
    "    in_dim=3,\n",
    "    out_dim=1,\n",
    "    layers=7,\n",
    "    neurons=32,\n",
    "    residual=True,\n",
    "    subnets=3,\n",
    "    in_scale= [1/5.0,1/20.0,1/100.0],\n",
    "    in_center=[3.0, 10.0, -15.0],\n",
    "    latent_vec= torch.from_numpy(np.random.randn(4, 12) / np.sqrt(12)).type(torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b051d771-1b24-4239-90a3-8c5ed9e6b679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPSequential(\n",
       "  (net): Sequential(\n",
       "    (input): LayerPerceptron(\n",
       "      (dense): Linear(in_features=15, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (res_01): ResidualLayerPerceptron(\n",
       "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (res_02): ResidualLayerPerceptron(\n",
       "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (res_03): ResidualLayerPerceptron(\n",
       "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (res_04): ResidualLayerPerceptron(\n",
       "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (res_05): ResidualLayerPerceptron(\n",
       "      (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (output): LayerPerceptron(\n",
       "      (dense): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.msnet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "48527c5e-f9b6-4d02-a0c6-8da126f47baf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = torch.rand(size=(16,3), dtype=torch.float)\n",
    "input_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8142e0b5-2554-43d5-88f2-040ea6638eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3746],\n",
       "        [ 0.3652],\n",
       "        [ 0.3747],\n",
       "        [ 0.3637],\n",
       "        [-0.0305],\n",
       "        [-0.0846],\n",
       "        [-0.0801],\n",
       "        [-0.0306],\n",
       "        [ 0.1266],\n",
       "        [ 0.1427],\n",
       "        [ 0.1109],\n",
       "        [ 0.0868],\n",
       "        [ 0.2716],\n",
       "        [ 0.3088],\n",
       "        [ 0.2672],\n",
       "        [ 0.3007]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe46c33-054f-4817-862b-973312b43db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
