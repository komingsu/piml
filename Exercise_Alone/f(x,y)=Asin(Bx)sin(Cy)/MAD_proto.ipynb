{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d5a7a0-462f-4a7f-a5b8-cc0b372db9c1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a054aa-6266-482c-98bf-539b1a0e8b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# 시각화\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Network\n",
    "from net.network import MultiScaleMLPSequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43d046d-9c0f-4e83-9843-9ee7cfc4a68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# CUDA check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779bbe3-4b16-4713-a3ca-c4251fd1d534",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57a60b-396c-48f2-a28e-377785feec75",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f615f4-794a-452f-bcec-c398f672e2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_derivatives(model, xy_data):\n",
    "    \"\"\"\n",
    "    Get fxx, fyy\n",
    "    \"\"\"\n",
    "    xy_data = xy_data.to(device)\n",
    "    # Ensure that xy_data has gradient information.\n",
    "    xy_data.requires_grad_(True)\n",
    "    \n",
    "    # Get the model prediction.\n",
    "    f_pred = model(xy_data)\n",
    "    \n",
    "    # Create a tensor of ones with the same shape as f_pred to be used for gradient computation.\n",
    "    # Reshape the ones tensor to match the shape of f_pred.\n",
    "    ones = torch.ones(f_pred.shape, device=device, requires_grad=False)\n",
    "    \n",
    "    # Compute the first derivatives.\n",
    "    f_x = torch.autograd.grad(f_pred, xy_data, grad_outputs=ones, create_graph=True)[0][:, 0]\n",
    "    f_y = torch.autograd.grad(f_pred, xy_data, grad_outputs=ones, create_graph=True)[0][:, 1]\n",
    "    \n",
    "    # Compute the second derivatives.\n",
    "    f_xx = torch.autograd.grad(f_x, xy_data, grad_outputs=ones[:, 0], create_graph=True)[0][:, 0]\n",
    "    f_yy = torch.autograd.grad(f_y, xy_data, grad_outputs=ones[:, 0], create_graph=True)[0][:, 1]\n",
    "    \n",
    "    return f_xx, f_yy\n",
    "\n",
    "def multiscale_task_loss(model, criterion, xy_daya, f_data, B, C, alpha=0.1):\n",
    "    \"\"\"\n",
    "    추가 도입 예정\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_dataloader(data, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    get dataloader\n",
    "    \"\"\"\n",
    "    xy_data = torch.tensor(np.stack([data.T[0], data.T[1]], axis=-1), dtype=torch.float)\n",
    "    f_data = torch.tensor(data.T[2], dtype=torch.float).view(-1, 1)\n",
    "    a_data = torch.tensor(data.T[3], dtype=torch.float).view(-1, 1)\n",
    "    b_data = torch.tensor(data.T[4], dtype=torch.float).view(-1, 1)\n",
    "    c_data = torch.tensor(data.T[5], dtype=torch.float).view(-1, 1)\n",
    "    \n",
    "    dataset = TensorDataset(xy_data, f_data, a_data, b_data, c_data)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return tqdm(loader, desc=\"Training \", leave=False)\n",
    "\n",
    "\n",
    "def train_model(model, loader, epochs):\n",
    "    \"\"\"\n",
    "    Implementation func of model training\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        batch_loss = 0.0\n",
    "        batch_data_loss = 0.0  # To record data loss\n",
    "        batch_pde_loss = 0.0   # To record pde loss\n",
    "\n",
    "        for batch_xy, batch_f, batch_a, batch_b, batch_c in loader:\n",
    "            batch_xy = batch_xy.to(device)\n",
    "            batch_f  = batch_f.to(device)\n",
    "            batch_a  = batch_a.to(device)\n",
    "            batch_b  = batch_b.to(device)\n",
    "            batch_c  = batch_c.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the losses\n",
    "            f_pred = model(batch_xy)\n",
    "            data_loss = criterion(f_pred, batch_f)\n",
    "            f_xx, f_yy = compute_derivatives(model, batch_xy)\n",
    "            true_pde = -(batch_b**2+batch_c**2)*f_pred\n",
    "            pde_loss = criterion(f_xx + f_yy, true_pde.squeeze())\n",
    "\n",
    "            # Combine the losses\n",
    "            loss = data_loss + alpha * pde_loss\n",
    "\n",
    "            # Backpropagate and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record losses\n",
    "            batch_loss += loss.item()\n",
    "            batch_data_loss += data_loss.item()\n",
    "            batch_pde_loss += pde_loss.item()\n",
    "\n",
    "        avg_loss = batch_loss / len(loader)\n",
    "        avg_data_loss = batch_data_loss / len(loader)\n",
    "        avg_pde_loss = batch_pde_loss / len(loader)\n",
    "\n",
    "        # Append the average losses for this epoch to the history\n",
    "        loss_history.append(avg_loss)\n",
    "        data_loss_history.append(avg_data_loss)\n",
    "        pde_loss_history.append(avg_pde_loss)\n",
    "\n",
    "        scheduler.step() # ReduceLROnPlateau 이면 Loss를 인자로 넣어야함\n",
    "\n",
    "        # Print the losses every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch {epoch}/{epochs}, Total Loss: {avg_loss}, Data Loss: {avg_data_loss}, PDE Loss: {avg_pde_loss}, LR: {current_lr}')\n",
    "\n",
    "    # 손실값 그래프를 그립니다.\n",
    "    print(\"total loss : \",loss_history[-1])\n",
    "    print(\"DATA  loss : \",data_loss_history[-1])\n",
    "    print(\"PDE   loss : \",pde_loss_history[-1])\n",
    "    plt.semilogy(loss_history, label=\"Total\")\n",
    "    plt.semilogy(data_loss_history, label=\"Data\")\n",
    "    plt.semilogy(pde_loss_history, label=\"PDE\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Epoch vs loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def test_inference(model, test_data_path):\n",
    "    with open(test_data_path, 'rb') as f:\n",
    "        data = np.load(f,allow_pickle=True)\n",
    "    x = data.T[0]\n",
    "    y = data.T[1]\n",
    "    X = data[:,:2]\n",
    "    f_true = data.T[2]\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    X = X.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        f_pred = model(X).cpu().numpy().squeeze()\n",
    "\n",
    "    loss = np.mean(np.sqrt(np.abs(np.square(f_true.reshape(-1))-np.square(f_pred.reshape(-1)))))\n",
    "    print(\"Test MSE Loss : \", loss)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3))\n",
    "    # grid\n",
    "    xi, yi = np.mgrid[x.min():x.max():500j, y.min():y.max():500j]\n",
    "\n",
    "    # f_true figure\n",
    "    rbf = scipy.interpolate.Rbf(x, y, f_true)\n",
    "    org = rbf(xi, yi)\n",
    "    img = ax[0].imshow(org.T, origin='lower',extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "    ax[0].scatter(x, y, c=f_true)\n",
    "    ax[0].set(xlabel='X', ylabel='Y')\n",
    "    fig.colorbar(img,shrink=0.9)\n",
    "\n",
    "    # f_pred figure\n",
    "    rbf = scipy.interpolate.Rbf(x, y, f_pred)\n",
    "    org = rbf(xi, yi)\n",
    "    img = ax[1].imshow(org.T, origin='lower',extent=[x.min(), x.max(), y.min(), y.max()])\n",
    "    ax[1].scatter(x, y, c=f_pred)\n",
    "    ax[1].set(xlabel='X', ylabel='Y')\n",
    "    fig.colorbar(img,shrink=0.9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be82d0-e19e-4889-8ca2-f016fc2e569e",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c22b241-b235-43b6-95e4-4719e17a1371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "batch_size = 20000\n",
    "shuffle = True # 데이터 셔플\n",
    "epochs = 5000   # 훈련 epoch\n",
    "alpha= 0.5      # Total Loss = Data Loss + alpha * PDE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d50d8d3-0133-41d6-a326-e4a943ba0a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6)\n"
     ]
    }
   ],
   "source": [
    "# Data load\n",
    "with open(\"./data/trainXYFABC.npy\", 'rb') as f:\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "f.close()\n",
    "\n",
    "# Sample data\n",
    "print(data.shape) #100000 of [x, y, f(x,y), A, B, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "401d333f-8bff-44b9-9e5c-e68002799559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5000, Total Loss: 27023.050006103516, Data Loss: 23.26734848022461, PDE Loss: 53999.56539230347, LR: 0.1999980280595281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#### 훈련 시작 ####\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain start\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain end\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loader, epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m data_loss \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m pde_loss\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Backpropagate and optimize\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Record losses\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loader = create_dataloader(data, batch_size, shuffle)\n",
    "\n",
    "model = MultiScaleMLPSequential(\n",
    "    in_dim=2,\n",
    "    out_dim=1,\n",
    "    layers=7,\n",
    "    neurons=32,\n",
    "    residual=True,\n",
    "    subnets=3,\n",
    "    in_scale= [2*np.pi,2*np.pi],\n",
    "    in_center=[0, 0],\n",
    "    vec_scen=4,\n",
    "    vec_size=12,\n",
    ")\n",
    "model = model.to(device)\n",
    "# loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.2)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10, verbose=False)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, 500, T_mult=1, eta_min=0.0002)\n",
    "\n",
    "# init\n",
    "loss_history = []\n",
    "data_loss_history = []\n",
    "pde_loss_history = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "#### 훈련 시작 ####\n",
    "print(\"Train start\\n\")\n",
    "model = train_model(model, loader, epochs)\n",
    "print(\"Train end\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe30b0-0f98-44a3-afbf-4bfadd336e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd802b-b8e8-4954-b510-69e154e82af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def piad2d(args):\n",
    "#     \"\"\"pretraining and reconstruction process\"\"\"\n",
    "#     config = preprocess_config(args)\n",
    "#     train_dataset = create_random_dataset(config)\n",
    "#     train_dataset = train_dataset.create_dataset(batch_size=config[\"batch_size\"], shuffle=True,\n",
    "#                                                  prebatched_data=True, drop_remainder=True)\n",
    "#     epoch_steps = len(train_dataset)\n",
    "#     print(\"check train dataset size: \", len(train_dataset))\n",
    "#     # load ckpt\n",
    "#     if config.get(\"load_ckpt\", False):\n",
    "#         param_dict = load_checkpoint(config[\"load_ckpt_path\"])\n",
    "#         if args.mode == \"pretrain\":\n",
    "#             loaded_ckpt_dict = param_dict\n",
    "#         else:\n",
    "#             loaded_ckpt_dict, latent_vector_ckpt = {}, 0\n",
    "#             for name in param_dict:\n",
    "#                 if name == \"model.latent_vector\":\n",
    "#                     latent_vector_ckpt = param_dict[name].data.asnumpy()\n",
    "#                 elif \"network\" in name and \"moment\" not in name:\n",
    "#                     loaded_ckpt_dict[name] = param_dict[name]\n",
    "#     # initialize latent vector\n",
    "#     num_scenarios, latent_size = config[\"num_scenarios\"], config[\"latent_vector_size\"]\n",
    "#     latent_vector = calc_latent_init(latent_size, latent_vector_ckpt, args.mode, num_scenarios)\n",
    "#     network = MultiScaleFCCell(config[\"input_size\"], config[\"output_size\"],\n",
    "#                                layers=config[\"layers\"], neurons=config[\"neurons\"], residual=config[\"residual\"],\n",
    "#                                weight_init=HeUniform(negative_slope=math.sqrt(5)), act=\"sin\",\n",
    "#                                num_scales=config[\"num_scales\"], amp_factor=config[\"amp_factor\"],\n",
    "#                                scale_factor=config[\"scale_factor\"], input_scale=config[\"input_scale\"],\n",
    "#                                input_center=config[\"input_center\"], latent_vector=latent_vector)\n",
    "#     network = network.to_float(ms.float16)\n",
    "#     network.input_scale.to_float(ms.float32)\n",
    "#     mtl_cell = MTLWeightedLossCell(num_losses=train_dataset.num_dataset) if config.get(\"enable_mtl\", True) else None\n",
    "#     # define problem\n",
    "#     train_prob = {}\n",
    "#     for dataset in train_dataset.all_datasets:\n",
    "#         train_prob[dataset.name] = Maxwell2DMur(network=network, config=config, domain_column=dataset.name + \"_points\",\n",
    "#                                                 ic_column=dataset.name + \"_points\", bc_column=dataset.name + \"_points\")\n",
    "#     print(\"check problem: \", train_prob)\n",
    "#     train_constraints = Constraints(train_dataset, train_prob)\n",
    "#     # optimizer\n",
    "#     params = load_net(args, config, loaded_ckpt_dict, mtl_cell, network)\n",
    "#     lr_scheduler = MultiStepLR(config[\"lr\"], config[\"milestones\"], config[\"lr_gamma\"], epoch_steps,\n",
    "#                                config[\"train_epoch\"])\n",
    "#     optimizer = nn.Adam(params, learning_rate=Tensor(lr_scheduler.get_lr()))\n",
    "#     # problem solver\n",
    "#     solver = Solver(network, optimizer=optimizer, mode=\"PINNs\", train_constraints=train_constraints,\n",
    "#                     test_constraints=None, metrics={'l2': L2(), 'distance': nn.MAE()}, loss_fn='smooth_l1_loss',\n",
    "#                     loss_scale_manager=DynamicLossScaleManager(), mtl_weighted_cell=mtl_cell,\n",
    "#                     latent_vector=latent_vector, latent_reg=config[\"latent_reg\"])\n",
    "#     callbacks = get_callbacks(args, config, epoch_steps, network)\n",
    "#     solver.train(config[\"train_epoch\"], train_dataset, callbacks=callbacks, dataset_sink_mode=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
