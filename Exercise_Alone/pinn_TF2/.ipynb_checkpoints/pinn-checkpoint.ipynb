{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e58e-5411-4c6d-b336-f4d63e63c805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/maziarraissi/PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8d0fb-0dc9-4d67-8b7c-d65aa3f53619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc533e4f-17bf-4214-a6bb-ffb14a3cd0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721a25c-6e50-4875-914e-ad4741f23cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
    "repoPath = os.path.join(\".\", \"PINNs\")\n",
    "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
    "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
    "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
    "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
    "sys.path.insert(0, utilsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b6122d-9e09-48c4-b92d-987c11c5ebf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "    global global_time_list, global_last_time\n",
    "    global_time_list = []\n",
    "    global_last_time = time.perf_counter()\n",
    "    \n",
    "def record_time():\n",
    "    global global_last_time, global_time_list\n",
    "    new_time = time.perf_counter()\n",
    "    global_time_list.append(new_time - global_last_time)\n",
    "    global_last_time = time.perf_counter()\n",
    "    #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "    \"\"\"Returns last interval records in millis.\"\"\"\n",
    "    global global_last_time, global_time_list\n",
    "    if global_time_list:\n",
    "        return 1000 * global_time_list[-1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54985d78-7123-4c8f-810f-1ef882114b29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf6bea-aa06-48d3-ae50-ead82e862332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1fd8f-2f3b-428b-9477-056c6026210a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "    return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05fd18-5f9c-48b6-949b-63df0ce0f7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "    \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
    "    \"\"\"\n",
    "\n",
    "    if config.maxIter == 0:\n",
    "        return\n",
    "\n",
    "    global final_loss, times\n",
    "    \n",
    "    maxIter = config.maxIter\n",
    "    maxEval = config.maxEval or maxIter*1.25\n",
    "    tolFun = config.tolFun or 1e-5\n",
    "    tolX = config.tolX or 1e-19\n",
    "    nCorrection = config.nCorrection or 100\n",
    "    lineSearch = config.lineSearch\n",
    "    lineSearchOpts = config.lineSearchOptions\n",
    "    learningRate = config.learningRate or 1\n",
    "    isverbose = config.verbose or False\n",
    "\n",
    "    # verbose function\n",
    "    if isverbose:\n",
    "        verbose = verbose_func\n",
    "    else:\n",
    "        verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "    f, g = opfunc(x)\n",
    "\n",
    "    f_hist = [f]\n",
    "    currentFuncEval = 1\n",
    "    state.funcEval = state.funcEval + 1\n",
    "    p = g.shape[0]\n",
    "\n",
    "    # check optimality of initial point\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <= tolFun:\n",
    "        verbose(\"optimality condition below tolFun\")\n",
    "        return x, f_hist\n",
    "\n",
    "    # optimize for a max of maxIter iterations\n",
    "    nIter = 0\n",
    "    times = []\n",
    "    while nIter < maxIter:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # keep track of nb of iterations\n",
    "        nIter = nIter + 1\n",
    "        state.nIter = state.nIter + 1\n",
    "\n",
    "        ############################################################\n",
    "        ## compute gradient descent direction\n",
    "        ############################################################\n",
    "        if state.nIter == 1:\n",
    "            d = -g\n",
    "            old_dirs = []\n",
    "            old_stps = []\n",
    "            Hdiag = 1\n",
    "        else:\n",
    "            # do lbfgs update (update memory)\n",
    "            y = g - g_old\n",
    "            s = d*t\n",
    "            ys = dot(y, s)\n",
    "            \n",
    "            if ys > 1e-10:\n",
    "                # updating memory\n",
    "                if len(old_dirs) == nCorrection:\n",
    "                    # shift history by one (limited-memory)\n",
    "                    del old_dirs[0]\n",
    "                    del old_stps[0]\n",
    "\n",
    "                # store new direction/step\n",
    "                old_dirs.append(s)\n",
    "                old_stps.append(y)\n",
    "\n",
    "                # update scale of initial Hessian approximation\n",
    "                Hdiag = ys/dot(y, y)\n",
    "\n",
    "            # compute the approximate (L-BFGS) inverse Hessian \n",
    "            # multiplied by the gradient\n",
    "            k = len(old_dirs)\n",
    "\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            ro = [0]*nCorrection\n",
    "            for i in range(k):\n",
    "                ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "                \n",
    "\n",
    "            # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            al = [0]*nCorrection\n",
    "\n",
    "            q = -g\n",
    "            for i in range(k-1, -1, -1):\n",
    "                al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "                q = q - al[i]*old_stps[i]\n",
    "\n",
    "            # multiply by initial Hessian\n",
    "            r = q*Hdiag\n",
    "            for i in range(k):\n",
    "                be_i = dot(old_stps[i], r) * ro[i]\n",
    "                r += (al[i]-be_i)*old_dirs[i]\n",
    "                \n",
    "            d = r\n",
    "            # final direction is in r/d (same object)\n",
    "\n",
    "        g_old = g\n",
    "        f_old = f\n",
    "        \n",
    "        ############################################################\n",
    "        ## compute step length\n",
    "        ############################################################\n",
    "        # directional derivative\n",
    "        gtd = dot(g, d)\n",
    "\n",
    "        # check that progress can be made along that direction\n",
    "        if gtd > -tolX:\n",
    "            verbose(\"Can not make progress along direction.\")\n",
    "            break\n",
    "\n",
    "        # reset initial guess for step size\n",
    "        if state.nIter == 1:\n",
    "            tmp1 = tf.abs(g)\n",
    "            t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "        else:\n",
    "            t = learningRate\n",
    "\n",
    "\n",
    "        # optional line search: user function\n",
    "        lsFuncEval = 0\n",
    "        if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "            # perform line search, using user function\n",
    "            f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "            f_hist.append(f)\n",
    "        else:\n",
    "            # no line search, simply move with fixed-step\n",
    "            x += t*d\n",
    "            \n",
    "            if nIter != maxIter:\n",
    "                # re-evaluate function only if not in last iteration\n",
    "                # the reason we do this: in a stochastic setting,\n",
    "                # no use to re-evaluate that function here\n",
    "                f, g = opfunc(x)\n",
    "                lsFuncEval = 1\n",
    "                f_hist.append(f)\n",
    "\n",
    "\n",
    "        # update func eval\n",
    "        currentFuncEval = currentFuncEval + lsFuncEval\n",
    "        state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "        ############################################################\n",
    "        ## check conditions\n",
    "        ############################################################\n",
    "        if nIter == maxIter:\n",
    "            break\n",
    "\n",
    "        if currentFuncEval >= maxEval:\n",
    "            # max nb of function evals\n",
    "            verbose('max nb of function evals')\n",
    "            break\n",
    "\n",
    "        tmp1 = tf.abs(g)\n",
    "        if tf.reduce_sum(tmp1) <=tolFun:\n",
    "            # check optimality\n",
    "            verbose('optimality condition below tolFun')\n",
    "            break\n",
    "        \n",
    "        tmp1 = tf.abs(d*t)\n",
    "        if tf.reduce_sum(tmp1) <= tolX:\n",
    "            # step size below tolX\n",
    "            verbose('step size below tolX')\n",
    "            break\n",
    "\n",
    "        if tf.abs(f-f_old) < tolX:\n",
    "            # function value changing less than tolX\n",
    "            verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "            break\n",
    "\n",
    "        if do_verbose:\n",
    "            log_fn(nIter, f.numpy(), True)\n",
    "            #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "            record_time()\n",
    "            times.append(last_time())\n",
    "\n",
    "        if nIter == maxIter - 1:\n",
    "            final_loss = f.numpy()\n",
    "\n",
    "\n",
    "    # save state\n",
    "    state.old_dirs = old_dirs\n",
    "    state.old_stps = old_stps\n",
    "    state.Hdiag = Hdiag\n",
    "    state.g_old = g_old\n",
    "    state.f_old = f_old\n",
    "    state.t = t\n",
    "    state.d = d\n",
    "\n",
    "    return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "    pass\n",
    "\n",
    "class Struct(dummy):\n",
    "    def __getattribute__(self, key):\n",
    "        if key == '__dict__':\n",
    "            return super(dummy, self).__getattribute__('__dict__')\n",
    "        return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83a33c-0790-439f-9d43-c0a9c6a998ff",
   "metadata": {},
   "source": [
    "# Continuous inference\n",
    "\n",
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644b66c-51b7-471e-a476-efe3dcba49d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "sys.path.insert(0, utilsPath)\n",
    "from plotting import newfig, savefig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8492f95-8da7-4fcb-8fda-9b649160bd6a",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b0772-7f4c-4747-9d11-4aa92b82d599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.1,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d59d7bc-047a-498c-8c03-01d6b29b4e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
    "        # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "        self.u_model = tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(\n",
    "            lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "                width, activation=tf.nn.tanh,\n",
    "                kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.nu = nu\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
    "        self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "        \n",
    "    # Defining custom loss\n",
    "    def __loss(self, u, u_pred):\n",
    "        f_pred = self.f_model()\n",
    "        return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "    def __grad(self, X, u):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss(u, self.u_model(X))\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the two inputs we’ll need later, x and t\n",
    "            tape.watch(self.x_f)\n",
    "            tape.watch(self.t_f)\n",
    "            # Packing together the inputs\n",
    "            X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
    "\n",
    "            # Getting the prediction\n",
    "            u = self.u_model(X_f)\n",
    "            # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "            u_x = tape.gradient(u, self.x_f)\n",
    "        \n",
    "        # Getting the other derivatives\n",
    "        u_xx = tape.gradient(u_x, self.x_f)\n",
    "        u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "\n",
    "        nu = self.get_params(numpy=True)\n",
    "\n",
    "        # Buidling the PINNs\n",
    "        return u_t + u*u_x - nu*u_xx\n",
    "\n",
    "    def get_params(self, numpy=False):\n",
    "        return self.nu\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.u_model.layers[1:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad(X_u, u)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            self.logger.log_train_epoch(epoch, loss_value)\n",
    "        \n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights(w)\n",
    "                loss_value = self.__loss(u, self.u_model(X_u))\n",
    "            grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat =  tf.concat(grad_flat, 0)\n",
    "            return loss_value, grad_flat\n",
    "        # tfp.optimizer.lbfgs_minimize(\n",
    "        #   loss_and_flat_grad,\n",
    "        #   initial_position=self.get_weights(),\n",
    "        #   num_correction_pairs=nt_config.nCorrection,\n",
    "        #   max_iterations=nt_config.maxIter,\n",
    "        #   f_relative_tolerance=nt_config.tolFun,\n",
    "        #   tolerance=nt_config.tolFun,\n",
    "        #   parallel_iterations=6)\n",
    "        lbfgs(loss_and_flat_grad,\n",
    "            self.get_weights(),\n",
    "            nt_config, Struct(), True,\n",
    "            lambda epoch, loss, is_iter:\n",
    "                self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "        self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        f_star = self.f_model()\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f76bd0-a0ea-4321-acc6-c2dc85ec1340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
    "def error():\n",
    "    u_pred, _ = pinn.predict(X_star)\n",
    "    return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b8622-109d-4101-ac76-e0cdc5039a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b13951-269a-48dd-85f3-9986badd5257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,Exact_u, X, T, x, t, file=\"cont_example1\")"
   ]
  },
  {
   "attachments": {
    "2c26c9a7-e458-47b2-9dbf-d2297ee912ed.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFlCAYAAADYnoD9AAAJMmlDQ1BkZWZhdWx0X3JnYi5pY2MA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\n"
    }
   },
   "cell_type": "markdown",
   "id": "708a1232-9cdc-4bcd-a707-c9229f70a83a",
   "metadata": {},
   "source": [
    "![cont_example1.png](attachment:2c26c9a7-e458-47b2-9dbf-d2297ee912ed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f63661-74c8-41ca-bbd8-42fb6e0f8bc4",
   "metadata": {},
   "source": [
    "# 2. Discrete Inference\n",
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN: (TODO)\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss: (TODO)\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b653404-5e23-4748-9e15-8fe1b9b2a4d9",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda3873-bd7c-4bc8-96e8-9735f9b0b58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data size on initial condition on u\n",
    "N_n = 250\n",
    "# Number of RK stages\n",
    "q = 500\n",
    "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q+1-sized output [u_1^n(x), ..., u_{q+1}^n(x)]\n",
    "layers = [1, 50, 50, 50, q + 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 200\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  lr=0.001,\n",
    "  beta_1=0.9,\n",
    "  beta_2=0.999,\n",
    "  epsilon=1e-08)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 1000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4a71c-5613-4436-808e-260f19792482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times):\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.nu = nu\n",
    "\n",
    "        self.dt = dt\n",
    "\n",
    "        self.q = max(q,1)\n",
    "        self.IRK_weights = IRK_weights\n",
    "        self.IRK_times = IRK_times\n",
    "\n",
    "        # Descriptive Keras model [2, 50, …, 50, q+1]\n",
    "        self.U_1_model = tf.keras.Sequential()\n",
    "        self.U_1_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        # Normalize\n",
    "        self.U_1_model.add(tf.keras.layers.Lambda(\n",
    "          lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            self.U_1_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "          if i != 1:\n",
    "            self.sizes_w.append(int(width * layers[1]))\n",
    "            self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        self.x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "    def U_0_model(self, x):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "          # Watching the two inputs we’ll need later, x and t\n",
    "          tape.watch(x)\n",
    "          tape.watch(self.dummy_x0_tf)\n",
    "\n",
    "          # Getting the prediction, and removing the last item (q+1)\n",
    "          U_1 = self.U_1_model(x) # shape=(len(x), q+1)\n",
    "          U = U_1[:, :-1] # shape=(len(x), q)\n",
    "\n",
    "          # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
    "          g_U = tape.gradient(U, x, output_gradients=self.dummy_x0_tf)\n",
    "          U_x = tape.gradient(g_U, self.dummy_x0_tf)\n",
    "          g_U_x = tape.gradient(U_x, x, output_gradients=self.dummy_x0_tf)\n",
    "        \n",
    "        # Doing the last one outside the with, to optimize performance\n",
    "        # Impossible to do for the earlier grad, because they’re needed after\n",
    "        U_xx = tape.gradient(g_U_x, self.dummy_x0_tf)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "\n",
    "        # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
    "        nu = self.get_params(numpy=True)\n",
    "        N = U*U_x - nu*U_xx # shape=(len(x), q)\n",
    "        return U_1 + self.dt*tf.matmul(N, self.IRK_weights.T)\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self, u_0, u_0_pred):\n",
    "        u_1_pred = self.U_1_model(self.x_1)\n",
    "        return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
    "          tf.reduce_sum(tf.square(u_1_pred))\n",
    "\n",
    "    def __grad(self, x_0, u_0):\n",
    "        with tf.GradientTape() as tape:\n",
    "          loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.U_1_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.U_1_model.layers[1:]:\n",
    "          weights_biases = layer.get_weights()\n",
    "          weights = weights_biases[0].flatten()\n",
    "          biases = weights_biases[1]\n",
    "          w.extend(weights)\n",
    "          w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.U_1_model.layers[1:]):\n",
    "          start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "          end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "          weights = w[start_weights:end_weights]\n",
    "          w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "          weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "          biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "          weights_biases = [weights, biases]\n",
    "          layer.set_weights(weights_biases)\n",
    "\n",
    "    def get_params(self, numpy=False):\n",
    "        return self.nu\n",
    "\n",
    "    def summary(self):\n",
    "        return self.U_1_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, x_0, u_0, tf_epochs, nt_config):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
    "        u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
    "\n",
    "        # Creating dummy tensors for the gradients\n",
    "        self.dummy_x0_tf = tf.ones([x_0.shape[0], self.q], dtype=self.dtype)\n",
    "\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "          # Optimization step\n",
    "          loss_value, grads = self.__grad(x_0, u_0)\n",
    "          self.optimizer.apply_gradients(\n",
    "            zip(grads, self.__wrap_training_variables()))\n",
    "          self.logger.log_train_epoch(epoch, loss_value)\n",
    "\n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "          with tf.GradientTape() as tape:\n",
    "            self.set_weights(w)\n",
    "            loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
    "          grad = tape.gradient(loss_value, self.U_1_model.trainable_variables)\n",
    "          grad_flat = []\n",
    "          for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "          grad_flat =  tf.concat(grad_flat, 0)\n",
    "          return loss_value, grad_flat\n",
    "        # tfp.optimizer.lbfgs_minimize(\n",
    "        #   loss_and_flat_grad,\n",
    "        #   initial_position=self.get_weights(),\n",
    "        #   num_correction_pairs=nt_config.nCorrection,\n",
    "        #   max_iterations=nt_config.maxIter,\n",
    "        #   f_relative_tolerance=nt_config.tolFun,\n",
    "        #   tolerance=nt_config.tolFun,\n",
    "        #   parallel_iterations=6)\n",
    "        lbfgs(loss_and_flat_grad,\n",
    "          self.get_weights(),\n",
    "          nt_config, Struct(), True,\n",
    "          lambda epoch, loss, is_iter:\n",
    "            self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "        \n",
    "        self.logger.log_train_end(tf_epochs)\n",
    "\n",
    "    def predict(self, x_star):\n",
    "        u_star = self.U_1_model(x_star)[:, -1]\n",
    "        return u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260c544-7d52-4131-b415-3ecc780b0e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "idx_t_0 = 10\n",
    "idx_t_1 = 90\n",
    "nu = 0.01/np.pi\n",
    "\n",
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, dt, \\\n",
    "  Exact_u, x_0, u_0, x_1, x_star, u_star, \\\n",
    "  IRK_weights, IRK_times = prep_data(path, N_n=N_n, q=q, lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times)\n",
    "def error():\n",
    "    u_pred = pinn.predict(x_star)\n",
    "    return np.linalg.norm(u_pred - u_star, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(x_0, u_0, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_1_pred = pinn.predict(x_star)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_1_pred = pinn.predict(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac99754-6b61-402b-a1bc-1b91ef99ace6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import os\n",
    "import numpy as np\n",
    "from pyDOE import lhs\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.use('pgf')\n",
    "\n",
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 390.0                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "\n",
    "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
    "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
    "    \"text.usetex\": True,                # use LaTeX to write all text\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
    "    \"font.sans-serif\": [],\n",
    "    \"font.monospace\": [],\n",
    "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
    "    \"font.size\": 10,\n",
    "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
    "    \"pgf.preamble\": r'\\usepackage[utf8x]{inputenc}\\usepackage[T1]{fontenc}',\n",
    "    }\n",
    "mpl.rcParams.update(pgf_with_latex)\n",
    "\n",
    "# I make my own newfig and savefig functions\n",
    "def newfig(width, nplots = 1):\n",
    "    fig = plt.figure(figsize=figsize(width, nplots))\n",
    "    ax = fig.add_subplot(111)\n",
    "    return fig, ax\n",
    "\n",
    "def savefig(filename, crop = True):\n",
    "    if crop == True:\n",
    "        # plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        # plt.savefig('{}.pdf'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        # plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "        plt.savefig('{}.png'.format(filename), bbox_inches='tight', pad_inches=0)\n",
    "    else:\n",
    "        # plt.savefig('{}.pgf'.format(filename))\n",
    "        # plt.savefig('{}.pdf'.format(filename))\n",
    "        # plt.savefig('{}.eps'.format(filename))\n",
    "        plt.savefig('{}.png'.format(filename))\n",
    "\n",
    "def prep_data(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
    "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
    "    data = scipy.io.loadmat(path)\n",
    "\n",
    "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
    "    t = data['t'].flatten()[:,None] # T x 1\n",
    "    x = data['x'].flatten()[:,None] # N x 1\n",
    "\n",
    "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
    "    Exact_u = np.real(data['usol']).T # T x N\n",
    "\n",
    "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
    "        dt = t[idx_t_1] - t[idx_t_0]\n",
    "        idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
    "        x_0 = x[idx_x,:]\n",
    "        u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
    "        u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "\n",
    "        # Boudanry data\n",
    "        x_1 = np.vstack((lb, ub))\n",
    "\n",
    "        # Test data\n",
    "        x_star = x\n",
    "        u_star = Exact_u[idx_t_1,:]\n",
    "\n",
    "        # Load IRK weights\n",
    "        tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "        IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
    "        IRK_times = tmp[q**2+q:]\n",
    "\n",
    "        return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
    "\n",
    "    # Meshing x and t in 2D (256,100)\n",
    "    X, T = np.meshgrid(x,t)\n",
    "\n",
    "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
    "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "    # Preparing the testing u_star\n",
    "    u_star = Exact_u.flatten()[:,None]\n",
    "                \n",
    "    # Noiseless data TODO: add support for noisy data    \n",
    "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "    X_u_train = X_star[idx,:]\n",
    "    u_train = u_star[idx,:]\n",
    "\n",
    "    if N_0 != None and N_1 != None:\n",
    "        Exact_u = Exact_u.T\n",
    "        idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
    "        x_0 = x[idx_x,:]\n",
    "        u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
    "        u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
    "\n",
    "        idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
    "        x_1 = x[idx_x,:]\n",
    "        u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
    "        u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
    "\n",
    "        dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
    "        q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
    "\n",
    "        # Load IRK weights\n",
    "        tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
    "        weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
    "        IRK_alpha = weights[0:-1,:]\n",
    "        IRK_beta = weights[-1:,:] \n",
    "        return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
    "\n",
    "    if N_f == None:\n",
    "        lb = X_star.min(axis=0)\n",
    "        ub = X_star.max(axis=0) \n",
    "        return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
    "\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
    "    lb = X_star.min(axis=0)\n",
    "    ub = X_star.max(axis=0) \n",
    "    # Getting the initial conditions (t=0)\n",
    "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
    "    uu1 = Exact_u[0:1,:].T\n",
    "    # Getting the lowest boundary conditions (x=-1) \n",
    "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
    "    uu2 = Exact_u[:,0:1]\n",
    "    # Getting the highest boundary conditions (x=1) \n",
    "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
    "    uu3 = Exact_u[:,-1:]\n",
    "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
    "    X_u_train = np.vstack([xx1, xx2, xx3])\n",
    "    u_train = np.vstack([uu1, uu2, uu3])\n",
    "\n",
    "    # Generating the x and t collocation points for f, with each having a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
    "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
    "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
    "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
    "    X_u_train = X_u_train[idx,:]\n",
    "    # Getting the corresponding u_train\n",
    "    u_train = u_train [idx,:]\n",
    "\n",
    "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, frequency=10):\n",
    "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __get_elapsed(self):\n",
    "        return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "    def __get_error_u(self):\n",
    "        return self.error_fn()\n",
    "\n",
    "    def set_error_fn(self, error_fn):\n",
    "        self.error_fn = error_fn\n",
    "  \n",
    "    def log_train_start(self, model):\n",
    "        print(\"\\nTraining started\")\n",
    "        print(\"================\")\n",
    "        self.model = model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "        if epoch % self.frequency == 0:\n",
    "              print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "    def log_train_opt(self, name):\n",
    "        # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "        print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "    def log_train_end(self, epoch, custom=\"\"):\n",
    "        print(\"==================\")\n",
    "        print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, file=None):\n",
    "\n",
    "    # Interpolating the results on the whole (x,t) domain.\n",
    "    # griddata(points, values, points at which to interpolate, method)\n",
    "    U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
    "\n",
    "    # Creating the figures\n",
    "    fig, ax = newfig(1.0, 1.1)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ####### Row 0: u(t,x) ##################    \n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
    "\n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "    ####### Row 1: u(t,x) slices ##################    \n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 2])\n",
    "    ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])    \n",
    "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if file != None:\n",
    "        savefig(file)\n",
    "\n",
    "def plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, file=None):\n",
    "    fig, ax = newfig(1.0, 1.2)\n",
    "    ax.axis('off')\n",
    "\n",
    "    ####### Row 0: h(t,x) ##################    \n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1/2 + 0.1, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "    h = ax.imshow(Exact_u.T, interpolation='nearest', cmap='rainbow', \n",
    "                extent=[t.min(), t.max(), x_star.min(), x_star.max()], \n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    ax.plot(t[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    leg = ax.legend(frameon=False, loc = 'best')\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "\n",
    "    ####### Row 1: h(t,x) slices ##################    \n",
    "    gs1 = gridspec.GridSpec(1, 2)\n",
    "    gs1.update(top=1-1/2-0.05, bottom=0.15, left=0.15, right=0.85, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x,Exact_u[idx_t_0,:], 'b-', linewidth = 2) \n",
    "    ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')      \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = %.2f$' % (t[idx_t_0]), fontsize = 10)\n",
    "    ax.set_xlim([lb-0.1, ub+0.1])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.8, -0.3), ncol=2, frameon=False)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x, Exact_u[idx_t_1,:], 'b-', linewidth = 2, label = 'Exact') \n",
    "    ax.plot(x_star, u_1_pred, 'r--', linewidth = 2, label = 'Prediction')      \n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = %.2f$' % (t[idx_t_1]), fontsize = 10)    \n",
    "    ax.set_xlim([lb-0.1, ub+0.1])\n",
    "\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.1, -0.3), ncol=2, frameon=False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if file != None:\n",
    "        savefig(file)\n",
    "\n",
    "\n",
    "def plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
    "                          ub, lb, u_1_pred, Exact, lambda_1_value, lambda_1_value_noisy, \n",
    "                          lambda_2_value, lambda_2_value_noisy,\n",
    "                          x, t, file=None):  \n",
    "    fig, ax = newfig(1.0, 1.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1/3+0.05, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "    h = ax.imshow(Exact, interpolation='nearest', cmap='rainbow',\n",
    "                extent=[t_star.min(),t_star.max(), lb[0], ub[0]],\n",
    "                origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "\n",
    "    line = np.linspace(x_star.min(), x_star.max(), 2)[:,None]\n",
    "    ax.plot(t_star[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1.0)\n",
    "    ax.plot(t_star[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1.0)    \n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "    gs1 = gridspec.GridSpec(1, 2)\n",
    "    gs1.update(top=1-1/3-0.1, bottom=1-2/3, left=0.15, right=0.85, wspace=0.5)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x_star,Exact[:,idx_t_0][:,None], 'b', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_0], u_0.shape[0]), fontsize = 10)\n",
    "\n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x_star,Exact[:,idx_t_1][:,None], 'b', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x_1, u_1, 'rx', linewidth = 2, label = 'Data')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_1], u_1.shape[0]), fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(-0.3, -0.3), ncol=2, frameon=False)\n",
    "\n",
    "    gs2 = gridspec.GridSpec(1, 2)\n",
    "    gs2.update(top=1-2/3-0.05, bottom=0, left=0.15, right=0.85, wspace=0.0)\n",
    "\n",
    "    ax = plt.subplot(gs2[0, 0])\n",
    "    ax.axis('off')\n",
    "    nu = 0.01/np.pi\n",
    "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x + %.6f u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & ' % (nu)\n",
    "    s2 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
    "    s3 = r'Identified PDE (1\\% noise) & '\n",
    "    s4 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
    "    s5 = r'\\end{tabular}$'\n",
    "    s = s1+s2+s3+s4+s5\n",
    "    ax.text(-0.1,0.2,s)\n",
    "    plt.show()\n",
    "\n",
    "def plot_ide_cont_results(X_star, u_pred, X_u_train, u_train, \\\n",
    "                          Exact_u, X, T, x, t, lambda_1_value, \\\n",
    "                          lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy):\n",
    "    fig, ax = newfig(1.0, 1.4)\n",
    "    ax.axis('off')\n",
    "\n",
    "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "    \n",
    "    ####### Row 0: u(t,x) ##################    \n",
    "    gs0 = gridspec.GridSpec(1, 2)\n",
    "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
    "    ax = plt.subplot(gs0[:, :])\n",
    "    \n",
    "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "                  origin='lower', aspect='auto')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(h, cax=cax)\n",
    "    \n",
    "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
    "    \n",
    "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "    \n",
    "    ax.set_xlabel('$t$')\n",
    "    ax.set_ylabel('$x$')\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
    "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "    \n",
    "    ####### Row 1: u(t,x) slices ##################    \n",
    "    gs1 = gridspec.GridSpec(1, 3)\n",
    "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
    "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')    \n",
    "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 1])\n",
    "    ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])\n",
    "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
    "    \n",
    "    ax = plt.subplot(gs1[0, 2])\n",
    "    ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$u(t,x)$')\n",
    "    ax.axis('square')\n",
    "    ax.set_xlim([-1.1,1.1])\n",
    "    ax.set_ylim([-1.1,1.1])    \n",
    "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
    "    \n",
    "    ####### Row 3: Identified PDE ##################    \n",
    "    gs2 = gridspec.GridSpec(1, 3)\n",
    "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
    "    \n",
    "    ax = plt.subplot(gs2[:, :])\n",
    "    ax.axis('off')\n",
    "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
    "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
    "    s3 = r'Identified PDE (1\\% noise) & '\n",
    "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
    "    s5 = r'\\end{tabular}$'\n",
    "    s = s1+s2+s3+s4+s5\n",
    "    ax.text(0.1,0.1,s)\n",
    "    plt.show()\n",
    "    # savefig('./figures/Burgers_identification')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, file=\"disc_example1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a68d3-fdcf-4420-a7bb-6fa639dc05bd",
   "metadata": {},
   "source": [
    "# 3. Continuous Identification\n",
    "\n",
    "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
    "\n",
    "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator.\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44caa71-49d3-4ece-b079-05b74bd37a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Data size on the solution u\n",
    "N_u = 2000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.001)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 1000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80a005-852e-4bb3-ae81-6a45b0bba054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, ub, lb):\n",
    "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "    self.u_model = tf.keras.Sequential()\n",
    "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.u_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]:\n",
    "        self.u_model.add(tf.keras.layers.Dense(\n",
    "          width, activation=tf.nn.tanh,\n",
    "          kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Defining the two additional trainable variables for identification\n",
    "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
    "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
    "    \n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "  # The actual PINN\n",
    "  def __f_model(self, X_u):\n",
    "    l1, l2 = self.get_params()\n",
    "    # Separating the collocation coordinates\n",
    "    x_f = tf.convert_to_tensor(X_u[:, 0:1], dtype=self.dtype)\n",
    "    t_f = tf.convert_to_tensor(X_u[:, 1:2], dtype=self.dtype)\n",
    "\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(x_f)\n",
    "      tape.watch(t_f)\n",
    "      # Packing together the inputs\n",
    "      X_f = tf.stack([x_f[:,0], t_f[:,0]], axis=1)\n",
    "\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(X_f)\n",
    "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "      u_x = tape.gradient(u, x_f)\n",
    "    \n",
    "    # Getting the other derivatives\n",
    "    u_xx = tape.gradient(u_x, x_f)\n",
    "    u_t = tape.gradient(u, t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_t + l1*u*u_x - l2*u_xx\n",
    "\n",
    "  # Defining custom loss\n",
    "  def __loss(self, X_u, u, u_pred):\n",
    "    f_pred = self.__f_model(X_u)\n",
    "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, X, u):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(X, u, self.u_model(X))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    var.extend([self.lambda_1, self.lambda_2])\n",
    "    return var\n",
    "\n",
    "  def get_weights(self):\n",
    "      w = []\n",
    "      for layer in self.u_model.layers[1:]:\n",
    "        weights_biases = layer.get_weights()\n",
    "        weights = weights_biases[0].flatten()\n",
    "        biases = weights_biases[1]\n",
    "        w.extend(weights)\n",
    "        w.extend(biases)\n",
    "      w.extend(self.lambda_1.numpy())\n",
    "      w.extend(self.lambda_2.numpy())\n",
    "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "    self.lambda_1.assign([w[-2]])\n",
    "    self.lambda_2.assign([w[-1]])\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    if numpy:\n",
    "      return l1.numpy()[0], l2.numpy()[0]\n",
    "    return l1, l2\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, X_u, u, tf_epochs, nt_config):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "    def log_train_epoch(epoch, loss, is_iter):\n",
    "      l1, l2 = self.get_params(numpy=True)\n",
    "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
    "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(X_u, u)\n",
    "      self.optimizer.apply_gradients(\n",
    "        zip(grads, self.__wrap_training_variables()))\n",
    "      log_train_epoch(epoch, loss_value, False)\n",
    "\n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        tape.watch(self.lambda_1)\n",
    "        tape.watch(self.lambda_2)\n",
    "        loss_value = self.__loss(X_u, u, self.u_model(X_u))\n",
    "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True, log_train_epoch)\n",
    "    \n",
    "    l1, l2 = self.get_params(numpy=True)\n",
    "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
    "\n",
    "  def predict(self, X_star):\n",
    "    u_star = self.u_model(X_star)\n",
    "    f_star = self.__f_model(X_star)\n",
    "    return u_star.numpy(), f_star.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959498b-26cf-4b4d-95c3-0c0a743c0097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.0)\n",
    "lambdas_star = (1.0, 0.01/np.pi)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
    "def error():\n",
    "    l1, l2 = pinn.get_params(numpy=True)\n",
    "    l1_star, l2_star = lambdas_star\n",
    "    error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
    "    error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
    "    return (error_lambda_1 + error_lambda_2) / 2\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_star)\n",
    "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
    "\n",
    "# Noise case\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.01)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "lambda_1_pred_noise, lambda_2_pred_noise = pinn.get_params(numpy=True)\n",
    "\n",
    "print(\"l1: \", lambda_1_pred)\n",
    "print(\"l2: \", lambda_2_pred)\n",
    "print(\"l1_noise: \", lambda_1_pred_noise)\n",
    "print(\"l2_noise: \", lambda_2_pred_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc892cb1-62ef-4bde-9d53-551d9b2896c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_ide_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, lambda_1_pred, lambda_1_pred_noise, lambda_2_pred, lambda_2_pred_noise, file=\"cont_example2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d99f1-eea4-4aa4-86d0-b1de63fca1b9",
   "metadata": {},
   "source": [
    "# 4. Discrete Identification\n",
    "\n",
    "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
    "\n",
    "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator.\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7896958-3e79-47fc-9f92-b71ffa047e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data size on initial condition on u\n",
    "N_0 = 199\n",
    "N_1 = 201\n",
    "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q-sized output defined later [u_1^n(x), ..., u_{q+1}^n(x)]\n",
    "layers = [1, 50, 50, 50, 0]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  lr=0.001,\n",
    "  beta_1=0.9,\n",
    "  beta_2=0.999,\n",
    "  epsilon=1e-08)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993a48c-f41c-454a-8a28-e95434ca43f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta):\n",
    "    self.lb = lb\n",
    "    self.ub = ub\n",
    "\n",
    "    self.dt = dt\n",
    "\n",
    "    self.q = max(q,1)\n",
    "    self.IRK_alpha = IRK_alpha\n",
    "    self.IRK_beta = IRK_beta\n",
    "\n",
    "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
    "    self.U_model = tf.keras.Sequential()\n",
    "    self.U_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.U_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]:\n",
    "        self.U_model.add(tf.keras.layers.Dense(\n",
    "          width, activation=tf.nn.tanh,\n",
    "          kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "  def __autograd(self, U, x, dummy):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(x)\n",
    "      tape.watch(dummy)\n",
    "\n",
    "      # Getting the prediction\n",
    "      U = self.U_model(x) # shape=(len(x), q)\n",
    "\n",
    "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
    "      g_U = tape.gradient(U, x, output_gradients=dummy)\n",
    "      U_x = tape.gradient(g_U, dummy)\n",
    "      g_U_x = tape.gradient(U_x, x, output_gradients=dummy)\n",
    "    \n",
    "    # Doing the last one outside the with, to optimize performance\n",
    "    # Impossible to do for the earlier grad, because they’re needed after\n",
    "    U_xx = tape.gradient(g_U_x, dummy)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    return U_x, U_xx\n",
    "\n",
    "  def U_0_model(self, x, customDummy=None):\n",
    "    U = self.U_model(x)\n",
    "    if customDummy != None:\n",
    "      dummy = customDummy\n",
    "    else:\n",
    "      dummy = self.dummy_x_0\n",
    "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    N = l1*U*U_x - l2*U_xx # shape=(len(x), q)\n",
    "    return U + self.dt*tf.matmul(N, self.IRK_alpha.T)\n",
    "\n",
    "  def U_1_model(self, x, customDummy=None):\n",
    "    U = self.U_model(x)\n",
    "    #dummy = customDummy or self.dummy_x_1\n",
    "    if customDummy != None:\n",
    "      dummy = customDummy\n",
    "    else:\n",
    "      dummy = self.dummy_x_1\n",
    "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
    "\n",
    "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    N = -l1*U*U_x + l2*U_xx # shape=(len(x), q)\n",
    "    return U + self.dt*tf.matmul(N, (self.IRK_beta - self.IRK_alpha).T)\n",
    "\n",
    "  # Defining custom loss\n",
    "  def __loss(self, x_0, u_0, x_1, u_1):\n",
    "    u_0_pred = self.U_0_model(x_0)\n",
    "    u_1_pred = self.U_1_model(x_1)\n",
    "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
    "      tf.reduce_sum(tf.square(u_1_pred - u_1))\n",
    "\n",
    "  def __grad(self, x_0, u_0, x_1, u_1):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.U_model.trainable_variables\n",
    "    var.extend([self.lambda_1, self.lambda_2])\n",
    "    return var\n",
    "\n",
    "  def get_weights(self):\n",
    "      w = []\n",
    "      for layer in self.U_model.layers[1:]:\n",
    "        weights_biases = layer.get_weights()\n",
    "        weights = weights_biases[0].flatten()\n",
    "        biases = weights_biases[1]\n",
    "        w.extend(weights)\n",
    "        w.extend(biases)\n",
    "      w.extend(self.lambda_1.numpy())\n",
    "      w.extend(self.lambda_2.numpy())\n",
    "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.U_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "    self.lambda_1.assign([w[-2]])\n",
    "    self.lambda_2.assign([w[-1]])\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    if numpy:\n",
    "      return l1.numpy()[0], l2.numpy()[0]\n",
    "    return l1, l2\n",
    "\n",
    "  def summary(self):\n",
    "    return self.U_model.summary()\n",
    "\n",
    "  def __createDummy(self, x):\n",
    "    return tf.ones([x.shape[0], self.q], dtype=self.dtype)\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, x_0, u_0, x_1, u_1, tf_epochs=1):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
    "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
    "    x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
    "    u_1 = tf.convert_to_tensor(u_1, dtype=self.dtype)\n",
    "\n",
    "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
    "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
    "\n",
    "    # Creating dummy tensors for the gradients\n",
    "    self.dummy_x_0 = self.__createDummy(x_0)\n",
    "    self.dummy_x_1 = self.__createDummy(x_1)\n",
    "\n",
    "    def log_train_epoch(epoch, loss, is_iter):\n",
    "      l1, l2 = self.get_params(numpy=True)\n",
    "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
    "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(x_0, u_0, x_1, u_1)\n",
    "      self.optimizer.apply_gradients(\n",
    "        zip(grads, self.__wrap_training_variables()))\n",
    "      log_train_epoch(epoch, loss_value, False)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        tape.watch(self.lambda_1)\n",
    "        tape.watch(self.lambda_2)\n",
    "        loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
    "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True, log_train_epoch)\n",
    "    \n",
    "    l1, l2 = self.get_params(numpy=True)\n",
    "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
    "\n",
    "  def predict(self, x_star):\n",
    "    x_star = tf.convert_to_tensor(x_star, dtype=self.dtype)\n",
    "    dummy = self.__createDummy(x_star)\n",
    "    U_0_star = self.U_0_model(x_star, dummy)\n",
    "    U_1_star = self.U_1_model(x_star, dummy)\n",
    "    return U_0_star, U_1_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465750b-c809-4c94-adeb-8abe875f4033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setup\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "idx_t_0 = 10\n",
    "skip = 80\n",
    "idx_t_1 = idx_t_0 + skip\n",
    "\n",
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
    "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
    "  lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "lambdas_star = (1.0, 0.01/np.pi)\n",
    "\n",
    "# Setting the output layer dynamically\n",
    "layers[-1] = q\n",
    " \n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
    "def error():\n",
    "  l1, l2 = pinn.get_params(numpy=True)\n",
    "  l1_star, l2_star = lambdas_star\n",
    "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
    "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
    "  return (error_lambda_1 + error_lambda_2) / 2\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
    "\n",
    "# Getting the model predictions\n",
    "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
    "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
    "\n",
    "# Noisy case (same as before with a different noise)\n",
    "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
    "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
    "  lb=lb, ub=ub, noise=0.01, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "layers[-1] = q\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
    "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
    "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
    "lambda_1_pred_noisy, lambda_2_pred_noisy = pinn.get_params(numpy=True)\n",
    "\n",
    "print(\"l1: \", lambda_1_pred)\n",
    "print(\"l2: \", lambda_2_pred)\n",
    "print(\"noisy l1: \", lambda_1_pred_noisy)\n",
    "print(\"noisy l2: \", lambda_2_pred_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79caa23-a49c-4bb1-8a42-0f0fd44fb8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
    "  ub, lb, U_1_pred, Exact_u, lambda_1_pred, lambda_1_pred_noisy, lambda_2_pred, lambda_2_pred_noisy, x_star, t_star, file=\"disc_example2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
