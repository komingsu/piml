{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e6e58e-5411-4c6d-b336-f4d63e63c805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/maziarraissi/PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d8d0fb-0dc9-4d67-8b7c-d65aa3f53619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc533e4f-17bf-4214-a6bb-ffb14a3cd0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210def54-a9e2-4c7c-a104-e7335dfd65c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f721a25c-6e50-4875-914e-ad4741f23cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repoPath = os.path.join(\".\", \"PINNs\")\n",
    "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
    "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
    "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
    "sys.path.insert(0, utilsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b6122d-9e09-48c4-b92d-987c11c5ebf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "    global global_time_list, global_last_time\n",
    "    global_time_list = []\n",
    "    global_last_time = time.perf_counter()\n",
    "    \n",
    "def record_time():\n",
    "    global global_last_time, global_time_list\n",
    "    new_time = time.perf_counter()\n",
    "    global_time_list.append(new_time - global_last_time)\n",
    "    global_last_time = time.perf_counter()\n",
    "    #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "    \"\"\"Returns last interval records in millis.\"\"\"\n",
    "    global global_last_time, global_time_list\n",
    "    if global_time_list:\n",
    "        return 1000 * global_time_list[-1]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54985d78-7123-4c8f-810f-1ef882114b29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdf6bea-aa06-48d3-ae50-ead82e862332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c1fd8f-2f3b-428b-9477-056c6026210a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dot(a, b):\n",
    "    \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "    return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c05fd18-5f9c-48b6-949b-63df0ce0f7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "    \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
    "    \"\"\"\n",
    "\n",
    "    if config.maxIter == 0:\n",
    "        return\n",
    "\n",
    "    global final_loss, times\n",
    "    \n",
    "    maxIter = config.maxIter\n",
    "    maxEval = config.maxEval or maxIter*1.25\n",
    "    tolFun = config.tolFun or 1e-5\n",
    "    tolX = config.tolX or 1e-19\n",
    "    nCorrection = config.nCorrection or 100\n",
    "    lineSearch = config.lineSearch\n",
    "    lineSearchOpts = config.lineSearchOptions\n",
    "    learningRate = config.learningRate or 1\n",
    "    isverbose = config.verbose or False\n",
    "\n",
    "    # verbose function\n",
    "    if isverbose:\n",
    "        verbose = verbose_func\n",
    "    else:\n",
    "        verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "    f, g = opfunc(x)\n",
    "\n",
    "    f_hist = [f]\n",
    "    currentFuncEval = 1\n",
    "    state.funcEval = state.funcEval + 1\n",
    "    p = g.shape[0]\n",
    "\n",
    "    # check optimality of initial point\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <= tolFun:\n",
    "        verbose(\"optimality condition below tolFun\")\n",
    "        return x, f_hist\n",
    "\n",
    "    # optimize for a max of maxIter iterations\n",
    "    nIter = 0\n",
    "    times = []\n",
    "    while nIter < maxIter:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # keep track of nb of iterations\n",
    "        nIter = nIter + 1\n",
    "        state.nIter = state.nIter + 1\n",
    "\n",
    "        ############################################################\n",
    "        ## compute gradient descent direction\n",
    "        ############################################################\n",
    "        if state.nIter == 1:\n",
    "            d = -g\n",
    "            old_dirs = []\n",
    "            old_stps = []\n",
    "            Hdiag = 1\n",
    "        else:\n",
    "            # do lbfgs update (update memory)\n",
    "            y = g - g_old\n",
    "            s = d*t\n",
    "            ys = dot(y, s)\n",
    "            \n",
    "            if ys > 1e-10:\n",
    "                # updating memory\n",
    "                if len(old_dirs) == nCorrection:\n",
    "                    # shift history by one (limited-memory)\n",
    "                    del old_dirs[0]\n",
    "                    del old_stps[0]\n",
    "\n",
    "                # store new direction/step\n",
    "                old_dirs.append(s)\n",
    "                old_stps.append(y)\n",
    "\n",
    "                # update scale of initial Hessian approximation\n",
    "                Hdiag = ys/dot(y, y)\n",
    "\n",
    "            # compute the approximate (L-BFGS) inverse Hessian \n",
    "            # multiplied by the gradient\n",
    "            k = len(old_dirs)\n",
    "\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            ro = [0]*nCorrection\n",
    "            for i in range(k):\n",
    "                ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "                \n",
    "\n",
    "            # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            al = [0]*nCorrection\n",
    "\n",
    "            q = -g\n",
    "            for i in range(k-1, -1, -1):\n",
    "                al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "                q = q - al[i]*old_stps[i]\n",
    "\n",
    "            # multiply by initial Hessian\n",
    "            r = q*Hdiag\n",
    "            for i in range(k):\n",
    "                be_i = dot(old_stps[i], r) * ro[i]\n",
    "                r += (al[i]-be_i)*old_dirs[i]\n",
    "                \n",
    "            d = r\n",
    "            # final direction is in r/d (same object)\n",
    "\n",
    "        g_old = g\n",
    "        f_old = f\n",
    "        \n",
    "        ############################################################\n",
    "        ## compute step length\n",
    "        ############################################################\n",
    "        # directional derivative\n",
    "        gtd = dot(g, d)\n",
    "\n",
    "        # check that progress can be made along that direction\n",
    "        if gtd > -tolX:\n",
    "            verbose(\"Can not make progress along direction.\")\n",
    "            break\n",
    "\n",
    "        # reset initial guess for step size\n",
    "        if state.nIter == 1:\n",
    "            tmp1 = tf.abs(g)\n",
    "            t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "        else:\n",
    "            t = learningRate\n",
    "\n",
    "\n",
    "        # optional line search: user function\n",
    "        lsFuncEval = 0\n",
    "        if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "            # perform line search, using user function\n",
    "            f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "            f_hist.append(f)\n",
    "        else:\n",
    "            # no line search, simply move with fixed-step\n",
    "            x += t*d\n",
    "            \n",
    "            if nIter != maxIter:\n",
    "                # re-evaluate function only if not in last iteration\n",
    "                # the reason we do this: in a stochastic setting,\n",
    "                # no use to re-evaluate that function here\n",
    "                f, g = opfunc(x)\n",
    "                lsFuncEval = 1\n",
    "                f_hist.append(f)\n",
    "\n",
    "\n",
    "        # update func eval\n",
    "        currentFuncEval = currentFuncEval + lsFuncEval\n",
    "        state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "        ############################################################\n",
    "        ## check conditions\n",
    "        ############################################################\n",
    "        if nIter == maxIter:\n",
    "            break\n",
    "\n",
    "        if currentFuncEval >= maxEval:\n",
    "            # max nb of function evals\n",
    "            verbose('max nb of function evals')\n",
    "            break\n",
    "\n",
    "        tmp1 = tf.abs(g)\n",
    "        if tf.reduce_sum(tmp1) <=tolFun:\n",
    "            # check optimality\n",
    "            verbose('optimality condition below tolFun')\n",
    "            break\n",
    "        \n",
    "        tmp1 = tf.abs(d*t)\n",
    "        if tf.reduce_sum(tmp1) <= tolX:\n",
    "            # step size below tolX\n",
    "            verbose('step size below tolX')\n",
    "            break\n",
    "\n",
    "        if tf.abs(f-f_old) < tolX:\n",
    "            # function value changing less than tolX\n",
    "            verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "            break\n",
    "\n",
    "        if do_verbose:\n",
    "            log_fn(nIter, f.numpy(), True)\n",
    "            #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "            record_time()\n",
    "            times.append(last_time())\n",
    "\n",
    "        if nIter == maxIter - 1:\n",
    "            final_loss = f.numpy()\n",
    "\n",
    "\n",
    "    # save state\n",
    "    state.old_dirs = old_dirs\n",
    "    state.old_stps = old_stps\n",
    "    state.Hdiag = Hdiag\n",
    "    state.g_old = g_old\n",
    "    state.f_old = f_old\n",
    "    state.t = t\n",
    "    state.d = d\n",
    "\n",
    "    return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "    pass\n",
    "\n",
    "class Struct(dummy):\n",
    "    def __getattribute__(self, key):\n",
    "        if key == '__dict__':\n",
    "            return super(dummy, self).__getattribute__('__dict__')\n",
    "        return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83a33c-0790-439f-9d43-c0a9c6a998ff",
   "metadata": {},
   "source": [
    "# Continuous inference\n",
    "\n",
    "$$u_t + u u_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8492f95-8da7-4fcb-8fda-9b649160bd6a",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269b0772-7f4c-4747-9d11-4aa92b82d599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 50\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.1,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d59d7bc-047a-498c-8c03-01d6b29b4e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
    "        # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "        self.u_model = tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(\n",
    "            lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            self.u_model.add(tf.keras.layers.Dense(\n",
    "                width, activation=tf.nn.tanh,\n",
    "                kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.nu = nu\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
    "        self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
    "        \n",
    "    # Defining custom loss\n",
    "    def __loss(self, u, u_pred):\n",
    "        f_pred = self.f_model()\n",
    "        return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "    def __grad(self, X, u):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss(u, self.u_model(X))\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the two inputs we’ll need later, x and t\n",
    "            tape.watch(self.x_f)\n",
    "            tape.watch(self.t_f)\n",
    "            # Packing together the inputs\n",
    "            X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
    "\n",
    "            # Getting the prediction\n",
    "            u = self.u_model(X_f)\n",
    "            # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "            u_x = tape.gradient(u, self.x_f)\n",
    "        \n",
    "        # Getting the other derivatives\n",
    "        u_xx = tape.gradient(u_x, self.x_f)\n",
    "        u_t = tape.gradient(u, self.t_f)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "\n",
    "        nu = self.get_params(numpy=True)\n",
    "\n",
    "        # Buidling the PINNs\n",
    "        return u_t + u*u_x - nu*u_xx\n",
    "\n",
    "    def get_params(self, numpy=False):\n",
    "        return self.nu\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.u_model.layers[1:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad(X_u, u)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            self.logger.log_train_epoch(epoch, loss_value)\n",
    "        \n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights(w)\n",
    "                loss_value = self.__loss(u, self.u_model(X_u))\n",
    "            grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat =  tf.concat(grad_flat, 0)\n",
    "            return loss_value, grad_flat\n",
    "        # tfp.optimizer.lbfgs_minimize(\n",
    "        #   loss_and_flat_grad,\n",
    "        #   initial_position=self.get_weights(),\n",
    "        #   num_correction_pairs=nt_config.nCorrection,\n",
    "        #   max_iterations=nt_config.maxIter,\n",
    "        #   f_relative_tolerance=nt_config.tolFun,\n",
    "        #   tolerance=nt_config.tolFun,\n",
    "        #   parallel_iterations=6)\n",
    "        lbfgs(loss_and_flat_grad,\n",
    "            self.get_weights(),\n",
    "            nt_config, Struct(), True,\n",
    "            lambda epoch, loss, is_iter:\n",
    "                self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "        self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        f_star = self.f_model()\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f76bd0-a0ea-4321-acc6-c2dc85ec1340",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n",
      "Eager execution: True\n",
      "WARNING:tensorflow:From /practice/piml/Exercise_Alone/pinn_TF2/utils.py:173: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU-accerelated: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 05:41:12.988411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 05:41:13.020994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.028208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.028483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.509541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.509810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.509823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-22 05:41:13.510052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.510097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-11-22 05:41:13.543781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.544428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.544853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.546461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.546744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.547005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.547410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.547423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-22 05:41:13.547699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:13.547730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f, noise=0.0)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
    "def error():\n",
    "    u_pred, _ = pinn.predict(X_star)\n",
    "    return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b8622-109d-4101-ac76-e0cdc5039a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b13951-269a-48dd-85f3-9986badd5257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,Exact_u, X, T, x, t, file=\"cont_example1\")"
   ]
  },
  {
   "attachments": {
    "2c26c9a7-e458-47b2-9dbf-d2297ee912ed.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFlCAYAAADYnoD9AAAJMmlDQ1BkZWZhdWx0X3JnYi5pY2MA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\n"
    }
   },
   "cell_type": "markdown",
   "id": "708a1232-9cdc-4bcd-a707-c9229f70a83a",
   "metadata": {},
   "source": [
    "![cont_example1.png](attachment:2c26c9a7-e458-47b2-9dbf-d2297ee912ed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f63661-74c8-41ca-bbd8-42fb6e0f8bc4",
   "metadata": {},
   "source": [
    "# 2. Discrete Inference\n",
    "$$u_t + uu_x - \\nu u_{xx} = 0$$\n",
    "\n",
    "With $x \\in [-1,1],\\quad t \\in [0,1],\\quad \\nu = (0.01/\\pi)$.\n",
    "\n",
    "And $u(0,x) = -\\sin(\\pi x),\\quad u(t,-1) = u(t,1) = 0$\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN: (TODO)\n",
    "$$f := u_t + u u_x - \\nu u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss: (TODO)\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t_f^i,x_f^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ and $\\{t_f^i, x_f^i\\}_{i=1}^{N_f}$ respectively the initial/boundary data on $u(t,x)$ and collocations points for $f(t,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b653404-5e23-4748-9e15-8fe1b9b2a4d9",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efda3873-bd7c-4bc8-96e8-9735f9b0b58b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data size on initial condition on u\n",
    "N_n = 250\n",
    "# Number of RK stages\n",
    "q = 500\n",
    "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q+1-sized output [u_1^n(x), ..., u_{q+1}^n(x)]\n",
    "layers = [1, 50, 50, 50, q + 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 200\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  lr=0.001,\n",
    "  beta_1=0.9,\n",
    "  beta_2=0.999,\n",
    "  epsilon=1e-08)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 1000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c4a71c-5613-4436-808e-260f19792482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times):\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.nu = nu\n",
    "\n",
    "        self.dt = dt\n",
    "\n",
    "        self.q = max(q,1)\n",
    "        self.IRK_weights = IRK_weights\n",
    "        self.IRK_times = IRK_times\n",
    "\n",
    "        # Descriptive Keras model [2, 50, …, 50, q+1]\n",
    "        self.U_1_model = tf.keras.Sequential()\n",
    "        self.U_1_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        # Normalize\n",
    "        self.U_1_model.add(tf.keras.layers.Lambda(\n",
    "          lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            self.U_1_model.add(tf.keras.layers.Dense(\n",
    "              width, activation=tf.nn.tanh,\n",
    "              kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "          if i != 1:\n",
    "            self.sizes_w.append(int(width * layers[1]))\n",
    "            self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        self.x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "    def U_0_model(self, x):\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "          # Watching the two inputs we’ll need later, x and t\n",
    "          tape.watch(x)\n",
    "          tape.watch(self.dummy_x0_tf)\n",
    "\n",
    "          # Getting the prediction, and removing the last item (q+1)\n",
    "          U_1 = self.U_1_model(x) # shape=(len(x), q+1)\n",
    "          U = U_1[:, :-1] # shape=(len(x), q)\n",
    "\n",
    "          # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
    "          g_U = tape.gradient(U, x, output_gradients=self.dummy_x0_tf)\n",
    "          U_x = tape.gradient(g_U, self.dummy_x0_tf)\n",
    "          g_U_x = tape.gradient(U_x, x, output_gradients=self.dummy_x0_tf)\n",
    "        \n",
    "        # Doing the last one outside the with, to optimize performance\n",
    "        # Impossible to do for the earlier grad, because they’re needed after\n",
    "        U_xx = tape.gradient(g_U_x, self.dummy_x0_tf)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "\n",
    "        # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
    "        nu = self.get_params(numpy=True)\n",
    "        N = U*U_x - nu*U_xx # shape=(len(x), q)\n",
    "        return U_1 + self.dt*tf.matmul(N, self.IRK_weights.T)\n",
    "\n",
    "    # Defining custom loss\n",
    "    def __loss(self, u_0, u_0_pred):\n",
    "        u_1_pred = self.U_1_model(self.x_1)\n",
    "        return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
    "          tf.reduce_sum(tf.square(u_1_pred))\n",
    "\n",
    "    def __grad(self, x_0, u_0):\n",
    "        with tf.GradientTape() as tape:\n",
    "          loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.U_1_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.U_1_model.layers[1:]:\n",
    "          weights_biases = layer.get_weights()\n",
    "          weights = weights_biases[0].flatten()\n",
    "          biases = weights_biases[1]\n",
    "          w.extend(weights)\n",
    "          w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.U_1_model.layers[1:]):\n",
    "          start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "          end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "          weights = w[start_weights:end_weights]\n",
    "          w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "          weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "          biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "          weights_biases = [weights, biases]\n",
    "          layer.set_weights(weights_biases)\n",
    "\n",
    "    def get_params(self, numpy=False):\n",
    "        return self.nu\n",
    "\n",
    "    def summary(self):\n",
    "        return self.U_1_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, x_0, u_0, tf_epochs, nt_config):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
    "        u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
    "\n",
    "        # Creating dummy tensors for the gradients\n",
    "        self.dummy_x0_tf = tf.ones([x_0.shape[0], self.q], dtype=self.dtype)\n",
    "\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "          # Optimization step\n",
    "          loss_value, grads = self.__grad(x_0, u_0)\n",
    "          self.optimizer.apply_gradients(\n",
    "            zip(grads, self.__wrap_training_variables()))\n",
    "          self.logger.log_train_epoch(epoch, loss_value)\n",
    "\n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "          with tf.GradientTape() as tape:\n",
    "            self.set_weights(w)\n",
    "            loss_value = self.__loss(u_0, self.U_0_model(x_0))\n",
    "          grad = tape.gradient(loss_value, self.U_1_model.trainable_variables)\n",
    "          grad_flat = []\n",
    "          for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "          grad_flat =  tf.concat(grad_flat, 0)\n",
    "          return loss_value, grad_flat\n",
    "        # tfp.optimizer.lbfgs_minimize(\n",
    "        #   loss_and_flat_grad,\n",
    "        #   initial_position=self.get_weights(),\n",
    "        #   num_correction_pairs=nt_config.nCorrection,\n",
    "        #   max_iterations=nt_config.maxIter,\n",
    "        #   f_relative_tolerance=nt_config.tolFun,\n",
    "        #   tolerance=nt_config.tolFun,\n",
    "        #   parallel_iterations=6)\n",
    "        lbfgs(loss_and_flat_grad,\n",
    "          self.get_weights(),\n",
    "          nt_config, Struct(), True,\n",
    "          lambda epoch, loss, is_iter:\n",
    "            self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "        \n",
    "        self.logger.log_train_end(tf_epochs)\n",
    "\n",
    "    def predict(self, x_star):\n",
    "        u_star = self.U_1_model(x_star)[:, -1]\n",
    "        return u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b260c544-7d52-4131-b415-3ecc780b0e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n",
      "Eager execution: True\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_1 (Lambda)           (None, 1)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                100       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 501)               25551     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,751\n",
      "Trainable params: 30,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 05:41:24.557602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:24.558203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:24.558518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:24.558888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:24.558902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-22 05:41:24.559121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 05:41:24.559149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 05:41:26.064462: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_epoch =      0  elapsed = 00:02  loss = 6.1188e+04  error = 9.9184e-01  \n",
      "tf_epoch =     10  elapsed = 00:02  loss = 5.3000e+04  error = 9.1167e-01  \n",
      "tf_epoch =     20  elapsed = 00:02  loss = 4.0422e+04  error = 8.3942e-01  \n",
      "tf_epoch =     30  elapsed = 00:03  loss = 3.0798e+04  error = 1.0075e+00  \n",
      "tf_epoch =     40  elapsed = 00:03  loss = 2.6586e+04  error = 1.2057e+00  \n",
      "tf_epoch =     50  elapsed = 00:04  loss = 2.3990e+04  error = 1.3010e+00  \n",
      "tf_epoch =     60  elapsed = 00:04  loss = 2.1986e+04  error = 1.3448e+00  \n",
      "tf_epoch =     70  elapsed = 00:05  loss = 2.0261e+04  error = 1.3669e+00  \n",
      "tf_epoch =     80  elapsed = 00:05  loss = 1.8732e+04  error = 1.3788e+00  \n",
      "tf_epoch =     90  elapsed = 00:05  loss = 1.7443e+04  error = 1.3852e+00  \n",
      "tf_epoch =    100  elapsed = 00:06  loss = 1.6429e+04  error = 1.3889e+00  \n",
      "tf_epoch =    110  elapsed = 00:06  loss = 1.5687e+04  error = 1.3913e+00  \n",
      "tf_epoch =    120  elapsed = 00:07  loss = 1.5193e+04  error = 1.3929e+00  \n",
      "tf_epoch =    130  elapsed = 00:07  loss = 1.4886e+04  error = 1.3941e+00  \n",
      "tf_epoch =    140  elapsed = 00:07  loss = 1.4672e+04  error = 1.3951e+00  \n",
      "tf_epoch =    150  elapsed = 00:08  loss = 1.4463e+04  error = 1.3959e+00  \n",
      "tf_epoch =    160  elapsed = 00:08  loss = 1.4275e+04  error = 1.3966e+00  \n",
      "tf_epoch =    170  elapsed = 00:09  loss = 1.4057e+04  error = 1.3972e+00  \n",
      "tf_epoch =    180  elapsed = 00:09  loss = 1.3781e+04  error = 1.3977e+00  \n",
      "tf_epoch =    190  elapsed = 00:10  loss = 1.3386e+04  error = 1.3985e+00  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:11  loss = 1.0356e+04  error = 1.4005e+00  \n",
      "nt_epoch =     20  elapsed = 00:11  loss = 8.1006e+03  error = 1.3995e+00  \n",
      "nt_epoch =     30  elapsed = 00:12  loss = 6.8200e+03  error = 1.3970e+00  \n",
      "nt_epoch =     40  elapsed = 00:13  loss = 5.4988e+03  error = 1.3881e+00  \n",
      "nt_epoch =     50  elapsed = 00:13  loss = 4.3255e+03  error = 1.3378e+00  \n",
      "nt_epoch =     60  elapsed = 00:14  loss = 3.1962e+03  error = 1.0390e+00  \n",
      "nt_epoch =     70  elapsed = 00:15  loss = 2.2686e+03  error = 2.5031e-01  \n",
      "nt_epoch =     80  elapsed = 00:16  loss = 1.7253e+03  error = 1.5188e-01  \n",
      "nt_epoch =     90  elapsed = 00:17  loss = 1.1808e+03  error = 2.6017e-01  \n",
      "nt_epoch =    100  elapsed = 00:17  loss = 7.5373e+02  error = 2.5343e-01  \n",
      "nt_epoch =    110  elapsed = 00:18  loss = 5.2352e+02  error = 2.3643e-01  \n",
      "nt_epoch =    120  elapsed = 00:19  loss = 4.1386e+02  error = 1.8747e-01  \n",
      "nt_epoch =    130  elapsed = 00:20  loss = 3.6543e+02  error = 1.6297e-01  \n",
      "nt_epoch =    140  elapsed = 00:21  loss = 2.7697e+02  error = 1.6528e-01  \n",
      "nt_epoch =    150  elapsed = 00:21  loss = 1.7829e+02  error = 1.6654e-01  \n",
      "nt_epoch =    160  elapsed = 00:22  loss = 1.3086e+02  error = 1.5984e-01  \n",
      "nt_epoch =    170  elapsed = 00:23  loss = 1.1823e+02  error = 1.4408e-01  \n",
      "nt_epoch =    180  elapsed = 00:24  loss = 1.0897e+02  error = 1.4156e-01  \n",
      "nt_epoch =    190  elapsed = 00:25  loss = 1.0275e+02  error = 1.3874e-01  \n",
      "nt_epoch =    200  elapsed = 00:25  loss = 9.8444e+01  error = 1.3203e-01  \n",
      "nt_epoch =    210  elapsed = 00:26  loss = 9.2495e+01  error = 1.2444e-01  \n",
      "nt_epoch =    220  elapsed = 00:27  loss = 8.5735e+01  error = 1.1526e-01  \n",
      "nt_epoch =    230  elapsed = 00:28  loss = 7.8657e+01  error = 1.0947e-01  \n",
      "nt_epoch =    240  elapsed = 00:29  loss = 7.2086e+01  error = 1.0527e-01  \n",
      "nt_epoch =    250  elapsed = 00:30  loss = 6.6598e+01  error = 9.8133e-02  \n",
      "nt_epoch =    260  elapsed = 00:30  loss = 6.3214e+01  error = 9.2249e-02  \n",
      "nt_epoch =    270  elapsed = 00:31  loss = 6.0690e+01  error = 8.8490e-02  \n",
      "nt_epoch =    280  elapsed = 00:32  loss = 5.7945e+01  error = 9.0182e-02  \n",
      "nt_epoch =    290  elapsed = 00:33  loss = 5.6109e+01  error = 8.6558e-02  \n",
      "nt_epoch =    300  elapsed = 00:34  loss = 5.3879e+01  error = 8.4852e-02  \n",
      "nt_epoch =    310  elapsed = 00:35  loss = 5.2241e+01  error = 8.2715e-02  \n",
      "nt_epoch =    320  elapsed = 00:35  loss = 5.0694e+01  error = 8.1769e-02  \n",
      "nt_epoch =    330  elapsed = 00:36  loss = 4.9931e+01  error = 8.2500e-02  \n",
      "nt_epoch =    340  elapsed = 00:37  loss = 4.7765e+01  error = 8.0817e-02  \n",
      "nt_epoch =    350  elapsed = 00:38  loss = 4.5887e+01  error = 8.0419e-02  \n",
      "nt_epoch =    360  elapsed = 00:39  loss = 4.4521e+01  error = 7.9620e-02  \n",
      "nt_epoch =    370  elapsed = 00:39  loss = 4.2723e+01  error = 7.8951e-02  \n",
      "nt_epoch =    380  elapsed = 00:40  loss = 4.1605e+01  error = 7.7261e-02  \n",
      "nt_epoch =    390  elapsed = 00:41  loss = 3.9226e+01  error = 7.4492e-02  \n",
      "nt_epoch =    400  elapsed = 00:42  loss = 3.7039e+01  error = 7.2892e-02  \n",
      "nt_epoch =    410  elapsed = 00:43  loss = 3.6238e+01  error = 7.1746e-02  \n",
      "nt_epoch =    420  elapsed = 00:44  loss = 3.5787e+01  error = 7.2776e-02  \n",
      "nt_epoch =    430  elapsed = 00:44  loss = 3.5105e+01  error = 7.2645e-02  \n",
      "nt_epoch =    440  elapsed = 00:45  loss = 3.3916e+01  error = 7.1522e-02  \n",
      "nt_epoch =    450  elapsed = 00:46  loss = 3.3000e+01  error = 7.1461e-02  \n",
      "nt_epoch =    460  elapsed = 00:47  loss = 3.1954e+01  error = 6.8887e-02  \n",
      "nt_epoch =    470  elapsed = 00:48  loss = 3.0676e+01  error = 6.8044e-02  \n",
      "nt_epoch =    480  elapsed = 00:49  loss = 2.9587e+01  error = 6.5318e-02  \n",
      "nt_epoch =    490  elapsed = 00:49  loss = 2.8705e+01  error = 6.1397e-02  \n",
      "nt_epoch =    500  elapsed = 00:50  loss = 2.7919e+01  error = 5.8235e-02  \n",
      "nt_epoch =    510  elapsed = 00:51  loss = 2.6953e+01  error = 5.6385e-02  \n",
      "nt_epoch =    520  elapsed = 00:52  loss = 2.6279e+01  error = 5.3951e-02  \n",
      "nt_epoch =    530  elapsed = 00:53  loss = 2.5715e+01  error = 5.3994e-02  \n",
      "nt_epoch =    540  elapsed = 00:54  loss = 2.5053e+01  error = 5.3510e-02  \n",
      "nt_epoch =    550  elapsed = 00:55  loss = 2.4654e+01  error = 5.1127e-02  \n",
      "nt_epoch =    560  elapsed = 00:55  loss = 2.4096e+01  error = 4.7525e-02  \n",
      "nt_epoch =    570  elapsed = 00:56  loss = 2.3056e+01  error = 4.6599e-02  \n",
      "nt_epoch =    580  elapsed = 00:57  loss = 2.2266e+01  error = 4.7002e-02  \n",
      "nt_epoch =    590  elapsed = 00:58  loss = 2.1671e+01  error = 4.8542e-02  \n",
      "nt_epoch =    600  elapsed = 00:59  loss = 2.0963e+01  error = 4.7786e-02  \n",
      "nt_epoch =    610  elapsed = 01:00  loss = 2.0327e+01  error = 4.5567e-02  \n",
      "nt_epoch =    620  elapsed = 01:01  loss = 1.9987e+01  error = 4.4113e-02  \n",
      "nt_epoch =    630  elapsed = 01:02  loss = 1.9548e+01  error = 4.3571e-02  \n",
      "nt_epoch =    640  elapsed = 01:03  loss = 1.9047e+01  error = 4.0485e-02  \n",
      "nt_epoch =    650  elapsed = 01:04  loss = 1.8571e+01  error = 3.9573e-02  \n",
      "nt_epoch =    660  elapsed = 01:05  loss = 1.8279e+01  error = 3.9667e-02  \n",
      "nt_epoch =    670  elapsed = 01:06  loss = 1.7972e+01  error = 3.7197e-02  \n",
      "nt_epoch =    680  elapsed = 01:07  loss = 1.7399e+01  error = 3.8177e-02  \n",
      "nt_epoch =    690  elapsed = 01:08  loss = 1.7054e+01  error = 3.6681e-02  \n",
      "nt_epoch =    700  elapsed = 01:09  loss = 1.6703e+01  error = 3.6162e-02  \n",
      "nt_epoch =    710  elapsed = 01:10  loss = 1.6154e+01  error = 3.5864e-02  \n",
      "nt_epoch =    720  elapsed = 01:11  loss = 1.5739e+01  error = 3.6870e-02  \n",
      "nt_epoch =    730  elapsed = 01:12  loss = 1.5142e+01  error = 3.6138e-02  \n",
      "nt_epoch =    740  elapsed = 01:13  loss = 1.4779e+01  error = 3.4913e-02  \n",
      "nt_epoch =    750  elapsed = 01:14  loss = 1.4512e+01  error = 3.3626e-02  \n",
      "nt_epoch =    760  elapsed = 01:15  loss = 1.4223e+01  error = 3.3157e-02  \n",
      "nt_epoch =    770  elapsed = 01:17  loss = 1.3892e+01  error = 3.2340e-02  \n",
      "nt_epoch =    780  elapsed = 01:18  loss = 1.3567e+01  error = 3.1091e-02  \n",
      "nt_epoch =    790  elapsed = 01:19  loss = 1.3320e+01  error = 3.0809e-02  \n",
      "nt_epoch =    800  elapsed = 01:20  loss = 1.2962e+01  error = 2.9926e-02  \n",
      "nt_epoch =    810  elapsed = 01:21  loss = 1.2675e+01  error = 2.9037e-02  \n",
      "nt_epoch =    820  elapsed = 01:22  loss = 1.2503e+01  error = 2.8709e-02  \n",
      "nt_epoch =    830  elapsed = 01:23  loss = 1.2219e+01  error = 2.8970e-02  \n",
      "nt_epoch =    840  elapsed = 01:24  loss = 1.2025e+01  error = 2.8654e-02  \n",
      "nt_epoch =    850  elapsed = 01:25  loss = 1.1813e+01  error = 2.8906e-02  \n",
      "nt_epoch =    860  elapsed = 01:26  loss = 1.1646e+01  error = 2.8060e-02  \n",
      "nt_epoch =    870  elapsed = 01:27  loss = 1.1493e+01  error = 2.7134e-02  \n",
      "nt_epoch =    880  elapsed = 01:28  loss = 1.1344e+01  error = 2.6368e-02  \n",
      "nt_epoch =    890  elapsed = 01:28  loss = 1.1246e+01  error = 2.6436e-02  \n",
      "nt_epoch =    900  elapsed = 01:29  loss = 1.1043e+01  error = 2.5491e-02  \n",
      "nt_epoch =    910  elapsed = 01:30  loss = 1.0795e+01  error = 2.6187e-02  \n",
      "nt_epoch =    920  elapsed = 01:31  loss = 1.0715e+01  error = 2.5793e-02  \n",
      "nt_epoch =    930  elapsed = 01:32  loss = 1.0414e+01  error = 2.6180e-02  \n",
      "nt_epoch =    940  elapsed = 01:33  loss = 1.0246e+01  error = 2.6165e-02  \n",
      "nt_epoch =    950  elapsed = 01:34  loss = 1.0131e+01  error = 2.6517e-02  \n",
      "nt_epoch =    960  elapsed = 01:35  loss = 9.9869e+00  error = 2.6006e-02  \n",
      "nt_epoch =    970  elapsed = 01:36  loss = 9.8668e+00  error = 2.6502e-02  \n",
      "nt_epoch =    980  elapsed = 01:36  loss = 9.7563e+00  error = 2.6730e-02  \n",
      "nt_epoch =    990  elapsed = 01:37  loss = 9.5708e+00  error = 2.6991e-02  \n",
      "==================\n",
      "Training finished (epoch 200): duration = 01:38  error = 2.7346e-02  \n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "idx_t_0 = 10\n",
    "idx_t_1 = 90\n",
    "nu = 0.01/np.pi\n",
    "\n",
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, dt, \\\n",
    "  Exact_u, x_0, u_0, x_1, x_star, u_star, \\\n",
    "  IRK_weights, IRK_times = prep_data(path, N_n=N_n, q=q, lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, x_1, lb, ub, nu, q, IRK_weights, IRK_times)\n",
    "def error():\n",
    "    u_pred = pinn.predict(x_star)\n",
    "    return np.linalg.norm(u_pred - u_star, 2) / np.linalg.norm(u_star, 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(x_0, u_0, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_1_pred = pinn.predict(x_star)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_1_pred = pinn.predict(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c535747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    }
   ],
   "source": [
    "plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, file=\"disc_example1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a68d3-fdcf-4420-a7bb-6fa639dc05bd",
   "metadata": {},
   "source": [
    "# 3. Continuous Identification\n",
    "\n",
    "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
    "\n",
    "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator.\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f44caa71-49d3-4ece-b079-05b74bd37a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 2000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.001)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 1000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee80a005-852e-4bb3-ae81-6a45b0bba054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, ub, lb):\n",
    "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
    "    self.u_model = tf.keras.Sequential()\n",
    "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.u_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]:\n",
    "        self.u_model.add(tf.keras.layers.Dense(\n",
    "          width, activation=tf.nn.tanh,\n",
    "          kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    # Defining the two additional trainable variables for identification\n",
    "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
    "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
    "    \n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "  # The actual PINN\n",
    "  def __f_model(self, X_u):\n",
    "    l1, l2 = self.get_params()\n",
    "    # Separating the collocation coordinates\n",
    "    x_f = tf.convert_to_tensor(X_u[:, 0:1], dtype=self.dtype)\n",
    "    t_f = tf.convert_to_tensor(X_u[:, 1:2], dtype=self.dtype)\n",
    "\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(x_f)\n",
    "      tape.watch(t_f)\n",
    "      # Packing together the inputs\n",
    "      X_f = tf.stack([x_f[:,0], t_f[:,0]], axis=1)\n",
    "\n",
    "\n",
    "      # Getting the prediction\n",
    "      u = self.u_model(X_f)\n",
    "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "      u_x = tape.gradient(u, x_f)\n",
    "    \n",
    "    # Getting the other derivatives\n",
    "    u_xx = tape.gradient(u_x, x_f)\n",
    "    u_t = tape.gradient(u, t_f)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    return u_t + l1*u*u_x - l2*u_xx\n",
    "\n",
    "  # Defining custom loss\n",
    "  def __loss(self, X_u, u, u_pred):\n",
    "    f_pred = self.__f_model(X_u)\n",
    "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "      tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "  def __grad(self, X, u):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(X, u, self.u_model(X))\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.u_model.trainable_variables\n",
    "    var.extend([self.lambda_1, self.lambda_2])\n",
    "    return var\n",
    "\n",
    "  def get_weights(self):\n",
    "      w = []\n",
    "      for layer in self.u_model.layers[1:]:\n",
    "        weights_biases = layer.get_weights()\n",
    "        weights = weights_biases[0].flatten()\n",
    "        biases = weights_biases[1]\n",
    "        w.extend(weights)\n",
    "        w.extend(biases)\n",
    "      w.extend(self.lambda_1.numpy())\n",
    "      w.extend(self.lambda_2.numpy())\n",
    "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "    self.lambda_1.assign([w[-2]])\n",
    "    self.lambda_2.assign([w[-1]])\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    if numpy:\n",
    "      return l1.numpy()[0], l2.numpy()[0]\n",
    "    return l1, l2\n",
    "\n",
    "  def summary(self):\n",
    "    return self.u_model.summary()\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, X_u, u, tf_epochs, nt_config):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "    def log_train_epoch(epoch, loss, is_iter):\n",
    "      l1, l2 = self.get_params(numpy=True)\n",
    "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
    "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(X_u, u)\n",
    "      self.optimizer.apply_gradients(\n",
    "        zip(grads, self.__wrap_training_variables()))\n",
    "      log_train_epoch(epoch, loss_value, False)\n",
    "\n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        tape.watch(self.lambda_1)\n",
    "        tape.watch(self.lambda_2)\n",
    "        loss_value = self.__loss(X_u, u, self.u_model(X_u))\n",
    "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True, log_train_epoch)\n",
    "    \n",
    "    l1, l2 = self.get_params(numpy=True)\n",
    "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
    "\n",
    "  def predict(self, X_star):\n",
    "    u_star = self.u_model(X_star)\n",
    "    f_star = self.__f_model(X_star)\n",
    "    return u_star.numpy(), f_star.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4959498b-26cf-4b4d-95c3-0c0a743c0097",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n",
      "Eager execution: True\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_2 (Lambda)           (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 20)                60        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,021\n",
      "Trainable params: 3,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 06:26:47.842373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:26:47.842689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:26:47.842917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:26:47.843430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:26:47.843447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-22 06:26:47.843708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:26:47.843738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_epoch =      0  elapsed = 00:00  loss = 5.9407e-01  error = 6.1009e-01  l1 = 0.000966  l2 = 0.002479\n",
      "tf_epoch =     10  elapsed = 00:00  loss = 3.0893e-01  error = 6.0845e-01  l1 = 0.003536  l2 = 0.002481\n",
      "tf_epoch =     20  elapsed = 00:01  loss = 2.5277e-01  error = 6.1006e-01  l1 = -0.005575  l2 = 0.002500\n",
      "tf_epoch =     30  elapsed = 00:02  loss = 2.3607e-01  error = 6.1136e-01  l1 = -0.017494  l2 = 0.002530\n",
      "tf_epoch =     40  elapsed = 00:02  loss = 2.1998e-01  error = 6.1248e-01  l1 = -0.030334  l2 = 0.002564\n",
      "tf_epoch =     50  elapsed = 00:03  loss = 2.0089e-01  error = 6.1307e-01  l1 = -0.041686  l2 = 0.002596\n",
      "tf_epoch =     60  elapsed = 00:04  loss = 1.7534e-01  error = 6.1300e-01  l1 = -0.049537  l2 = 0.002621\n",
      "tf_epoch =     70  elapsed = 00:05  loss = 1.4044e-01  error = 6.1422e-01  l1 = -0.051598  l2 = 0.002620\n",
      "tf_epoch =     80  elapsed = 00:05  loss = 1.0898e-01  error = 6.1704e-01  l1 = -0.044117  l2 = 0.002578\n",
      "tf_epoch =     90  elapsed = 00:06  loss = 8.8361e-02  error = 6.1689e-01  l1 = -0.027384  l2 = 0.002526\n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:07  loss = 3.9972e-02  error = 5.4457e-01  l1 = 0.139241  l2 = 0.002456\n",
      "nt_epoch =     20  elapsed = 00:08  loss = 3.2393e-02  error = 5.5558e-01  l1 = 0.121072  l2 = 0.002444\n",
      "nt_epoch =     30  elapsed = 00:09  loss = 2.8311e-02  error = 5.6338e-01  l1 = 0.114229  l2 = 0.002416\n",
      "nt_epoch =     40  elapsed = 00:10  loss = 2.5504e-02  error = 5.5275e-01  l1 = 0.149576  l2 = 0.002371\n",
      "nt_epoch =     50  elapsed = 00:12  loss = 2.3219e-02  error = 5.2819e-01  l1 = 0.193352  l2 = 0.002388\n",
      "nt_epoch =     60  elapsed = 00:13  loss = 2.1834e-02  error = 5.2462e-01  l1 = 0.197022  l2 = 0.002399\n",
      "nt_epoch =     70  elapsed = 00:14  loss = 2.0814e-02  error = 4.8569e-01  l1 = 0.242050  l2 = 0.002504\n",
      "nt_epoch =     80  elapsed = 00:15  loss = 1.7978e-02  error = 4.2517e-01  l1 = 0.301130  l2 = 0.002701\n",
      "nt_epoch =     90  elapsed = 00:17  loss = 1.6561e-02  error = 3.8670e-01  l1 = 0.340857  l2 = 0.002819\n",
      "nt_epoch =    100  elapsed = 00:18  loss = 1.5141e-02  error = 3.4949e-01  l1 = 0.402022  l2 = 0.002862\n",
      "nt_epoch =    110  elapsed = 00:19  loss = 1.3863e-02  error = 3.4254e-01  l1 = 0.415461  l2 = 0.002863\n",
      "nt_epoch =    120  elapsed = 00:20  loss = 1.3136e-02  error = 3.2271e-01  l1 = 0.444719  l2 = 0.002896\n",
      "nt_epoch =    130  elapsed = 00:21  loss = 1.2627e-02  error = 3.0341e-01  l1 = 0.488032  l2 = 0.002881\n",
      "nt_epoch =    140  elapsed = 00:22  loss = 1.2210e-02  error = 2.9007e-01  l1 = 0.501620  l2 = 0.002923\n",
      "nt_epoch =    150  elapsed = 00:23  loss = 1.1813e-02  error = 2.7389e-01  l1 = 0.525415  l2 = 0.002950\n",
      "nt_epoch =    160  elapsed = 00:25  loss = 1.1550e-02  error = 2.7150e-01  l1 = 0.529563  l2 = 0.002952\n",
      "nt_epoch =    170  elapsed = 00:26  loss = 1.1363e-02  error = 2.6614e-01  l1 = 0.542746  l2 = 0.002944\n",
      "nt_epoch =    180  elapsed = 00:27  loss = 1.0929e-02  error = 2.6219e-01  l1 = 0.558162  l2 = 0.002920\n",
      "nt_epoch =    190  elapsed = 00:28  loss = 1.0347e-02  error = 2.3967e-01  l1 = 0.592013  l2 = 0.002956\n",
      "nt_epoch =    200  elapsed = 00:29  loss = 9.9770e-03  error = 2.4423e-01  l1 = 0.589898  l2 = 0.002934\n",
      "nt_epoch =    210  elapsed = 00:30  loss = 9.4967e-03  error = 2.3791e-01  l1 = 0.607394  l2 = 0.002918\n",
      "nt_epoch =    220  elapsed = 00:31  loss = 8.5838e-03  error = 2.4034e-01  l1 = 0.615135  l2 = 0.002878\n",
      "nt_epoch =    230  elapsed = 00:32  loss = 8.4140e-03  error = 2.3427e-01  l1 = 0.627770  l2 = 0.002877\n",
      "nt_epoch =    240  elapsed = 00:33  loss = 8.3391e-03  error = 2.3024e-01  l1 = 0.638313  l2 = 0.002869\n",
      "nt_epoch =    250  elapsed = 00:35  loss = 8.1389e-03  error = 2.2649e-01  l1 = 0.651195  l2 = 0.002851\n",
      "nt_epoch =    260  elapsed = 00:36  loss = 8.0972e-03  error = 2.2414e-01  l1 = 0.653923  l2 = 0.002858\n",
      "nt_epoch =    270  elapsed = 00:37  loss = 8.0635e-03  error = 2.2538e-01  l1 = 0.654440  l2 = 0.002848\n",
      "nt_epoch =    280  elapsed = 00:38  loss = 7.9031e-03  error = 2.2731e-01  l1 = 0.649081  l2 = 0.002853\n",
      "nt_epoch =    290  elapsed = 00:39  loss = 7.6863e-03  error = 2.2068e-01  l1 = 0.658195  l2 = 0.002866\n",
      "nt_epoch =    300  elapsed = 00:40  loss = 7.5418e-03  error = 2.2078e-01  l1 = 0.658765  l2 = 0.002864\n",
      "nt_epoch =    310  elapsed = 00:41  loss = 7.4014e-03  error = 2.2124e-01  l1 = 0.660190  l2 = 0.002856\n",
      "nt_epoch =    320  elapsed = 00:42  loss = 7.3307e-03  error = 2.2125e-01  l1 = 0.661267  l2 = 0.002853\n",
      "nt_epoch =    330  elapsed = 00:43  loss = 7.2460e-03  error = 2.1957e-01  l1 = 0.663963  l2 = 0.002855\n",
      "==================\n",
      "Training finished (epoch 100): duration = 00:44  error = 3.2460e+31  l1 = -787.711731  l2 = 206648414996600171207310966784.000000\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_3 (Lambda)           (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 20)                60        \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,021\n",
      "Trainable params: 3,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "tf_epoch =      0  elapsed = 00:44  loss = 4.8235e-01  error = 6.1082e-01  l1 = -0.000975  l2 = 0.002481\n",
      "tf_epoch =     10  elapsed = 00:45  loss = 2.4184e-01  error = 6.0619e-01  l1 = 0.005116  l2 = 0.002491\n",
      "tf_epoch =     20  elapsed = 00:45  loss = 1.7472e-01  error = 6.0491e-01  l1 = -0.006580  l2 = 0.002536\n",
      "tf_epoch =     30  elapsed = 00:46  loss = 1.3522e-01  error = 6.0484e-01  l1 = -0.014911  l2 = 0.002563\n",
      "tf_epoch =     40  elapsed = 00:46  loss = 9.7331e-02  error = 6.0488e-01  l1 = 0.000097  l2 = 0.002515\n",
      "tf_epoch =     50  elapsed = 00:47  loss = 7.4161e-02  error = 6.0219e-01  l1 = 0.025644  l2 = 0.002451\n",
      "tf_epoch =     60  elapsed = 00:47  loss = 5.9215e-02  error = 5.9899e-01  l1 = 0.052684  l2 = 0.002385\n",
      "tf_epoch =     70  elapsed = 00:48  loss = 4.9540e-02  error = 5.9553e-01  l1 = 0.078013  l2 = 0.002327\n",
      "tf_epoch =     80  elapsed = 00:49  loss = 4.3279e-02  error = 5.9181e-01  l1 = 0.097699  l2 = 0.002288\n",
      "tf_epoch =     90  elapsed = 00:49  loss = 3.9403e-02  error = 5.8800e-01  l1 = 0.108297  l2 = 0.002278\n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:51  loss = 3.3816e-02  error = 5.8884e-01  l1 = 0.096516  l2 = 0.002310\n",
      "nt_epoch =     20  elapsed = 00:51  loss = 3.0526e-02  error = 5.7128e-01  l1 = 0.129288  l2 = 0.002318\n",
      "nt_epoch =     30  elapsed = 00:52  loss = 2.8095e-02  error = 5.5467e-01  l1 = 0.151891  l2 = 0.002352\n",
      "nt_epoch =     40  elapsed = 00:53  loss = 2.4766e-02  error = 5.1008e-01  l1 = 0.204852  l2 = 0.002467\n",
      "nt_epoch =     50  elapsed = 00:54  loss = 2.2170e-02  error = 4.5192e-01  l1 = 0.259685  l2 = 0.002663\n",
      "nt_epoch =     60  elapsed = 00:55  loss = 2.0101e-02  error = 4.1182e-01  l1 = 0.302631  l2 = 0.002781\n",
      "nt_epoch =     70  elapsed = 00:56  loss = 1.8253e-02  error = 3.5176e-01  l1 = 0.376866  l2 = 0.002927\n",
      "nt_epoch =     80  elapsed = 00:58  loss = 1.7193e-02  error = 3.5184e-01  l1 = 0.381056  l2 = 0.002913\n",
      "nt_epoch =     90  elapsed = 00:59  loss = 1.6276e-02  error = 3.3417e-01  l1 = 0.395257  l2 = 0.002981\n",
      "nt_epoch =    100  elapsed = 01:00  loss = 1.5745e-02  error = 3.1423e-01  l1 = 0.406069  l2 = 0.003073\n",
      "nt_epoch =    110  elapsed = 01:01  loss = 1.5036e-02  error = 2.9134e-01  l1 = 0.443957  l2 = 0.003268\n",
      "nt_epoch =    120  elapsed = 01:02  loss = 1.4323e-02  error = 3.0619e-01  l1 = 0.468637  l2 = 0.003441\n",
      "nt_epoch =    130  elapsed = 01:03  loss = 1.3424e-02  error = 3.2444e-01  l1 = 0.496186  l2 = 0.003645\n",
      "nt_epoch =    140  elapsed = 01:04  loss = 1.2757e-02  error = 3.4515e-01  l1 = 0.470400  l2 = 0.003695\n",
      "nt_epoch =    150  elapsed = 01:05  loss = 1.1367e-02  error = 3.9779e-01  l1 = 0.522502  l2 = 0.004196\n",
      "nt_epoch =    160  elapsed = 01:06  loss = 1.0328e-02  error = 4.0102e-01  l1 = 0.577521  l2 = 0.004391\n",
      "nt_epoch =    170  elapsed = 01:07  loss = 9.4669e-03  error = 4.0686e-01  l1 = 0.590839  l2 = 0.004471\n",
      "nt_epoch =    180  elapsed = 01:08  loss = 8.8402e-03  error = 4.1444e-01  l1 = 0.639394  l2 = 0.004674\n",
      "nt_epoch =    190  elapsed = 01:10  loss = 8.1312e-03  error = 4.0640e-01  l1 = 0.663160  l2 = 0.004698\n",
      "nt_epoch =    200  elapsed = 01:11  loss = 7.9093e-03  error = 4.2935e-01  l1 = 0.654688  l2 = 0.004817\n",
      "nt_epoch =    210  elapsed = 01:12  loss = 7.1324e-03  error = 4.6341e-01  l1 = 0.687563  l2 = 0.005139\n",
      "nt_epoch =    220  elapsed = 01:13  loss = 7.0391e-03  error = 4.6621e-01  l1 = 0.689280  l2 = 0.005162\n",
      "nt_epoch =    230  elapsed = 01:14  loss = 6.6838e-03  error = 4.9236e-01  l1 = 0.724203  l2 = 0.005440\n",
      "nt_epoch =    240  elapsed = 01:15  loss = 6.4778e-03  error = 4.9995e-01  l1 = 0.723692  l2 = 0.005486\n",
      "nt_epoch =    250  elapsed = 01:16  loss = 6.1351e-03  error = 5.1551e-01  l1 = 0.739883  l2 = 0.005637\n",
      "nt_epoch =    260  elapsed = 01:17  loss = 5.8792e-03  error = 5.3258e-01  l1 = 0.755157  l2 = 0.005794\n",
      "nt_epoch =    270  elapsed = 01:18  loss = 5.6280e-03  error = 5.4006e-01  l1 = 0.768333  l2 = 0.005884\n",
      "nt_epoch =    280  elapsed = 01:19  loss = 5.4381e-03  error = 5.4241e-01  l1 = 0.764923  l2 = 0.005888\n",
      "nt_epoch =    290  elapsed = 01:20  loss = 5.1792e-03  error = 5.4030e-01  l1 = 0.780881  l2 = 0.005925\n",
      "nt_epoch =    300  elapsed = 01:22  loss = 5.0353e-03  error = 5.3713e-01  l1 = 0.789596  l2 = 0.005933\n",
      "nt_epoch =    310  elapsed = 01:23  loss = 4.5913e-03  error = 5.3530e-01  l1 = 0.831834  l2 = 0.006056\n",
      "nt_epoch =    320  elapsed = 01:24  loss = 4.1333e-03  error = 5.3442e-01  l1 = 0.838271  l2 = 0.006071\n",
      "nt_epoch =    330  elapsed = 01:25  loss = 3.9144e-03  error = 5.3340e-01  l1 = 0.845274  l2 = 0.006086\n",
      "nt_epoch =    340  elapsed = 01:26  loss = 3.8228e-03  error = 5.3460e-01  l1 = 0.844237  l2 = 0.006091\n",
      "nt_epoch =    350  elapsed = 01:27  loss = 3.6410e-03  error = 5.2812e-01  l1 = 0.868303  l2 = 0.006126\n",
      "nt_epoch =    360  elapsed = 01:29  loss = 3.4827e-03  error = 5.2795e-01  l1 = 0.873743  l2 = 0.006142\n",
      "nt_epoch =    370  elapsed = 01:30  loss = 3.3449e-03  error = 5.1645e-01  l1 = 0.881081  l2 = 0.006092\n",
      "nt_epoch =    380  elapsed = 01:31  loss = 3.2996e-03  error = 5.1353e-01  l1 = 0.882154  l2 = 0.006077\n",
      "nt_epoch =    390  elapsed = 01:32  loss = 3.1784e-03  error = 5.0767e-01  l1 = 0.895392  l2 = 0.006082\n",
      "nt_epoch =    400  elapsed = 01:33  loss = 3.0770e-03  error = 5.0286e-01  l1 = 0.903873  l2 = 0.006078\n",
      "nt_epoch =    410  elapsed = 01:34  loss = 2.9717e-03  error = 5.0382e-01  l1 = 0.911230  l2 = 0.006108\n",
      "nt_epoch =    420  elapsed = 01:35  loss = 2.8867e-03  error = 5.0459e-01  l1 = 0.905351  l2 = 0.006094\n",
      "nt_epoch =    430  elapsed = 01:36  loss = 2.7775e-03  error = 5.0568e-01  l1 = 0.898975  l2 = 0.006081\n",
      "nt_epoch =    440  elapsed = 01:37  loss = 2.6578e-03  error = 4.9498e-01  l1 = 0.898992  l2 = 0.006013\n",
      "nt_epoch =    450  elapsed = 01:38  loss = 2.5819e-03  error = 4.9212e-01  l1 = 0.907812  l2 = 0.006023\n",
      "nt_epoch =    460  elapsed = 01:39  loss = 2.5025e-03  error = 4.9559e-01  l1 = 0.906983  l2 = 0.006042\n",
      "nt_epoch =    470  elapsed = 01:41  loss = 2.4373e-03  error = 4.9068e-01  l1 = 0.918433  l2 = 0.006047\n",
      "nt_epoch =    480  elapsed = 01:42  loss = 2.4202e-03  error = 4.8946e-01  l1 = 0.919624  l2 = 0.006043\n",
      "nt_epoch =    490  elapsed = 01:43  loss = 2.3770e-03  error = 4.8853e-01  l1 = 0.917225  l2 = 0.006030\n",
      "nt_epoch =    500  elapsed = 01:44  loss = 2.3121e-03  error = 4.8574e-01  l1 = 0.917716  l2 = 0.006013\n",
      "nt_epoch =    510  elapsed = 01:45  loss = 2.2795e-03  error = 4.8639e-01  l1 = 0.914580  l2 = 0.006008\n",
      "nt_epoch =    520  elapsed = 01:46  loss = 2.2621e-03  error = 4.8699e-01  l1 = 0.914583  l2 = 0.006011\n",
      "nt_epoch =    530  elapsed = 01:47  loss = 2.2624e-03  error = 4.8658e-01  l1 = 0.915848  l2 = 0.006013\n",
      "nt_epoch =    540  elapsed = 01:48  loss = 2.2069e-03  error = 4.8700e-01  l1 = 0.924210  l2 = 0.006042\n",
      "nt_epoch =    550  elapsed = 01:49  loss = 2.1377e-03  error = 4.8532e-01  l1 = 0.932387  l2 = 0.006058\n",
      "nt_epoch =    560  elapsed = 01:50  loss = 2.0978e-03  error = 4.8539e-01  l1 = 0.929399  l2 = 0.006048\n",
      "nt_epoch =    570  elapsed = 01:51  loss = 2.0426e-03  error = 4.8274e-01  l1 = 0.924240  l2 = 0.006015\n",
      "nt_epoch =    580  elapsed = 01:52  loss = 2.0078e-03  error = 4.8208e-01  l1 = 0.920365  l2 = 0.005999\n",
      "nt_epoch =    590  elapsed = 01:53  loss = 2.0014e-03  error = 4.8235e-01  l1 = 0.918047  l2 = 0.005993\n",
      "nt_epoch =    600  elapsed = 01:54  loss = 1.9726e-03  error = 4.8116e-01  l1 = 0.916911  l2 = 0.005982\n",
      "nt_epoch =    610  elapsed = 01:55  loss = 1.9398e-03  error = 4.8080e-01  l1 = 0.919053  l2 = 0.005986\n",
      "nt_epoch =    620  elapsed = 01:56  loss = 1.9241e-03  error = 4.7986e-01  l1 = 0.918682  l2 = 0.005979\n",
      "nt_epoch =    630  elapsed = 01:57  loss = 1.9004e-03  error = 4.7951e-01  l1 = 0.917527  l2 = 0.005973\n",
      "nt_epoch =    640  elapsed = 01:58  loss = 1.8806e-03  error = 4.7731e-01  l1 = 0.920019  l2 = 0.005967\n",
      "nt_epoch =    650  elapsed = 01:59  loss = 1.8757e-03  error = 4.7685e-01  l1 = 0.921314  l2 = 0.005968\n",
      "nt_epoch =    660  elapsed = 02:00  loss = 1.8412e-03  error = 4.7379e-01  l1 = 0.924078  l2 = 0.005958\n",
      "nt_epoch =    670  elapsed = 02:01  loss = 1.7972e-03  error = 4.6748e-01  l1 = 0.926112  l2 = 0.005924\n",
      "nt_epoch =    680  elapsed = 02:03  loss = 1.7751e-03  error = 4.6645e-01  l1 = 0.926563  l2 = 0.005919\n",
      "nt_epoch =    690  elapsed = 02:04  loss = 1.7450e-03  error = 4.6341e-01  l1 = 0.925559  l2 = 0.005896\n",
      "nt_epoch =    700  elapsed = 02:05  loss = 1.6971e-03  error = 4.6040e-01  l1 = 0.925420  l2 = 0.005877\n",
      "nt_epoch =    710  elapsed = 02:06  loss = 1.6592e-03  error = 4.5739e-01  l1 = 0.925564  l2 = 0.005858\n",
      "nt_epoch =    720  elapsed = 02:07  loss = 1.6538e-03  error = 4.5567e-01  l1 = 0.926495  l2 = 0.005850\n",
      "nt_epoch =    730  elapsed = 02:08  loss = 1.6406e-03  error = 4.5423e-01  l1 = 0.927640  l2 = 0.005844\n",
      "nt_epoch =    740  elapsed = 02:09  loss = 1.6254e-03  error = 4.5227e-01  l1 = 0.931382  l2 = 0.005844\n",
      "nt_epoch =    750  elapsed = 02:10  loss = 1.5562e-03  error = 4.4591e-01  l1 = 0.937975  l2 = 0.005824\n",
      "nt_epoch =    760  elapsed = 02:11  loss = 1.5401e-03  error = 4.4434e-01  l1 = 0.938018  l2 = 0.005815\n",
      "nt_epoch =    770  elapsed = 02:12  loss = 1.5054e-03  error = 4.4196e-01  l1 = 0.940267  l2 = 0.005807\n",
      "nt_epoch =    780  elapsed = 02:13  loss = 1.4821e-03  error = 4.3839e-01  l1 = 0.941025  l2 = 0.005786\n",
      "nt_epoch =    790  elapsed = 02:14  loss = 1.4491e-03  error = 4.3343e-01  l1 = 0.946244  l2 = 0.005771\n",
      "nt_epoch =    800  elapsed = 02:16  loss = 1.4029e-03  error = 4.2721e-01  l1 = 0.954772  l2 = 0.005759\n",
      "nt_epoch =    810  elapsed = 02:17  loss = 1.3684e-03  error = 4.2262e-01  l1 = 0.958267  l2 = 0.005741\n",
      "nt_epoch =    820  elapsed = 02:18  loss = 1.3553e-03  error = 4.2267e-01  l1 = 0.958549  l2 = 0.005742\n",
      "nt_epoch =    830  elapsed = 02:19  loss = 1.3387e-03  error = 4.2139e-01  l1 = 0.959668  l2 = 0.005737\n",
      "nt_epoch =    840  elapsed = 02:20  loss = 1.2963e-03  error = 4.1538e-01  l1 = 0.967095  l2 = 0.005723\n",
      "nt_epoch =    850  elapsed = 02:21  loss = 1.2858e-03  error = 4.1345e-01  l1 = 0.968398  l2 = 0.005715\n",
      "nt_epoch =    860  elapsed = 02:22  loss = 1.2818e-03  error = 4.1154e-01  l1 = 0.970562  l2 = 0.005709\n",
      "nt_epoch =    870  elapsed = 02:23  loss = 1.2722e-03  error = 4.0981e-01  l1 = 0.972718  l2 = 0.005705\n",
      "nt_epoch =    880  elapsed = 02:24  loss = 1.2720e-03  error = 4.0679e-01  l1 = 0.973876  l2 = 0.005690\n",
      "nt_epoch =    890  elapsed = 02:25  loss = 1.2521e-03  error = 4.0808e-01  l1 = 0.975746  l2 = 0.005704\n",
      "nt_epoch =    900  elapsed = 02:26  loss = 1.2240e-03  error = 4.0265e-01  l1 = 0.980449  l2 = 0.005684\n",
      "==================\n",
      "Training finished (epoch 100): duration = 02:27  error = 3.2416e+11  l1 = -648327069696.000000  l2 = 0.000000\n",
      "l1:  -787.71173\n",
      "l2:  2.0664841e+29\n",
      "l1_noise:  -648327100000.0\n",
      "l2_noise:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.0)\n",
    "lambdas_star = (1.0, 0.01/np.pi)\n",
    "\n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
    "def error():\n",
    "    l1, l2 = pinn.get_params(numpy=True)\n",
    "    l1_star, l2_star = lambdas_star\n",
    "    error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
    "    error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
    "    return (error_lambda_1 + error_lambda_2) / 2\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "\n",
    "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
    "u_pred, f_pred = pinn.predict(X_star)\n",
    "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
    "\n",
    "# Noise case\n",
    "x, t, X, T, Exact_u, X_star, u_star, \\\n",
    "  X_u_train, u_train, ub, lb = prep_data(path, N_u, noise=0.01)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, ub, lb)\n",
    "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
    "lambda_1_pred_noise, lambda_2_pred_noise = pinn.get_params(numpy=True)\n",
    "\n",
    "print(\"l1: \", lambda_1_pred)\n",
    "print(\"l2: \", lambda_2_pred)\n",
    "print(\"l1_noise: \", lambda_1_pred_noise)\n",
    "print(\"l2_noise: \", lambda_2_pred_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc892cb1-62ef-4bde-9d53-551d9b2896c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error measuring \\rmfamily\\fontsize{10.000000}{12.000000}\\selectfont \\(\\displaystyle \\begin{tabular}{ |c|c| }  \\hline Correct PDE & \\)u_t + u u_x - 0.0031831 u_{xx} = 0\\(\\displaystyle  \\\\  \\hline Identified PDE (clean data) & \\)u_t + -787.71173 u u_x - 206648414996600171207310966784.0000000 u_{xx} = 0\\(\\displaystyle  \\\\  \\hline Identified PDE (1\\% noise) & \\)u_t + -648327069696.00000 u u_x - 0.0000000 u_{xx} = 0\\(\\displaystyle   \\\\  \\hline \\end{tabular}\\)\nLaTeX Output:\n\n! LaTeX Error: Bad math environment delimiter.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H <return>  for immediate help.\n ...                                              \n                                                  \n<*> ...\\displaystyle   \\\\  \\hline \\end{tabular}\\)}\n                                                  \\typeout{\\the\\wd0,\\the\\ht0...\n!  ==> Fatal error occurred, no output PDF file produced!\nTranscript written on texput.log.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLatexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_pgf.py:354\u001b[0m, in \u001b[0;36mLatexManager._get_box_metrics\u001b[0;34m(self, tex)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expect_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LatexError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Here and below, use '{}' instead of {!r} to avoid doubling all\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# backslashes.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_pgf.py:280\u001b[0m, in \u001b[0;36mLatexManager._expect_prompt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_expect_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_pgf.py:276\u001b[0m, in \u001b[0;36mLatexManager._expect\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LatexError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaTeX process halted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chars))\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chars)\n",
      "\u001b[0;31mLatexError\u001b[0m: LaTeX process halted\n\n! LaTeX Error: Bad math environment delimiter.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H <return>  for immediate help.\n ...                                              \n                                                  \n<*> ...\\displaystyle   \\\\  \\hline \\end{tabular}\\)}\n                                                  \\typeout{\\the\\wd0,\\the\\ht0...\n!  ==> Fatal error occurred, no output PDF file produced!\nTranscript written on texput.log.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_ide_cont_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_u_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mExact_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_1_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_1_pred_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_2_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_2_pred_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcont_example2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/practice/piml/Exercise_Alone/pinn_TF2/utils.py:478\u001b[0m, in \u001b[0;36mplot_ide_cont_results\u001b[0;34m(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy, file)\u001b[0m\n\u001b[1;32m    475\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m     \u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/practice/piml/Exercise_Alone/pinn_TF2/utils.py:58\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(filename, crop)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(filename, crop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m crop \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# plt.savefig('{}.pgf'.format(filename), bbox_inches='tight', pad_inches=0)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m         \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_inches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_inches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m# plt.savefig('{}.eps'.format(filename), bbox_inches='tight', pad_inches=0)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename), bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m, pad_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py:1023\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1022\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m-> 1023\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/figure.py:3343\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3339\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3340\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   3341\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m-> 3343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[1;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[1;32m   2340\u001b[0m     )\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2342\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/figure.py:3140\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3137\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3140\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3144\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/text.py:752\u001b[0m, in \u001b[0;36mText.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    749\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cm_set(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_text()):\n\u001b[0;32m--> 752\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m     trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m# don't use self.get_position here, which refers to text\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# position in Text:\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/text.py:386\u001b[0m, in \u001b[0;36mText._get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    384\u001b[0m clean_line, ismath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_math(line)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_line:\n\u001b[0;32m--> 386\u001b[0m     w, h, d \u001b[38;5;241m=\u001b[39m \u001b[43m_get_text_metrics_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fontproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mismath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mismath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     w \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m=\u001b[39m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/text.py:97\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache\u001b[0;34m(renderer, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call ``renderer.get_text_width_height_descent``, caching the results.\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Cached based on a copy of fontprop so that later in-place mutations of\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# the passed-in argument do not mess up the cache.\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_text_metrics_with_cache_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontprop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mismath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/text.py:105\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache_impl\u001b[0;34m(renderer_ref, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;241m4096\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_metrics_with_cache_impl\u001b[39m(\n\u001b[1;32m    103\u001b[0m         renderer_ref, text, fontprop, ismath, dpi):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# dpi is unused, but participates in cache invalidation (via the renderer).\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrenderer_ref\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_width_height_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mismath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_pgf.py:749\u001b[0m, in \u001b[0;36mRendererPgf.get_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_width_height_descent\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, prop, ismath):\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# get text metrics in units of latex pt, convert to display units\u001b[39;00m\n\u001b[0;32m--> 749\u001b[0m     w, h, d \u001b[38;5;241m=\u001b[39m (\u001b[43mLatexManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cached_or_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_width_height_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# TODO: this should be latex_pt_to_in instead of mpl_pt_to_in\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# but having a little bit more space around the text looks better,\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m# plus the bounding box reported by LaTeX is VERY narrow\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     f \u001b[38;5;241m=\u001b[39m mpl_pt_to_in \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdpi\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_pgf.py:343\u001b[0m, in \u001b[0;36mLatexManager.get_width_height_descent\u001b[0;34m(self, text, prop)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_width_height_descent\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, prop):\n\u001b[1;32m    339\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    Get the width, total height, and descent (in TeX points) for a text\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    typeset by the current LaTeX environment.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_box_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_escape_and_apply_props\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backends/backend_pgf.py:358\u001b[0m, in \u001b[0;36mLatexManager._get_box_metrics\u001b[0;34m(self, tex)\u001b[0m\n\u001b[1;32m    354\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expect_prompt()\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LatexError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Here and below, use '{}' instead of {!r} to avoid doubling all\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# backslashes.\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError measuring \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLaTeX Output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m                      \u001b[38;5;241m.\u001b[39mformat(tex, err\u001b[38;5;241m.\u001b[39mlatex_output)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Parse metrics from the answer string.  Last line is prompt, and\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# next-to-last-line is blank line from \\typeout.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     width, height, offset \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Error measuring \\rmfamily\\fontsize{10.000000}{12.000000}\\selectfont \\(\\displaystyle \\begin{tabular}{ |c|c| }  \\hline Correct PDE & \\)u_t + u u_x - 0.0031831 u_{xx} = 0\\(\\displaystyle  \\\\  \\hline Identified PDE (clean data) & \\)u_t + -787.71173 u u_x - 206648414996600171207310966784.0000000 u_{xx} = 0\\(\\displaystyle  \\\\  \\hline Identified PDE (1\\% noise) & \\)u_t + -648327069696.00000 u u_x - 0.0000000 u_{xx} = 0\\(\\displaystyle   \\\\  \\hline \\end{tabular}\\)\nLaTeX Output:\n\n! LaTeX Error: Bad math environment delimiter.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H <return>  for immediate help.\n ...                                              \n                                                  \n<*> ...\\displaystyle   \\\\  \\hline \\end{tabular}\\)}\n                                                  \\typeout{\\the\\wd0,\\the\\ht0...\n!  ==> Fatal error occurred, no output PDF file produced!\nTranscript written on texput.log.\n"
     ]
    }
   ],
   "source": [
    "plot_ide_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, lambda_1_pred, lambda_1_pred_noise, lambda_2_pred, lambda_2_pred_noise, file=\"cont_example2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d99f1-eea4-4aa4-86d0-b1de63fca1b9",
   "metadata": {},
   "source": [
    "# 4. Discrete Identification\n",
    "\n",
    "$$u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx} = 0$$\n",
    "\n",
    "With $\\lambda_1$ and $\\lambda_2$ real parameters of the differential operator.\n",
    "\n",
    "Approximating $u(t,x)$ with a deep NN, we define the PINN:\n",
    "$$f := u_t + \\lambda_1 u u_x - \\lambda_2 u_{xx}.$$\n",
    "\n",
    "We train the shared parameters between the deep NN and the PINN minimizing the loss:\n",
    "$$MSE =\\frac{1}{N_u}\\sum_{i=1}^{N_u} |u(t^i_u,x_u^i) - u^i|^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_u}|f(t_u^i,x_u^i)|^2,$$\n",
    "with $\\{t_u^i, x_u^i, u^i\\}_{i=1}^{N_u}$ respectively the trainring data on $u(t,x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7896958-3e79-47fc-9f92-b71ffa047e73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Data size on initial condition on u\n",
    "N_0 = 199\n",
    "N_1 = 201\n",
    "# DeepNN topology (1-sized input [x], 3 hidden layer of 50-width, q-sized output defined later [u_1^n(x), ..., u_{q+1}^n(x)]\n",
    "layers = [1, 50, 50, 50, 0]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 100\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  lr=0.001,\n",
    "  beta_1=0.9,\n",
    "  beta_2=0.999,\n",
    "  epsilon=1e-08)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f993a48c-f41c-454a-8a28-e95434ca43f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class PhysicsInformedNN(object):\n",
    "  def __init__(self, layers, optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta):\n",
    "    self.lb = lb\n",
    "    self.ub = ub\n",
    "\n",
    "    self.dt = dt\n",
    "\n",
    "    self.q = max(q,1)\n",
    "    self.IRK_alpha = IRK_alpha\n",
    "    self.IRK_beta = IRK_beta\n",
    "\n",
    "    # Descriptive Keras model [2, 50, …, 50, q+1]\n",
    "    self.U_model = tf.keras.Sequential()\n",
    "    self.U_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "    self.U_model.add(tf.keras.layers.Lambda(\n",
    "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
    "    for width in layers[1:]:\n",
    "        self.U_model.add(tf.keras.layers.Dense(\n",
    "          width, activation=tf.nn.tanh,\n",
    "          kernel_initializer='glorot_normal'))\n",
    "\n",
    "    # Computing the sizes of weights/biases for future decomposition\n",
    "    self.sizes_w = []\n",
    "    self.sizes_b = []\n",
    "    for i, width in enumerate(layers):\n",
    "      if i != 1:\n",
    "        self.sizes_w.append(int(width * layers[1]))\n",
    "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "    self.dtype = tf.float32\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.logger = logger\n",
    "\n",
    "  def __autograd(self, U, x, dummy):\n",
    "    # Using the new GradientTape paradigm of TF2.0,\n",
    "    # which keeps track of operations to get the gradient at runtime\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "      # Watching the two inputs we’ll need later, x and t\n",
    "      tape.watch(x)\n",
    "      tape.watch(dummy)\n",
    "\n",
    "      # Getting the prediction\n",
    "      U = self.U_model(x) # shape=(len(x), q)\n",
    "\n",
    "      # Deriving INSIDE the tape (2-step-dummy grad technique because U is a mat)\n",
    "      g_U = tape.gradient(U, x, output_gradients=dummy)\n",
    "      U_x = tape.gradient(g_U, dummy)\n",
    "      g_U_x = tape.gradient(U_x, x, output_gradients=dummy)\n",
    "    \n",
    "    # Doing the last one outside the with, to optimize performance\n",
    "    # Impossible to do for the earlier grad, because they’re needed after\n",
    "    U_xx = tape.gradient(g_U_x, dummy)\n",
    "\n",
    "    # Letting the tape go\n",
    "    del tape\n",
    "    return U_x, U_xx\n",
    "\n",
    "  def U_0_model(self, x, customDummy=None):\n",
    "    U = self.U_model(x)\n",
    "    if customDummy != None:\n",
    "      dummy = customDummy\n",
    "    else:\n",
    "      dummy = self.dummy_x_0\n",
    "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
    "\n",
    "    # Buidling the PINNs\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    N = l1*U*U_x - l2*U_xx # shape=(len(x), q)\n",
    "    return U + self.dt*tf.matmul(N, self.IRK_alpha.T)\n",
    "\n",
    "  def U_1_model(self, x, customDummy=None):\n",
    "    U = self.U_model(x)\n",
    "    #dummy = customDummy or self.dummy_x_1\n",
    "    if customDummy != None:\n",
    "      dummy = customDummy\n",
    "    else:\n",
    "      dummy = self.dummy_x_1\n",
    "    U_x, U_xx = self.__autograd(U, x, dummy)\n",
    "\n",
    "    # Buidling the PINNs, shape = (len(x), q+1), IRK shape = (q, q+1)\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    N = -l1*U*U_x + l2*U_xx # shape=(len(x), q)\n",
    "    return U + self.dt*tf.matmul(N, (self.IRK_beta - self.IRK_alpha).T)\n",
    "\n",
    "  # Defining custom loss\n",
    "  def __loss(self, x_0, u_0, x_1, u_1):\n",
    "    u_0_pred = self.U_0_model(x_0)\n",
    "    u_1_pred = self.U_1_model(x_1)\n",
    "    return tf.reduce_sum(tf.square(u_0_pred - u_0)) + \\\n",
    "      tf.reduce_sum(tf.square(u_1_pred - u_1))\n",
    "\n",
    "  def __grad(self, x_0, u_0, x_1, u_1):\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
    "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "  def __wrap_training_variables(self):\n",
    "    var = self.U_model.trainable_variables\n",
    "    var.extend([self.lambda_1, self.lambda_2])\n",
    "    return var\n",
    "\n",
    "  def get_weights(self):\n",
    "      w = []\n",
    "      for layer in self.U_model.layers[1:]:\n",
    "        weights_biases = layer.get_weights()\n",
    "        weights = weights_biases[0].flatten()\n",
    "        biases = weights_biases[1]\n",
    "        w.extend(weights)\n",
    "        w.extend(biases)\n",
    "      w.extend(self.lambda_1.numpy())\n",
    "      w.extend(self.lambda_2.numpy())\n",
    "      return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "  def set_weights(self, w):\n",
    "    for i, layer in enumerate(self.U_model.layers[1:]):\n",
    "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "      weights = w[start_weights:end_weights]\n",
    "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "      weights_biases = [weights, biases]\n",
    "      layer.set_weights(weights_biases)\n",
    "    self.lambda_1.assign([w[-2]])\n",
    "    self.lambda_2.assign([w[-1]])\n",
    "\n",
    "  def get_params(self, numpy=False):\n",
    "    l1 = self.lambda_1\n",
    "    l2 = tf.exp(self.lambda_2)\n",
    "    if numpy:\n",
    "      return l1.numpy()[0], l2.numpy()[0]\n",
    "    return l1, l2\n",
    "\n",
    "  def summary(self):\n",
    "    return self.U_model.summary()\n",
    "\n",
    "  def __createDummy(self, x):\n",
    "    return tf.ones([x.shape[0], self.q], dtype=self.dtype)\n",
    "\n",
    "  # The training function\n",
    "  def fit(self, x_0, u_0, x_1, u_1, tf_epochs=1):\n",
    "    self.logger.log_train_start(self)\n",
    "\n",
    "    # Creating the tensors\n",
    "    x_0 = tf.convert_to_tensor(x_0, dtype=self.dtype)\n",
    "    u_0 = tf.convert_to_tensor(u_0, dtype=self.dtype)\n",
    "    x_1 = tf.convert_to_tensor(x_1, dtype=self.dtype)\n",
    "    u_1 = tf.convert_to_tensor(u_1, dtype=self.dtype)\n",
    "\n",
    "    self.lambda_1 = tf.Variable([0.0], dtype=self.dtype)\n",
    "    self.lambda_2 = tf.Variable([-6.0], dtype=self.dtype)\n",
    "\n",
    "    # Creating dummy tensors for the gradients\n",
    "    self.dummy_x_0 = self.__createDummy(x_0)\n",
    "    self.dummy_x_1 = self.__createDummy(x_1)\n",
    "\n",
    "    def log_train_epoch(epoch, loss, is_iter):\n",
    "      l1, l2 = self.get_params(numpy=True)\n",
    "      custom = f\"l1 = {l1:5f}  l2 = {l2:8f}\"\n",
    "      self.logger.log_train_epoch(epoch, loss, custom, is_iter)\n",
    "\n",
    "    self.logger.log_train_opt(\"Adam\")\n",
    "    for epoch in range(tf_epochs):\n",
    "      # Optimization step\n",
    "      loss_value, grads = self.__grad(x_0, u_0, x_1, u_1)\n",
    "      self.optimizer.apply_gradients(\n",
    "        zip(grads, self.__wrap_training_variables()))\n",
    "      log_train_epoch(epoch, loss_value, False)\n",
    "    \n",
    "    self.logger.log_train_opt(\"LBFGS\")\n",
    "    def loss_and_flat_grad(w):\n",
    "      with tf.GradientTape() as tape:\n",
    "        self.set_weights(w)\n",
    "        tape.watch(self.lambda_1)\n",
    "        tape.watch(self.lambda_2)\n",
    "        loss_value = self.__loss(x_0, u_0, x_1, u_1)\n",
    "      grad = tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "      grad_flat = []\n",
    "      for g in grad:\n",
    "        grad_flat.append(tf.reshape(g, [-1]))\n",
    "      grad_flat =  tf.concat(grad_flat, 0)\n",
    "      return loss_value, grad_flat\n",
    "    # tfp.optimizer.lbfgs_minimize(\n",
    "    #   loss_and_flat_grad,\n",
    "    #   initial_position=self.get_weights(),\n",
    "    #   num_correction_pairs=nt_config.nCorrection,\n",
    "    #   max_iterations=nt_config.maxIter,\n",
    "    #   f_relative_tolerance=nt_config.tolFun,\n",
    "    #   tolerance=nt_config.tolFun,\n",
    "    #   parallel_iterations=6)\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      self.get_weights(),\n",
    "      nt_config, Struct(), True, log_train_epoch)\n",
    "    \n",
    "    l1, l2 = self.get_params(numpy=True)\n",
    "    self.logger.log_train_end(tf_epochs, f\"l1 = {l1:5f}  l2 = {l2:8f}\")\n",
    "\n",
    "  def predict(self, x_star):\n",
    "    x_star = tf.convert_to_tensor(x_star, dtype=self.dtype)\n",
    "    dummy = self.__createDummy(x_star)\n",
    "    U_0_star = self.U_0_model(x_star, dummy)\n",
    "    U_1_star = self.U_1_model(x_star, dummy)\n",
    "    return U_0_star, U_1_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f465750b-c809-4c94-adeb-8abe875f4033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n",
      "Eager execution: True\n",
      "WARNING:tensorflow:From /practice/piml/Exercise_Alone/pinn_TF2/utils.py:173: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda (Lambda)             (None, 1)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                100       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 81)                4131      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,331\n",
      "Trainable params: 9,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 06:59:30.520025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 06:59:30.544831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:30.552483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:30.552732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.022226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.022527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.022541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-22 06:59:31.022763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.022805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /device:GPU:0 with 3433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-11-22 06:59:31.072985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.073356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.073616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.074439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.074832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.075153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.075582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.075595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-11-22 06:59:31.075863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 06:59:31.075888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 06:59:31.950200: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_epoch =      0  elapsed = 00:01  loss = 1.1932e+04  error = 6.1075e-01  l1 = -0.001000  l2 = 0.002481\n",
      "tf_epoch =     10  elapsed = 00:01  loss = 9.6115e+03  error = 6.1174e-01  l1 = -0.010920  l2 = 0.002506\n",
      "tf_epoch =     20  elapsed = 00:02  loss = 7.6302e+03  error = 6.1292e-01  l1 = -0.021914  l2 = 0.002534\n",
      "tf_epoch =     30  elapsed = 00:03  loss = 6.3343e+03  error = 6.1419e-01  l1 = -0.034379  l2 = 0.002566\n",
      "tf_epoch =     40  elapsed = 00:04  loss = 5.8713e+03  error = 6.1537e-01  l1 = -0.047966  l2 = 0.002601\n",
      "tf_epoch =     50  elapsed = 00:05  loss = 5.5221e+03  error = 6.1626e-01  l1 = -0.061451  l2 = 0.002639\n",
      "tf_epoch =     60  elapsed = 00:06  loss = 5.1174e+03  error = 6.1662e-01  l1 = -0.073660  l2 = 0.002675\n",
      "tf_epoch =     70  elapsed = 00:07  loss = 4.6041e+03  error = 6.1623e-01  l1 = -0.083915  l2 = 0.002710\n",
      "tf_epoch =     80  elapsed = 00:08  loss = 4.0219e+03  error = 6.1506e-01  l1 = -0.091490  l2 = 0.002742\n",
      "tf_epoch =     90  elapsed = 00:08  loss = 3.4808e+03  error = 6.1379e-01  l1 = -0.095073  l2 = 0.002761\n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 00:10  loss = 1.4705e+03  error = 3.6899e-01  l1 = 0.281487  l2 = 0.003245\n",
      "nt_epoch =     20  elapsed = 00:12  loss = 7.9220e+02  error = 5.4785e-01  l1 = 0.421887  l2 = 0.004831\n",
      "nt_epoch =     30  elapsed = 00:13  loss = 4.3753e+02  error = 2.0761e+00  l1 = 0.742511  l2 = 0.015580\n",
      "nt_epoch =     40  elapsed = 00:14  loss = 3.3349e+02  error = 2.3699e+00  l1 = 0.927746  l2 = 0.018040\n",
      "nt_epoch =     50  elapsed = 00:16  loss = 2.6130e+02  error = 2.3489e+00  l1 = 1.028880  l2 = 0.018045\n",
      "nt_epoch =     60  elapsed = 00:17  loss = 2.0996e+02  error = 2.1879e+00  l1 = 1.005095  l2 = 0.017096\n",
      "nt_epoch =     70  elapsed = 00:18  loss = 1.6142e+02  error = 1.9255e+00  l1 = 0.939107  l2 = 0.015247\n",
      "nt_epoch =     80  elapsed = 00:19  loss = 1.4124e+02  error = 1.7598e+00  l1 = 0.939638  l2 = 0.014194\n",
      "nt_epoch =     90  elapsed = 00:20  loss = 1.2869e+02  error = 1.6092e+00  l1 = 0.940787  l2 = 0.013239\n",
      "nt_epoch =    100  elapsed = 00:22  loss = 1.1959e+02  error = 1.4437e+00  l1 = 0.947197  l2 = 0.012206\n",
      "nt_epoch =    110  elapsed = 00:23  loss = 1.1338e+02  error = 1.3957e+00  l1 = 0.968770  l2 = 0.011969\n",
      "nt_epoch =    120  elapsed = 00:24  loss = 1.0479e+02  error = 1.2208e+00  l1 = 0.972852  l2 = 0.010868\n",
      "nt_epoch =    130  elapsed = 00:26  loss = 9.0399e+01  error = 1.1650e+00  l1 = 0.989213  l2 = 0.010566\n",
      "nt_epoch =    140  elapsed = 00:27  loss = 7.6731e+01  error = 1.1464e+00  l1 = 0.997421  l2 = 0.010473\n",
      "nt_epoch =    150  elapsed = 00:28  loss = 6.7107e+01  error = 1.1277e+00  l1 = 0.998248  l2 = 0.010357\n",
      "nt_epoch =    160  elapsed = 00:29  loss = 5.7689e+01  error = 1.0591e+00  l1 = 1.010884  l2 = 0.009891\n",
      "nt_epoch =    170  elapsed = 00:30  loss = 5.1012e+01  error = 9.2862e-01  l1 = 1.004949  l2 = 0.009079\n",
      "nt_epoch =    180  elapsed = 00:32  loss = 4.5392e+01  error = 8.2927e-01  l1 = 1.004315  l2 = 0.008449\n",
      "nt_epoch =    190  elapsed = 00:33  loss = 4.1400e+01  error = 7.9557e-01  l1 = 1.003702  l2 = 0.008236\n",
      "nt_epoch =    200  elapsed = 00:34  loss = 3.9061e+01  error = 7.8914e-01  l1 = 1.000482  l2 = 0.008205\n",
      "nt_epoch =    210  elapsed = 00:35  loss = 3.5926e+01  error = 7.6724e-01  l1 = 0.996839  l2 = 0.008057\n",
      "nt_epoch =    220  elapsed = 00:37  loss = 3.2693e+01  error = 7.5291e-01  l1 = 1.002631  l2 = 0.007968\n",
      "nt_epoch =    230  elapsed = 00:38  loss = 2.9212e+01  error = 7.2199e-01  l1 = 0.989288  l2 = 0.007745\n",
      "nt_epoch =    240  elapsed = 00:39  loss = 2.5856e+01  error = 6.8020e-01  l1 = 0.984416  l2 = 0.007464\n",
      "nt_epoch =    250  elapsed = 00:40  loss = 2.3803e+01  error = 6.5306e-01  l1 = 0.985996  l2 = 0.007296\n",
      "nt_epoch =    260  elapsed = 00:41  loss = 2.2258e+01  error = 6.2614e-01  l1 = 0.979942  l2 = 0.007105\n",
      "nt_epoch =    270  elapsed = 00:43  loss = 2.0719e+01  error = 6.2554e-01  l1 = 0.981628  l2 = 0.007107\n",
      "nt_epoch =    280  elapsed = 00:44  loss = 1.9668e+01  error = 6.2410e-01  l1 = 0.985782  l2 = 0.007111\n",
      "nt_epoch =    290  elapsed = 00:45  loss = 1.8789e+01  error = 6.2362e-01  l1 = 0.978401  l2 = 0.007084\n",
      "nt_epoch =    300  elapsed = 00:46  loss = 1.8143e+01  error = 6.1620e-01  l1 = 0.979939  l2 = 0.007042\n",
      "nt_epoch =    310  elapsed = 00:48  loss = 1.7583e+01  error = 6.1529e-01  l1 = 0.979002  l2 = 0.007033\n",
      "nt_epoch =    320  elapsed = 00:49  loss = 1.6991e+01  error = 6.1677e-01  l1 = 0.980264  l2 = 0.007047\n",
      "nt_epoch =    330  elapsed = 00:50  loss = 1.6464e+01  error = 6.1634e-01  l1 = 0.980338  l2 = 0.007044\n",
      "nt_epoch =    340  elapsed = 00:51  loss = 1.5986e+01  error = 6.1076e-01  l1 = 0.978911  l2 = 0.007004\n",
      "nt_epoch =    350  elapsed = 00:52  loss = 1.5478e+01  error = 6.0019e-01  l1 = 0.982289  l2 = 0.006948\n",
      "nt_epoch =    360  elapsed = 00:54  loss = 1.5104e+01  error = 6.0221e-01  l1 = 0.981297  l2 = 0.006957\n",
      "nt_epoch =    370  elapsed = 00:55  loss = 1.4691e+01  error = 5.9122e-01  l1 = 0.980884  l2 = 0.006886\n",
      "nt_epoch =    380  elapsed = 00:56  loss = 1.4234e+01  error = 5.9298e-01  l1 = 0.984702  l2 = 0.006909\n",
      "nt_epoch =    390  elapsed = 00:58  loss = 1.3877e+01  error = 5.8991e-01  l1 = 0.986928  l2 = 0.006897\n",
      "nt_epoch =    400  elapsed = 00:59  loss = 1.3483e+01  error = 5.8009e-01  l1 = 0.985725  l2 = 0.006831\n",
      "nt_epoch =    410  elapsed = 01:00  loss = 1.3089e+01  error = 5.6587e-01  l1 = 0.984341  l2 = 0.006736\n",
      "nt_epoch =    420  elapsed = 01:01  loss = 1.2824e+01  error = 5.5989e-01  l1 = 0.982671  l2 = 0.006692\n",
      "nt_epoch =    430  elapsed = 01:03  loss = 1.2530e+01  error = 5.5560e-01  l1 = 0.984575  l2 = 0.006671\n",
      "nt_epoch =    440  elapsed = 01:04  loss = 1.2209e+01  error = 5.5494e-01  l1 = 0.983674  l2 = 0.006664\n",
      "nt_epoch =    450  elapsed = 01:05  loss = 1.1838e+01  error = 5.4959e-01  l1 = 0.983201  l2 = 0.006628\n",
      "nt_epoch =    460  elapsed = 01:06  loss = 1.1351e+01  error = 5.3528e-01  l1 = 0.984381  l2 = 0.006541\n",
      "nt_epoch =    470  elapsed = 01:07  loss = 1.0959e+01  error = 5.2632e-01  l1 = 0.980227  l2 = 0.006471\n",
      "nt_epoch =    480  elapsed = 01:09  loss = 1.0589e+01  error = 5.2169e-01  l1 = 0.982854  l2 = 0.006450\n",
      "nt_epoch =    490  elapsed = 01:10  loss = 1.0318e+01  error = 5.1624e-01  l1 = 0.982355  l2 = 0.006413\n",
      "nt_epoch =    500  elapsed = 01:11  loss = 1.0114e+01  error = 5.1106e-01  l1 = 0.982655  l2 = 0.006381\n",
      "nt_epoch =    510  elapsed = 01:13  loss = 9.8630e+00  error = 5.0438e-01  l1 = 0.982905  l2 = 0.006340\n",
      "nt_epoch =    520  elapsed = 01:14  loss = 9.6123e+00  error = 4.9705e-01  l1 = 0.984070  l2 = 0.006297\n",
      "nt_epoch =    530  elapsed = 01:15  loss = 9.3682e+00  error = 4.9374e-01  l1 = 0.986906  l2 = 0.006285\n",
      "nt_epoch =    540  elapsed = 01:17  loss = 9.1823e+00  error = 4.8998e-01  l1 = 0.985348  l2 = 0.006256\n",
      "nt_epoch =    550  elapsed = 01:18  loss = 8.9983e+00  error = 4.8354e-01  l1 = 0.985098  l2 = 0.006214\n",
      "nt_epoch =    560  elapsed = 01:19  loss = 8.8411e+00  error = 4.7814e-01  l1 = 0.983939  l2 = 0.006176\n",
      "nt_epoch =    570  elapsed = 01:21  loss = 8.6685e+00  error = 4.7166e-01  l1 = 0.986656  l2 = 0.006143\n",
      "nt_epoch =    580  elapsed = 01:22  loss = 8.4650e+00  error = 4.6841e-01  l1 = 0.986907  l2 = 0.006123\n",
      "nt_epoch =    590  elapsed = 01:23  loss = 8.2864e+00  error = 4.6389e-01  l1 = 0.983577  l2 = 0.006084\n",
      "nt_epoch =    600  elapsed = 01:25  loss = 8.0885e+00  error = 4.5626e-01  l1 = 0.985486  l2 = 0.006042\n",
      "nt_epoch =    610  elapsed = 01:26  loss = 7.9032e+00  error = 4.5243e-01  l1 = 0.986662  l2 = 0.006021\n",
      "nt_epoch =    620  elapsed = 01:27  loss = 7.6629e+00  error = 4.4431e-01  l1 = 0.986387  l2 = 0.005968\n",
      "nt_epoch =    630  elapsed = 01:28  loss = 7.4504e+00  error = 4.3967e-01  l1 = 0.984460  l2 = 0.005933\n",
      "nt_epoch =    640  elapsed = 01:30  loss = 7.3457e+00  error = 4.3830e-01  l1 = 0.984248  l2 = 0.005923\n",
      "nt_epoch =    650  elapsed = 01:31  loss = 7.1440e+00  error = 4.3154e-01  l1 = 0.986480  l2 = 0.005887\n",
      "nt_epoch =    660  elapsed = 01:32  loss = 6.9770e+00  error = 4.2637e-01  l1 = 0.984576  l2 = 0.005848\n",
      "nt_epoch =    670  elapsed = 01:34  loss = 6.8426e+00  error = 4.2209e-01  l1 = 0.984796  l2 = 0.005822\n",
      "nt_epoch =    680  elapsed = 01:35  loss = 6.7109e+00  error = 4.1714e-01  l1 = 0.985911  l2 = 0.005794\n",
      "nt_epoch =    690  elapsed = 01:36  loss = 6.5892e+00  error = 4.1396e-01  l1 = 0.984649  l2 = 0.005770\n",
      "nt_epoch =    700  elapsed = 01:38  loss = 6.4322e+00  error = 4.0755e-01  l1 = 0.984675  l2 = 0.005729\n",
      "nt_epoch =    710  elapsed = 01:39  loss = 6.3035e+00  error = 4.0105e-01  l1 = 0.984724  l2 = 0.005688\n",
      "nt_epoch =    720  elapsed = 01:40  loss = 6.1989e+00  error = 3.9695e-01  l1 = 0.984222  l2 = 0.005660\n",
      "nt_epoch =    730  elapsed = 01:42  loss = 6.0798e+00  error = 3.9251e-01  l1 = 0.985411  l2 = 0.005635\n",
      "nt_epoch =    740  elapsed = 01:43  loss = 5.9606e+00  error = 3.8855e-01  l1 = 0.985205  l2 = 0.005610\n",
      "nt_epoch =    750  elapsed = 01:44  loss = 5.8695e+00  error = 3.8659e-01  l1 = 0.983968  l2 = 0.005593\n",
      "nt_epoch =    760  elapsed = 01:46  loss = 5.7595e+00  error = 3.8280e-01  l1 = 0.984509  l2 = 0.005571\n",
      "nt_epoch =    770  elapsed = 01:47  loss = 5.6678e+00  error = 3.8025e-01  l1 = 0.985918  l2 = 0.005559\n",
      "nt_epoch =    780  elapsed = 01:48  loss = 5.5977e+00  error = 3.7726e-01  l1 = 0.986125  l2 = 0.005541\n",
      "nt_epoch =    790  elapsed = 01:50  loss = 5.5372e+00  error = 3.7569e-01  l1 = 0.985341  l2 = 0.005528\n",
      "nt_epoch =    800  elapsed = 01:51  loss = 5.4095e+00  error = 3.7223e-01  l1 = 0.984857  l2 = 0.005505\n",
      "nt_epoch =    810  elapsed = 01:52  loss = 5.3549e+00  error = 3.6955e-01  l1 = 0.984545  l2 = 0.005487\n",
      "nt_epoch =    820  elapsed = 01:53  loss = 5.2588e+00  error = 3.6612e-01  l1 = 0.983822  l2 = 0.005462\n",
      "nt_epoch =    830  elapsed = 01:55  loss = 5.1805e+00  error = 3.6530e-01  l1 = 0.983067  l2 = 0.005455\n",
      "nt_epoch =    840  elapsed = 01:56  loss = 5.0771e+00  error = 3.6289e-01  l1 = 0.982482  l2 = 0.005438\n",
      "nt_epoch =    850  elapsed = 01:57  loss = 5.0084e+00  error = 3.6181e-01  l1 = 0.982481  l2 = 0.005431\n",
      "nt_epoch =    860  elapsed = 01:58  loss = 4.9123e+00  error = 3.5770e-01  l1 = 0.982705  l2 = 0.005405\n",
      "nt_epoch =    870  elapsed = 02:00  loss = 4.8584e+00  error = 3.5652e-01  l1 = 0.982121  l2 = 0.005396\n",
      "nt_epoch =    880  elapsed = 02:01  loss = 4.7996e+00  error = 3.5599e-01  l1 = 0.981453  l2 = 0.005390\n",
      "nt_epoch =    890  elapsed = 02:02  loss = 4.7249e+00  error = 3.5545e-01  l1 = 0.981711  l2 = 0.005388\n",
      "nt_epoch =    900  elapsed = 02:04  loss = 4.6585e+00  error = 3.5224e-01  l1 = 0.981683  l2 = 0.005367\n",
      "nt_epoch =    910  elapsed = 02:05  loss = 4.5662e+00  error = 3.4706e-01  l1 = 0.982061  l2 = 0.005335\n",
      "nt_epoch =    920  elapsed = 02:06  loss = 4.5105e+00  error = 3.4449e-01  l1 = 0.981859  l2 = 0.005318\n",
      "nt_epoch =    930  elapsed = 02:07  loss = 4.4671e+00  error = 3.4335e-01  l1 = 0.981476  l2 = 0.005310\n",
      "nt_epoch =    940  elapsed = 02:09  loss = 4.4299e+00  error = 3.4243e-01  l1 = 0.980486  l2 = 0.005301\n",
      "nt_epoch =    950  elapsed = 02:10  loss = 4.3810e+00  error = 3.4091e-01  l1 = 0.980339  l2 = 0.005291\n",
      "nt_epoch =    960  elapsed = 02:11  loss = 4.3415e+00  error = 3.3961e-01  l1 = 0.980538  l2 = 0.005283\n",
      "nt_epoch =    970  elapsed = 02:13  loss = 4.3052e+00  error = 3.3841e-01  l1 = 0.981540  l2 = 0.005279\n",
      "nt_epoch =    980  elapsed = 02:14  loss = 4.2436e+00  error = 3.3642e-01  l1 = 0.981466  l2 = 0.005266\n",
      "nt_epoch =    990  elapsed = 02:15  loss = 4.1991e+00  error = 3.3570e-01  l1 = 0.980460  l2 = 0.005258\n",
      "nt_epoch =   1000  elapsed = 02:17  loss = 4.1612e+00  error = 3.3522e-01  l1 = 0.979875  l2 = 0.005253\n",
      "nt_epoch =   1010  elapsed = 02:18  loss = 4.1315e+00  error = 3.3306e-01  l1 = 0.979811  l2 = 0.005239\n",
      "nt_epoch =   1020  elapsed = 02:19  loss = 4.1113e+00  error = 3.3209e-01  l1 = 0.979637  l2 = 0.005232\n",
      "nt_epoch =   1030  elapsed = 02:20  loss = 4.0632e+00  error = 3.3161e-01  l1 = 0.979958  l2 = 0.005230\n",
      "nt_epoch =   1040  elapsed = 02:22  loss = 4.0142e+00  error = 3.3039e-01  l1 = 0.979735  l2 = 0.005222\n",
      "nt_epoch =   1050  elapsed = 02:23  loss = 4.0004e+00  error = 3.3085e-01  l1 = 0.979620  l2 = 0.005224\n",
      "nt_epoch =   1060  elapsed = 02:24  loss = 3.9628e+00  error = 3.2931e-01  l1 = 0.979632  l2 = 0.005215\n",
      "nt_epoch =   1070  elapsed = 02:26  loss = 3.9168e+00  error = 3.2738e-01  l1 = 0.979941  l2 = 0.005203\n",
      "nt_epoch =   1080  elapsed = 02:27  loss = 3.9012e+00  error = 3.2625e-01  l1 = 0.979880  l2 = 0.005196\n",
      "nt_epoch =   1090  elapsed = 02:28  loss = 3.8824e+00  error = 3.2512e-01  l1 = 0.979510  l2 = 0.005188\n",
      "nt_epoch =   1100  elapsed = 02:29  loss = 3.8399e+00  error = 3.2314e-01  l1 = 0.979030  l2 = 0.005174\n",
      "nt_epoch =   1110  elapsed = 02:30  loss = 3.8118e+00  error = 3.2175e-01  l1 = 0.979225  l2 = 0.005165\n",
      "nt_epoch =   1120  elapsed = 02:32  loss = 3.7915e+00  error = 3.2147e-01  l1 = 0.979308  l2 = 0.005164\n",
      "nt_epoch =   1130  elapsed = 02:33  loss = 3.7679e+00  error = 3.1941e-01  l1 = 0.979510  l2 = 0.005151\n",
      "nt_epoch =   1140  elapsed = 02:34  loss = 3.7398e+00  error = 3.1785e-01  l1 = 0.979467  l2 = 0.005141\n",
      "nt_epoch =   1150  elapsed = 02:36  loss = 3.7137e+00  error = 3.1713e-01  l1 = 0.979687  l2 = 0.005137\n",
      "nt_epoch =   1160  elapsed = 02:37  loss = 3.6930e+00  error = 3.1621e-01  l1 = 0.979724  l2 = 0.005132\n",
      "nt_epoch =   1170  elapsed = 02:38  loss = 3.6673e+00  error = 3.1551e-01  l1 = 0.979670  l2 = 0.005127\n",
      "nt_epoch =   1180  elapsed = 02:40  loss = 3.6409e+00  error = 3.1533e-01  l1 = 0.979478  l2 = 0.005125\n",
      "nt_epoch =   1190  elapsed = 02:41  loss = 3.6266e+00  error = 3.1456e-01  l1 = 0.979544  l2 = 0.005121\n",
      "nt_epoch =   1200  elapsed = 02:42  loss = 3.6102e+00  error = 3.1391e-01  l1 = 0.979578  l2 = 0.005116\n",
      "nt_epoch =   1210  elapsed = 02:43  loss = 3.5927e+00  error = 3.1331e-01  l1 = 0.979424  l2 = 0.005112\n",
      "nt_epoch =   1220  elapsed = 02:45  loss = 3.5669e+00  error = 3.1266e-01  l1 = 0.979416  l2 = 0.005108\n",
      "nt_epoch =   1230  elapsed = 02:46  loss = 3.5456e+00  error = 3.1266e-01  l1 = 0.979578  l2 = 0.005109\n",
      "nt_epoch =   1240  elapsed = 02:47  loss = 3.5233e+00  error = 3.1269e-01  l1 = 0.979706  l2 = 0.005109\n",
      "nt_epoch =   1250  elapsed = 02:48  loss = 3.5082e+00  error = 3.1230e-01  l1 = 0.979947  l2 = 0.005107\n",
      "nt_epoch =   1260  elapsed = 02:50  loss = 3.4835e+00  error = 3.1138e-01  l1 = 0.979965  l2 = 0.005102\n",
      "nt_epoch =   1270  elapsed = 02:51  loss = 3.4674e+00  error = 3.1072e-01  l1 = 0.979853  l2 = 0.005097\n",
      "nt_epoch =   1280  elapsed = 02:52  loss = 3.4448e+00  error = 3.0926e-01  l1 = 0.979750  l2 = 0.005087\n",
      "nt_epoch =   1290  elapsed = 02:53  loss = 3.4213e+00  error = 3.0818e-01  l1 = 0.979596  l2 = 0.005080\n",
      "nt_epoch =   1300  elapsed = 02:55  loss = 3.3996e+00  error = 3.0718e-01  l1 = 0.979558  l2 = 0.005074\n",
      "nt_epoch =   1310  elapsed = 02:56  loss = 3.3787e+00  error = 3.0675e-01  l1 = 0.979676  l2 = 0.005071\n",
      "nt_epoch =   1320  elapsed = 02:57  loss = 3.3680e+00  error = 3.0713e-01  l1 = 0.979602  l2 = 0.005073\n",
      "nt_epoch =   1330  elapsed = 02:58  loss = 3.3517e+00  error = 3.0653e-01  l1 = 0.979684  l2 = 0.005070\n",
      "nt_epoch =   1340  elapsed = 03:00  loss = 3.3400e+00  error = 3.0563e-01  l1 = 0.979351  l2 = 0.005063\n",
      "nt_epoch =   1350  elapsed = 03:01  loss = 3.3194e+00  error = 3.0420e-01  l1 = 0.979089  l2 = 0.005053\n",
      "nt_epoch =   1360  elapsed = 03:02  loss = 3.2969e+00  error = 3.0365e-01  l1 = 0.979503  l2 = 0.005051\n",
      "nt_epoch =   1370  elapsed = 03:03  loss = 3.2870e+00  error = 3.0282e-01  l1 = 0.979563  l2 = 0.005046\n",
      "nt_epoch =   1380  elapsed = 03:05  loss = 3.2676e+00  error = 3.0289e-01  l1 = 0.979861  l2 = 0.005047\n",
      "nt_epoch =   1390  elapsed = 03:06  loss = 3.2537e+00  error = 3.0198e-01  l1 = 0.980178  l2 = 0.005042\n",
      "nt_epoch =   1400  elapsed = 03:07  loss = 3.2327e+00  error = 3.0150e-01  l1 = 0.979950  l2 = 0.005039\n",
      "nt_epoch =   1410  elapsed = 03:08  loss = 3.2207e+00  error = 3.0187e-01  l1 = 0.979807  l2 = 0.005041\n",
      "nt_epoch =   1420  elapsed = 03:10  loss = 3.2032e+00  error = 3.0066e-01  l1 = 0.979358  l2 = 0.005031\n",
      "nt_epoch =   1430  elapsed = 03:11  loss = 3.1858e+00  error = 2.9998e-01  l1 = 0.979112  l2 = 0.005026\n",
      "nt_epoch =   1440  elapsed = 03:12  loss = 3.1695e+00  error = 2.9975e-01  l1 = 0.979145  l2 = 0.005025\n",
      "nt_epoch =   1450  elapsed = 03:14  loss = 3.1366e+00  error = 2.9922e-01  l1 = 0.979832  l2 = 0.005024\n",
      "nt_epoch =   1460  elapsed = 03:15  loss = 3.1235e+00  error = 2.9827e-01  l1 = 0.980257  l2 = 0.005019\n",
      "nt_epoch =   1470  elapsed = 03:16  loss = 3.1105e+00  error = 2.9812e-01  l1 = 0.980502  l2 = 0.005019\n",
      "nt_epoch =   1480  elapsed = 03:17  loss = 3.0801e+00  error = 2.9647e-01  l1 = 0.980639  l2 = 0.005009\n",
      "nt_epoch =   1490  elapsed = 03:19  loss = 3.0676e+00  error = 2.9569e-01  l1 = 0.980773  l2 = 0.005004\n",
      "nt_epoch =   1500  elapsed = 03:20  loss = 3.0595e+00  error = 2.9498e-01  l1 = 0.980829  l2 = 0.005000\n",
      "nt_epoch =   1510  elapsed = 03:21  loss = 3.0497e+00  error = 2.9433e-01  l1 = 0.980855  l2 = 0.004996\n",
      "nt_epoch =   1520  elapsed = 03:22  loss = 3.0330e+00  error = 2.9391e-01  l1 = 0.980799  l2 = 0.004993\n",
      "nt_epoch =   1530  elapsed = 03:24  loss = 3.0228e+00  error = 2.9302e-01  l1 = 0.980836  l2 = 0.004988\n",
      "nt_epoch =   1540  elapsed = 03:25  loss = 3.0211e+00  error = 2.9218e-01  l1 = 0.981072  l2 = 0.004983\n",
      "nt_epoch =   1550  elapsed = 03:26  loss = 2.9923e+00  error = 2.9014e-01  l1 = 0.981281  l2 = 0.004971\n",
      "nt_epoch =   1560  elapsed = 03:27  loss = 2.9726e+00  error = 2.8844e-01  l1 = 0.981591  l2 = 0.004961\n",
      "nt_epoch =   1570  elapsed = 03:29  loss = 2.9558e+00  error = 2.8678e-01  l1 = 0.981197  l2 = 0.004949\n",
      "nt_epoch =   1580  elapsed = 03:30  loss = 2.9451e+00  error = 2.8677e-01  l1 = 0.981050  l2 = 0.004948\n",
      "nt_epoch =   1590  elapsed = 03:31  loss = 2.9352e+00  error = 2.8643e-01  l1 = 0.980911  l2 = 0.004946\n",
      "nt_epoch =   1600  elapsed = 03:33  loss = 2.9200e+00  error = 2.8538e-01  l1 = 0.980748  l2 = 0.004939\n",
      "nt_epoch =   1610  elapsed = 03:34  loss = 2.9152e+00  error = 2.8488e-01  l1 = 0.980953  l2 = 0.004936\n",
      "nt_epoch =   1620  elapsed = 03:35  loss = 2.9094e+00  error = 2.8459e-01  l1 = 0.980666  l2 = 0.004933\n",
      "nt_epoch =   1630  elapsed = 03:36  loss = 2.8926e+00  error = 2.8476e-01  l1 = 0.980173  l2 = 0.004933\n",
      "nt_epoch =   1640  elapsed = 03:38  loss = 2.8739e+00  error = 2.8327e-01  l1 = 0.980378  l2 = 0.004924\n",
      "nt_epoch =   1650  elapsed = 03:39  loss = 2.8604e+00  error = 2.8276e-01  l1 = 0.980755  l2 = 0.004922\n",
      "nt_epoch =   1660  elapsed = 03:40  loss = 2.8536e+00  error = 2.8220e-01  l1 = 0.980929  l2 = 0.004919\n",
      "nt_epoch =   1670  elapsed = 03:42  loss = 2.8511e+00  error = 2.8219e-01  l1 = 0.981012  l2 = 0.004919\n",
      "nt_epoch =   1680  elapsed = 03:43  loss = 2.8411e+00  error = 2.8190e-01  l1 = 0.981303  l2 = 0.004918\n",
      "nt_epoch =   1690  elapsed = 03:44  loss = 2.8141e+00  error = 2.8095e-01  l1 = 0.981484  l2 = 0.004913\n",
      "nt_epoch =   1700  elapsed = 03:45  loss = 2.7964e+00  error = 2.7939e-01  l1 = 0.981454  l2 = 0.004903\n",
      "nt_epoch =   1710  elapsed = 03:47  loss = 2.7849e+00  error = 2.7921e-01  l1 = 0.981545  l2 = 0.004902\n",
      "nt_epoch =   1720  elapsed = 03:48  loss = 2.7842e+00  error = 2.7863e-01  l1 = 0.981710  l2 = 0.004899\n",
      "nt_epoch =   1730  elapsed = 03:49  loss = 2.7727e+00  error = 2.7821e-01  l1 = 0.981622  l2 = 0.004896\n",
      "nt_epoch =   1740  elapsed = 03:51  loss = 2.7597e+00  error = 2.7697e-01  l1 = 0.981644  l2 = 0.004888\n",
      "nt_epoch =   1750  elapsed = 03:52  loss = 2.7477e+00  error = 2.7577e-01  l1 = 0.981591  l2 = 0.004880\n",
      "nt_epoch =   1760  elapsed = 03:53  loss = 2.7302e+00  error = 2.7494e-01  l1 = 0.981519  l2 = 0.004875\n",
      "nt_epoch =   1770  elapsed = 03:55  loss = 2.7177e+00  error = 2.7451e-01  l1 = 0.981420  l2 = 0.004872\n",
      "nt_epoch =   1780  elapsed = 03:56  loss = 2.7035e+00  error = 2.7379e-01  l1 = 0.981445  l2 = 0.004867\n",
      "nt_epoch =   1790  elapsed = 03:57  loss = 2.6816e+00  error = 2.7258e-01  l1 = 0.981479  l2 = 0.004859\n",
      "nt_epoch =   1800  elapsed = 03:59  loss = 2.6664e+00  error = 2.7095e-01  l1 = 0.981823  l2 = 0.004850\n",
      "nt_epoch =   1810  elapsed = 04:00  loss = 2.6571e+00  error = 2.6983e-01  l1 = 0.982188  l2 = 0.004844\n",
      "nt_epoch =   1820  elapsed = 04:01  loss = 2.6432e+00  error = 2.6925e-01  l1 = 0.982697  l2 = 0.004842\n",
      "nt_epoch =   1830  elapsed = 04:02  loss = 2.6324e+00  error = 2.6788e-01  l1 = 0.983014  l2 = 0.004834\n",
      "nt_epoch =   1840  elapsed = 04:04  loss = 2.6199e+00  error = 2.6793e-01  l1 = 0.983057  l2 = 0.004835\n",
      "nt_epoch =   1850  elapsed = 04:05  loss = 2.6042e+00  error = 2.6700e-01  l1 = 0.982712  l2 = 0.004828\n",
      "nt_epoch =   1860  elapsed = 04:06  loss = 2.5834e+00  error = 2.6677e-01  l1 = 0.982601  l2 = 0.004826\n",
      "nt_epoch =   1870  elapsed = 04:07  loss = 2.5784e+00  error = 2.6691e-01  l1 = 0.982438  l2 = 0.004826\n",
      "nt_epoch =   1880  elapsed = 04:09  loss = 2.5651e+00  error = 2.6696e-01  l1 = 0.982475  l2 = 0.004827\n",
      "nt_epoch =   1890  elapsed = 04:10  loss = 2.5502e+00  error = 2.6637e-01  l1 = 0.982701  l2 = 0.004824\n",
      "nt_epoch =   1900  elapsed = 04:12  loss = 2.5421e+00  error = 2.6492e-01  l1 = 0.983137  l2 = 0.004816\n",
      "nt_epoch =   1910  elapsed = 04:13  loss = 2.5248e+00  error = 2.6380e-01  l1 = 0.983480  l2 = 0.004810\n",
      "nt_epoch =   1920  elapsed = 04:14  loss = 2.5153e+00  error = 2.6282e-01  l1 = 0.983550  l2 = 0.004804\n",
      "nt_epoch =   1930  elapsed = 04:16  loss = 2.5065e+00  error = 2.6208e-01  l1 = 0.983510  l2 = 0.004799\n",
      "nt_epoch =   1940  elapsed = 04:17  loss = 2.4947e+00  error = 2.6141e-01  l1 = 0.983487  l2 = 0.004795\n",
      "nt_epoch =   1950  elapsed = 04:18  loss = 2.4707e+00  error = 2.6111e-01  l1 = 0.983581  l2 = 0.004793\n",
      "nt_epoch =   1960  elapsed = 04:19  loss = 2.4522e+00  error = 2.6031e-01  l1 = 0.983743  l2 = 0.004789\n",
      "nt_epoch =   1970  elapsed = 04:21  loss = 2.4439e+00  error = 2.5993e-01  l1 = 0.983858  l2 = 0.004786\n",
      "nt_epoch =   1980  elapsed = 04:22  loss = 2.4303e+00  error = 2.5933e-01  l1 = 0.983840  l2 = 0.004783\n",
      "nt_epoch =   1990  elapsed = 04:23  loss = 2.4150e+00  error = 2.5888e-01  l1 = 0.983594  l2 = 0.004779\n",
      "==================\n",
      "Training finished (epoch 100): duration = 04:24  error = 2.5882e-01  l1 = 0.983499  l2 = 0.004778\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_1 (Lambda)           (None, 1)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                100       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 81)                4131      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,331\n",
      "Trainable params: 9,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "tf_epoch =      0  elapsed = 04:24  loss = 1.1782e+04  error = 6.1151e-01  l1 = -0.000980  l2 = 0.002476\n",
      "tf_epoch =     10  elapsed = 04:25  loss = 8.4796e+03  error = 6.1449e-01  l1 = -0.019812  l2 = 0.002517\n",
      "tf_epoch =     20  elapsed = 04:26  loss = 6.0641e+03  error = 6.1711e-01  l1 = -0.042639  l2 = 0.002573\n",
      "tf_epoch =     30  elapsed = 04:27  loss = 5.4048e+03  error = 6.1916e-01  l1 = -0.066629  l2 = 0.002637\n",
      "tf_epoch =     40  elapsed = 04:28  loss = 4.4876e+03  error = 6.1963e-01  l1 = -0.086633  l2 = 0.002697\n",
      "tf_epoch =     50  elapsed = 04:28  loss = 3.5569e+03  error = 6.1839e-01  l1 = -0.098415  l2 = 0.002743\n",
      "tf_epoch =     60  elapsed = 04:29  loss = 2.8749e+03  error = 6.1922e-01  l1 = -0.096801  l2 = 0.002732\n",
      "tf_epoch =     70  elapsed = 04:30  loss = 2.4625e+03  error = 6.2138e-01  l1 = -0.081114  l2 = 0.002669\n",
      "tf_epoch =     80  elapsed = 04:31  loss = 2.2001e+03  error = 6.2143e-01  l1 = -0.058384  l2 = 0.002596\n",
      "tf_epoch =     90  elapsed = 04:31  loss = 1.9843e+03  error = 6.2014e-01  l1 = -0.034024  l2 = 0.002527\n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 04:33  loss = 1.0437e+03  error = 4.2495e-01  l1 = 0.338392  l2 = 0.002584\n",
      "nt_epoch =     20  elapsed = 04:34  loss = 7.8658e+02  error = 4.2272e-01  l1 = 0.359378  l2 = 0.003835\n",
      "nt_epoch =     30  elapsed = 04:35  loss = 6.0357e+02  error = 1.6446e+00  l1 = 0.531232  l2 = 0.012161\n",
      "nt_epoch =     40  elapsed = 04:36  loss = 3.8646e+02  error = 2.4970e+00  l1 = 0.863662  l2 = 0.018646\n",
      "nt_epoch =     50  elapsed = 04:37  loss = 2.9600e+02  error = 2.6562e+00  l1 = 1.031640  l2 = 0.019992\n",
      "nt_epoch =     60  elapsed = 04:39  loss = 2.3320e+02  error = 2.3390e+00  l1 = 1.009751  l2 = 0.018042\n",
      "nt_epoch =     70  elapsed = 04:40  loss = 1.7660e+02  error = 2.1299e+00  l1 = 0.970476  l2 = 0.016648\n",
      "nt_epoch =     80  elapsed = 04:41  loss = 1.3194e+02  error = 1.8845e+00  l1 = 0.961601  l2 = 0.015058\n",
      "nt_epoch =     90  elapsed = 04:42  loss = 1.1800e+02  error = 1.7817e+00  l1 = 0.966683  l2 = 0.014420\n",
      "nt_epoch =    100  elapsed = 04:44  loss = 1.0510e+02  error = 1.6340e+00  l1 = 0.969932  l2 = 0.013490\n",
      "nt_epoch =    110  elapsed = 04:45  loss = 9.2463e+01  error = 1.5950e+00  l1 = 0.998081  l2 = 0.013331\n",
      "nt_epoch =    120  elapsed = 04:46  loss = 8.0975e+01  error = 1.5273e+00  l1 = 0.997562  l2 = 0.012899\n",
      "nt_epoch =    130  elapsed = 04:47  loss = 6.9108e+01  error = 1.3696e+00  l1 = 0.991367  l2 = 0.011875\n",
      "nt_epoch =    140  elapsed = 04:49  loss = 5.9656e+01  error = 1.3088e+00  l1 = 1.009803  l2 = 0.011484\n",
      "nt_epoch =    150  elapsed = 04:50  loss = 5.2841e+01  error = 1.1983e+00  l1 = 1.002652  l2 = 0.010803\n",
      "nt_epoch =    160  elapsed = 04:51  loss = 4.7849e+01  error = 1.1016e+00  l1 = 0.999132  l2 = 0.010193\n",
      "nt_epoch =    170  elapsed = 04:52  loss = 4.4470e+01  error = 1.0504e+00  l1 = 1.008782  l2 = 0.009842\n",
      "nt_epoch =    180  elapsed = 04:54  loss = 4.1097e+01  error = 9.3980e-01  l1 = 0.997673  l2 = 0.009159\n",
      "nt_epoch =    190  elapsed = 04:55  loss = 3.7994e+01  error = 8.9302e-01  l1 = 1.004178  l2 = 0.008855\n",
      "nt_epoch =    200  elapsed = 04:56  loss = 3.4956e+01  error = 8.1377e-01  l1 = 0.998389  l2 = 0.008359\n",
      "nt_epoch =    210  elapsed = 04:57  loss = 3.1515e+01  error = 7.5686e-01  l1 = 0.988118  l2 = 0.007964\n",
      "nt_epoch =    220  elapsed = 04:59  loss = 2.8835e+01  error = 7.3149e-01  l1 = 0.992242  l2 = 0.007815\n",
      "nt_epoch =    230  elapsed = 05:00  loss = 2.6507e+01  error = 7.0022e-01  l1 = 0.986301  l2 = 0.007597\n",
      "nt_epoch =    240  elapsed = 05:01  loss = 2.4325e+01  error = 6.6055e-01  l1 = 0.978569  l2 = 0.007320\n",
      "nt_epoch =    250  elapsed = 05:02  loss = 2.2563e+01  error = 6.4307e-01  l1 = 0.982618  l2 = 0.007222\n",
      "nt_epoch =    260  elapsed = 05:04  loss = 2.1574e+01  error = 6.3247e-01  l1 = 0.984204  l2 = 0.007159\n",
      "nt_epoch =    270  elapsed = 05:05  loss = 2.0684e+01  error = 6.1390e-01  l1 = 0.978490  l2 = 0.007023\n",
      "nt_epoch =    280  elapsed = 05:06  loss = 1.9523e+01  error = 6.1414e-01  l1 = 0.981955  l2 = 0.007035\n",
      "nt_epoch =    290  elapsed = 05:08  loss = 1.8357e+01  error = 5.9635e-01  l1 = 0.978827  l2 = 0.006912\n",
      "nt_epoch =    300  elapsed = 05:09  loss = 1.7407e+01  error = 5.7759e-01  l1 = 0.979246  l2 = 0.006794\n",
      "nt_epoch =    310  elapsed = 05:10  loss = 1.6674e+01  error = 5.5845e-01  l1 = 0.978157  l2 = 0.006669\n",
      "nt_epoch =    320  elapsed = 05:11  loss = 1.5866e+01  error = 5.4501e-01  l1 = 0.979972  l2 = 0.006589\n",
      "nt_epoch =    330  elapsed = 05:13  loss = 1.5040e+01  error = 5.2760e-01  l1 = 0.978870  l2 = 0.006475\n",
      "nt_epoch =    340  elapsed = 05:14  loss = 1.4079e+01  error = 5.0321e-01  l1 = 0.980966  l2 = 0.006326\n",
      "nt_epoch =    350  elapsed = 05:15  loss = 1.3133e+01  error = 4.9014e-01  l1 = 0.977328  l2 = 0.006231\n",
      "nt_epoch =    360  elapsed = 05:16  loss = 1.2415e+01  error = 4.7729e-01  l1 = 0.983923  l2 = 0.006170\n",
      "nt_epoch =    370  elapsed = 05:18  loss = 1.2028e+01  error = 4.7626e-01  l1 = 0.981393  l2 = 0.006156\n",
      "nt_epoch =    380  elapsed = 05:19  loss = 1.1702e+01  error = 4.7084e-01  l1 = 0.982238  l2 = 0.006124\n",
      "nt_epoch =    390  elapsed = 05:20  loss = 1.1347e+01  error = 4.6653e-01  l1 = 0.986466  l2 = 0.006110\n",
      "nt_epoch =    400  elapsed = 05:22  loss = 1.0948e+01  error = 4.5967e-01  l1 = 0.981902  l2 = 0.006052\n",
      "nt_epoch =    410  elapsed = 05:23  loss = 1.0650e+01  error = 4.5350e-01  l1 = 0.983934  l2 = 0.006019\n",
      "nt_epoch =    420  elapsed = 05:24  loss = 1.0240e+01  error = 4.5544e-01  l1 = 0.986686  l2 = 0.006040\n",
      "nt_epoch =    430  elapsed = 05:26  loss = 9.9787e+00  error = 4.5380e-01  l1 = 0.987156  l2 = 0.006031\n",
      "nt_epoch =    440  elapsed = 05:27  loss = 9.5172e+00  error = 4.4903e-01  l1 = 0.986989  l2 = 0.006000\n",
      "nt_epoch =    450  elapsed = 05:28  loss = 9.1935e+00  error = 4.4316e-01  l1 = 0.984914  l2 = 0.005956\n",
      "nt_epoch =    460  elapsed = 05:30  loss = 8.8067e+00  error = 4.3508e-01  l1 = 0.985060  l2 = 0.005905\n",
      "nt_epoch =    470  elapsed = 05:31  loss = 8.5444e+00  error = 4.2721e-01  l1 = 0.983783  l2 = 0.005851\n",
      "nt_epoch =    480  elapsed = 05:32  loss = 8.3412e+00  error = 4.2384e-01  l1 = 0.983907  l2 = 0.005830\n",
      "nt_epoch =    490  elapsed = 05:33  loss = 8.1479e+00  error = 4.1690e-01  l1 = 0.983535  l2 = 0.005785\n",
      "nt_epoch =    500  elapsed = 05:35  loss = 7.9546e+00  error = 4.1209e-01  l1 = 0.982362  l2 = 0.005750\n",
      "nt_epoch =    510  elapsed = 05:36  loss = 7.7450e+00  error = 4.0877e-01  l1 = 0.984417  l2 = 0.005736\n",
      "nt_epoch =    520  elapsed = 05:37  loss = 7.5015e+00  error = 4.0190e-01  l1 = 0.983052  l2 = 0.005688\n",
      "nt_epoch =    530  elapsed = 05:39  loss = 7.3683e+00  error = 3.9932e-01  l1 = 0.982624  l2 = 0.005670\n",
      "nt_epoch =    540  elapsed = 05:40  loss = 7.1908e+00  error = 3.9440e-01  l1 = 0.982680  l2 = 0.005639\n",
      "nt_epoch =    550  elapsed = 05:41  loss = 7.0460e+00  error = 3.8741e-01  l1 = 0.981128  l2 = 0.005589\n",
      "nt_epoch =    560  elapsed = 05:42  loss = 6.9182e+00  error = 3.8304e-01  l1 = 0.981187  l2 = 0.005562\n",
      "nt_epoch =    570  elapsed = 05:44  loss = 6.8112e+00  error = 3.8057e-01  l1 = 0.981950  l2 = 0.005548\n",
      "nt_epoch =    580  elapsed = 05:45  loss = 6.7028e+00  error = 3.7797e-01  l1 = 0.981994  l2 = 0.005532\n",
      "nt_epoch =    590  elapsed = 05:46  loss = 6.5849e+00  error = 3.7432e-01  l1 = 0.983388  l2 = 0.005513\n",
      "nt_epoch =    600  elapsed = 05:48  loss = 6.5147e+00  error = 3.7281e-01  l1 = 0.982958  l2 = 0.005502\n",
      "nt_epoch =    610  elapsed = 05:49  loss = 6.4245e+00  error = 3.6815e-01  l1 = 0.981928  l2 = 0.005469\n",
      "nt_epoch =    620  elapsed = 05:50  loss = 6.3520e+00  error = 3.6573e-01  l1 = 0.982194  l2 = 0.005455\n",
      "nt_epoch =    630  elapsed = 05:52  loss = 6.2852e+00  error = 3.6400e-01  l1 = 0.982898  l2 = 0.005446\n",
      "nt_epoch =    640  elapsed = 05:53  loss = 6.2046e+00  error = 3.6279e-01  l1 = 0.983433  l2 = 0.005440\n",
      "nt_epoch =    650  elapsed = 05:54  loss = 6.1561e+00  error = 3.6372e-01  l1 = 0.983244  l2 = 0.005445\n",
      "nt_epoch =    660  elapsed = 05:56  loss = 6.0997e+00  error = 3.6226e-01  l1 = 0.982674  l2 = 0.005434\n",
      "nt_epoch =    670  elapsed = 05:57  loss = 6.0701e+00  error = 3.6029e-01  l1 = 0.982471  l2 = 0.005421\n",
      "nt_epoch =    680  elapsed = 05:58  loss = 5.9983e+00  error = 3.5829e-01  l1 = 0.983175  l2 = 0.005410\n",
      "nt_epoch =    690  elapsed = 05:59  loss = 5.9425e+00  error = 3.5947e-01  l1 = 0.983932  l2 = 0.005420\n",
      "nt_epoch =    700  elapsed = 06:01  loss = 5.9032e+00  error = 3.6062e-01  l1 = 0.984022  l2 = 0.005428\n",
      "nt_epoch =    710  elapsed = 06:02  loss = 5.8517e+00  error = 3.5793e-01  l1 = 0.983796  l2 = 0.005410\n",
      "nt_epoch =    720  elapsed = 06:03  loss = 5.7928e+00  error = 3.5653e-01  l1 = 0.982756  l2 = 0.005398\n",
      "nt_epoch =    730  elapsed = 06:05  loss = 5.7451e+00  error = 3.5666e-01  l1 = 0.983773  l2 = 0.005402\n",
      "nt_epoch =    740  elapsed = 06:06  loss = 5.7123e+00  error = 3.5548e-01  l1 = 0.984237  l2 = 0.005396\n",
      "nt_epoch =    750  elapsed = 06:07  loss = 5.6714e+00  error = 3.5304e-01  l1 = 0.984919  l2 = 0.005383\n",
      "nt_epoch =    760  elapsed = 06:09  loss = 5.6443e+00  error = 3.5165e-01  l1 = 0.985439  l2 = 0.005375\n",
      "nt_epoch =    770  elapsed = 06:10  loss = 5.6051e+00  error = 3.5183e-01  l1 = 0.984406  l2 = 0.005373\n",
      "nt_epoch =    780  elapsed = 06:11  loss = 5.5527e+00  error = 3.5305e-01  l1 = 0.984044  l2 = 0.005380\n",
      "nt_epoch =    790  elapsed = 06:13  loss = 5.4675e+00  error = 3.5241e-01  l1 = 0.985885  l2 = 0.005382\n",
      "nt_epoch =    800  elapsed = 06:14  loss = 5.4071e+00  error = 3.5195e-01  l1 = 0.986126  l2 = 0.005379\n",
      "nt_epoch =    810  elapsed = 06:15  loss = 5.3526e+00  error = 3.5094e-01  l1 = 0.985601  l2 = 0.005371\n",
      "nt_epoch =    820  elapsed = 06:17  loss = 5.3145e+00  error = 3.5009e-01  l1 = 0.985680  l2 = 0.005366\n",
      "nt_epoch =    830  elapsed = 06:18  loss = 5.2597e+00  error = 3.4758e-01  l1 = 0.986031  l2 = 0.005351\n",
      "nt_epoch =    840  elapsed = 06:19  loss = 5.2091e+00  error = 3.4696e-01  l1 = 0.985999  l2 = 0.005347\n",
      "nt_epoch =    850  elapsed = 06:21  loss = 5.1786e+00  error = 3.4694e-01  l1 = 0.986050  l2 = 0.005347\n",
      "nt_epoch =    860  elapsed = 06:22  loss = 5.1498e+00  error = 3.4672e-01  l1 = 0.985726  l2 = 0.005345\n",
      "nt_epoch =    870  elapsed = 06:24  loss = 5.0918e+00  error = 3.4517e-01  l1 = 0.985954  l2 = 0.005336\n",
      "nt_epoch =    880  elapsed = 06:25  loss = 5.0468e+00  error = 3.4378e-01  l1 = 0.986157  l2 = 0.005328\n",
      "nt_epoch =    890  elapsed = 06:26  loss = 5.0108e+00  error = 3.4384e-01  l1 = 0.985716  l2 = 0.005327\n",
      "nt_epoch =    900  elapsed = 06:27  loss = 4.9784e+00  error = 3.4376e-01  l1 = 0.984928  l2 = 0.005324\n",
      "nt_epoch =    910  elapsed = 06:29  loss = 4.9482e+00  error = 3.4374e-01  l1 = 0.985866  l2 = 0.005326\n",
      "nt_epoch =    920  elapsed = 06:30  loss = 4.9025e+00  error = 3.4319e-01  l1 = 0.986791  l2 = 0.005326\n",
      "nt_epoch =    930  elapsed = 06:32  loss = 4.8658e+00  error = 3.4324e-01  l1 = 0.986397  l2 = 0.005325\n",
      "nt_epoch =    940  elapsed = 06:33  loss = 4.8386e+00  error = 3.4302e-01  l1 = 0.986088  l2 = 0.005323\n",
      "nt_epoch =    950  elapsed = 06:34  loss = 4.8111e+00  error = 3.4251e-01  l1 = 0.985731  l2 = 0.005318\n",
      "nt_epoch =    960  elapsed = 06:36  loss = 4.7644e+00  error = 3.3981e-01  l1 = 0.986515  l2 = 0.005303\n",
      "nt_epoch =    970  elapsed = 06:37  loss = 4.7259e+00  error = 3.3694e-01  l1 = 0.986005  l2 = 0.005284\n",
      "nt_epoch =    980  elapsed = 06:38  loss = 4.6932e+00  error = 3.3575e-01  l1 = 0.985137  l2 = 0.005273\n",
      "nt_epoch =    990  elapsed = 06:40  loss = 4.6760e+00  error = 3.3404e-01  l1 = 0.984728  l2 = 0.005261\n",
      "nt_epoch =   1000  elapsed = 06:41  loss = 4.6441e+00  error = 3.3314e-01  l1 = 0.984767  l2 = 0.005255\n",
      "nt_epoch =   1010  elapsed = 06:42  loss = 4.5820e+00  error = 3.3063e-01  l1 = 0.986107  l2 = 0.005244\n",
      "nt_epoch =   1020  elapsed = 06:44  loss = 4.5603e+00  error = 3.3027e-01  l1 = 0.986220  l2 = 0.005242\n",
      "nt_epoch =   1030  elapsed = 06:45  loss = 4.5432e+00  error = 3.2956e-01  l1 = 0.985911  l2 = 0.005236\n",
      "nt_epoch =   1040  elapsed = 06:47  loss = 4.5080e+00  error = 3.2904e-01  l1 = 0.985204  l2 = 0.005231\n",
      "nt_epoch =   1050  elapsed = 06:48  loss = 4.4795e+00  error = 3.2738e-01  l1 = 0.985069  l2 = 0.005220\n",
      "nt_epoch =   1060  elapsed = 06:49  loss = 4.4419e+00  error = 3.2722e-01  l1 = 0.985665  l2 = 0.005221\n",
      "nt_epoch =   1070  elapsed = 06:50  loss = 4.4062e+00  error = 3.2576e-01  l1 = 0.986039  l2 = 0.005213\n",
      "nt_epoch =   1080  elapsed = 06:52  loss = 4.3822e+00  error = 3.2461e-01  l1 = 0.985978  l2 = 0.005205\n",
      "nt_epoch =   1090  elapsed = 06:53  loss = 4.3418e+00  error = 3.2492e-01  l1 = 0.985195  l2 = 0.005205\n",
      "nt_epoch =   1100  elapsed = 06:55  loss = 4.3091e+00  error = 3.2312e-01  l1 = 0.984851  l2 = 0.005192\n",
      "nt_epoch =   1110  elapsed = 06:56  loss = 4.2924e+00  error = 3.2249e-01  l1 = 0.984910  l2 = 0.005188\n",
      "nt_epoch =   1120  elapsed = 06:57  loss = 4.2675e+00  error = 3.2164e-01  l1 = 0.984992  l2 = 0.005183\n",
      "nt_epoch =   1130  elapsed = 06:59  loss = 4.2387e+00  error = 3.2134e-01  l1 = 0.984926  l2 = 0.005181\n",
      "nt_epoch =   1140  elapsed = 07:00  loss = 4.2128e+00  error = 3.2073e-01  l1 = 0.984556  l2 = 0.005176\n",
      "nt_epoch =   1150  elapsed = 07:01  loss = 4.1970e+00  error = 3.1988e-01  l1 = 0.984543  l2 = 0.005170\n",
      "nt_epoch =   1160  elapsed = 07:03  loss = 4.1819e+00  error = 3.1890e-01  l1 = 0.984464  l2 = 0.005164\n",
      "nt_epoch =   1170  elapsed = 07:04  loss = 4.1427e+00  error = 3.1899e-01  l1 = 0.984387  l2 = 0.005164\n",
      "nt_epoch =   1180  elapsed = 07:05  loss = 4.1288e+00  error = 3.1796e-01  l1 = 0.984671  l2 = 0.005159\n",
      "nt_epoch =   1190  elapsed = 07:07  loss = 4.1099e+00  error = 3.1783e-01  l1 = 0.984566  l2 = 0.005157\n",
      "nt_epoch =   1200  elapsed = 07:08  loss = 4.0904e+00  error = 3.1758e-01  l1 = 0.984498  l2 = 0.005156\n",
      "nt_epoch =   1210  elapsed = 07:09  loss = 4.0479e+00  error = 3.1662e-01  l1 = 0.983754  l2 = 0.005147\n",
      "nt_epoch =   1220  elapsed = 07:11  loss = 4.0342e+00  error = 3.1515e-01  l1 = 0.983692  l2 = 0.005137\n",
      "nt_epoch =   1230  elapsed = 07:12  loss = 4.0017e+00  error = 3.1453e-01  l1 = 0.983891  l2 = 0.005134\n",
      "nt_epoch =   1240  elapsed = 07:13  loss = 3.9785e+00  error = 3.1385e-01  l1 = 0.984406  l2 = 0.005132\n",
      "nt_epoch =   1250  elapsed = 07:15  loss = 3.9494e+00  error = 3.1383e-01  l1 = 0.985365  l2 = 0.005134\n",
      "nt_epoch =   1260  elapsed = 07:16  loss = 3.9339e+00  error = 3.1282e-01  l1 = 0.985621  l2 = 0.005129\n",
      "nt_epoch =   1270  elapsed = 07:17  loss = 3.9147e+00  error = 3.1203e-01  l1 = 0.985148  l2 = 0.005122\n",
      "nt_epoch =   1280  elapsed = 07:19  loss = 3.8949e+00  error = 3.1110e-01  l1 = 0.984628  l2 = 0.005115\n",
      "nt_epoch =   1290  elapsed = 07:20  loss = 3.8738e+00  error = 3.1025e-01  l1 = 0.984149  l2 = 0.005108\n",
      "nt_epoch =   1300  elapsed = 07:21  loss = 3.8547e+00  error = 3.0877e-01  l1 = 0.983762  l2 = 0.005097\n",
      "nt_epoch =   1310  elapsed = 07:23  loss = 3.8472e+00  error = 3.0838e-01  l1 = 0.983638  l2 = 0.005094\n",
      "nt_epoch =   1320  elapsed = 07:24  loss = 3.8301e+00  error = 3.0762e-01  l1 = 0.983875  l2 = 0.005090\n",
      "nt_epoch =   1330  elapsed = 07:25  loss = 3.8237e+00  error = 3.0663e-01  l1 = 0.984341  l2 = 0.005085\n",
      "nt_epoch =   1340  elapsed = 07:26  loss = 3.8100e+00  error = 3.0650e-01  l1 = 0.985094  l2 = 0.005087\n",
      "nt_epoch =   1350  elapsed = 07:28  loss = 3.7849e+00  error = 3.0650e-01  l1 = 0.985403  l2 = 0.005088\n",
      "nt_epoch =   1360  elapsed = 07:29  loss = 3.7638e+00  error = 3.0654e-01  l1 = 0.984738  l2 = 0.005086\n",
      "nt_epoch =   1370  elapsed = 07:30  loss = 3.7455e+00  error = 3.0673e-01  l1 = 0.983931  l2 = 0.005085\n",
      "nt_epoch =   1380  elapsed = 07:32  loss = 3.7282e+00  error = 3.0656e-01  l1 = 0.983725  l2 = 0.005083\n",
      "nt_epoch =   1390  elapsed = 07:33  loss = 3.7111e+00  error = 3.0640e-01  l1 = 0.983485  l2 = 0.005081\n",
      "nt_epoch =   1400  elapsed = 07:34  loss = 3.6976e+00  error = 3.0611e-01  l1 = 0.983799  l2 = 0.005080\n",
      "nt_epoch =   1410  elapsed = 07:36  loss = 3.6825e+00  error = 3.0489e-01  l1 = 0.984233  l2 = 0.005074\n",
      "nt_epoch =   1420  elapsed = 07:37  loss = 3.6552e+00  error = 3.0446e-01  l1 = 0.984472  l2 = 0.005072\n",
      "nt_epoch =   1430  elapsed = 07:38  loss = 3.6325e+00  error = 3.0367e-01  l1 = 0.984673  l2 = 0.005068\n",
      "nt_epoch =   1440  elapsed = 07:39  loss = 3.6291e+00  error = 3.0356e-01  l1 = 0.984666  l2 = 0.005067\n",
      "nt_epoch =   1450  elapsed = 07:41  loss = 3.6149e+00  error = 3.0345e-01  l1 = 0.984742  l2 = 0.005066\n",
      "nt_epoch =   1460  elapsed = 07:42  loss = 3.6109e+00  error = 3.0292e-01  l1 = 0.984821  l2 = 0.005063\n",
      "nt_epoch =   1470  elapsed = 07:43  loss = 3.5920e+00  error = 3.0190e-01  l1 = 0.985027  l2 = 0.005057\n",
      "nt_epoch =   1480  elapsed = 07:45  loss = 3.5774e+00  error = 3.0120e-01  l1 = 0.985103  l2 = 0.005053\n",
      "nt_epoch =   1490  elapsed = 07:46  loss = 3.5727e+00  error = 3.0029e-01  l1 = 0.985453  l2 = 0.005049\n",
      "nt_epoch =   1500  elapsed = 07:47  loss = 3.5577e+00  error = 2.9902e-01  l1 = 0.985252  l2 = 0.005040\n",
      "nt_epoch =   1510  elapsed = 07:48  loss = 3.5451e+00  error = 2.9843e-01  l1 = 0.985589  l2 = 0.005037\n",
      "nt_epoch =   1520  elapsed = 07:50  loss = 3.5267e+00  error = 2.9666e-01  l1 = 0.985272  l2 = 0.005025\n",
      "nt_epoch =   1530  elapsed = 07:51  loss = 3.5102e+00  error = 2.9617e-01  l1 = 0.985721  l2 = 0.005023\n",
      "nt_epoch =   1540  elapsed = 07:52  loss = 3.4918e+00  error = 2.9550e-01  l1 = 0.985623  l2 = 0.005019\n",
      "nt_epoch =   1550  elapsed = 07:54  loss = 3.4806e+00  error = 2.9453e-01  l1 = 0.985789  l2 = 0.005013\n",
      "nt_epoch =   1560  elapsed = 07:55  loss = 3.4933e+00  error = 2.9359e-01  l1 = 0.985240  l2 = 0.005005\n",
      "nt_epoch =   1570  elapsed = 07:56  loss = 3.4551e+00  error = 2.9172e-01  l1 = 0.985864  l2 = 0.004995\n",
      "nt_epoch =   1580  elapsed = 07:57  loss = 3.4440e+00  error = 2.9104e-01  l1 = 0.985866  l2 = 0.004991\n",
      "nt_epoch =   1590  elapsed = 07:59  loss = 3.4334e+00  error = 2.9019e-01  l1 = 0.986127  l2 = 0.004986\n",
      "nt_epoch =   1600  elapsed = 08:00  loss = 3.4092e+00  error = 2.8969e-01  l1 = 0.985862  l2 = 0.004982\n",
      "nt_epoch =   1610  elapsed = 08:01  loss = 3.4087e+00  error = 2.8809e-01  l1 = 0.985699  l2 = 0.004972\n",
      "nt_epoch =   1620  elapsed = 08:03  loss = 3.3816e+00  error = 2.8782e-01  l1 = 0.985265  l2 = 0.004969\n",
      "nt_epoch =   1630  elapsed = 08:04  loss = 3.3852e+00  error = 2.8701e-01  l1 = 0.985316  l2 = 0.004964\n",
      "nt_epoch =   1640  elapsed = 08:05  loss = 3.3737e+00  error = 2.8620e-01  l1 = 0.985507  l2 = 0.004959\n",
      "nt_epoch =   1650  elapsed = 08:06  loss = 3.3571e+00  error = 2.8524e-01  l1 = 0.985825  l2 = 0.004954\n",
      "nt_epoch =   1660  elapsed = 08:08  loss = 3.3531e+00  error = 2.8386e-01  l1 = 0.985905  l2 = 0.004945\n",
      "nt_epoch =   1670  elapsed = 08:09  loss = 3.3472e+00  error = 2.8407e-01  l1 = 0.985733  l2 = 0.004946\n",
      "nt_epoch =   1680  elapsed = 08:10  loss = 3.3263e+00  error = 2.8296e-01  l1 = 0.985183  l2 = 0.004937\n",
      "nt_epoch =   1690  elapsed = 08:12  loss = 3.3124e+00  error = 2.8312e-01  l1 = 0.984937  l2 = 0.004938\n",
      "nt_epoch =   1700  elapsed = 08:13  loss = 3.3083e+00  error = 2.8236e-01  l1 = 0.984923  l2 = 0.004933\n",
      "nt_epoch =   1710  elapsed = 08:14  loss = 3.3022e+00  error = 2.8198e-01  l1 = 0.985497  l2 = 0.004932\n",
      "nt_epoch =   1720  elapsed = 08:16  loss = 3.2877e+00  error = 2.8120e-01  l1 = 0.985579  l2 = 0.004927\n",
      "nt_epoch =   1730  elapsed = 08:17  loss = 3.2773e+00  error = 2.8096e-01  l1 = 0.985731  l2 = 0.004926\n",
      "nt_epoch =   1740  elapsed = 08:19  loss = 3.2726e+00  error = 2.8039e-01  l1 = 0.985831  l2 = 0.004923\n",
      "nt_epoch =   1750  elapsed = 08:20  loss = 3.2639e+00  error = 2.7976e-01  l1 = 0.985681  l2 = 0.004919\n",
      "nt_epoch =   1760  elapsed = 08:21  loss = 3.2561e+00  error = 2.7910e-01  l1 = 0.985268  l2 = 0.004913\n",
      "nt_epoch =   1770  elapsed = 08:22  loss = 3.2368e+00  error = 2.7793e-01  l1 = 0.984830  l2 = 0.004904\n",
      "nt_epoch =   1780  elapsed = 08:24  loss = 3.2356e+00  error = 2.7783e-01  l1 = 0.984758  l2 = 0.004903\n",
      "nt_epoch =   1790  elapsed = 08:25  loss = 3.2187e+00  error = 2.7665e-01  l1 = 0.984803  l2 = 0.004896\n",
      "nt_epoch =   1800  elapsed = 08:26  loss = 3.2211e+00  error = 2.7633e-01  l1 = 0.984966  l2 = 0.004894\n",
      "nt_epoch =   1810  elapsed = 08:28  loss = 3.2105e+00  error = 2.7568e-01  l1 = 0.985186  l2 = 0.004891\n",
      "nt_epoch =   1820  elapsed = 08:29  loss = 3.1978e+00  error = 2.7456e-01  l1 = 0.985564  l2 = 0.004885\n",
      "nt_epoch =   1830  elapsed = 08:30  loss = 3.1915e+00  error = 2.7328e-01  l1 = 0.985831  l2 = 0.004878\n",
      "nt_epoch =   1840  elapsed = 08:32  loss = 3.1848e+00  error = 2.7325e-01  l1 = 0.985774  l2 = 0.004877\n",
      "nt_epoch =   1850  elapsed = 08:33  loss = 3.1835e+00  error = 2.7254e-01  l1 = 0.985586  l2 = 0.004872\n",
      "nt_epoch =   1860  elapsed = 08:34  loss = 3.1719e+00  error = 2.7235e-01  l1 = 0.985173  l2 = 0.004870\n",
      "nt_epoch =   1870  elapsed = 08:35  loss = 3.1690e+00  error = 2.7134e-01  l1 = 0.984715  l2 = 0.004862\n",
      "nt_epoch =   1880  elapsed = 08:37  loss = 3.1637e+00  error = 2.7111e-01  l1 = 0.984642  l2 = 0.004860\n",
      "nt_epoch =   1890  elapsed = 08:38  loss = 3.1602e+00  error = 2.7038e-01  l1 = 0.984449  l2 = 0.004855\n",
      "nt_epoch =   1900  elapsed = 08:39  loss = 3.1552e+00  error = 2.7050e-01  l1 = 0.984769  l2 = 0.004857\n",
      "nt_epoch =   1910  elapsed = 08:40  loss = 3.1458e+00  error = 2.6954e-01  l1 = 0.984812  l2 = 0.004851\n",
      "nt_epoch =   1920  elapsed = 08:42  loss = 3.1421e+00  error = 2.6914e-01  l1 = 0.985385  l2 = 0.004850\n",
      "nt_epoch =   1930  elapsed = 08:43  loss = 3.1334e+00  error = 2.6838e-01  l1 = 0.985688  l2 = 0.004846\n",
      "nt_epoch =   1940  elapsed = 08:44  loss = 3.1381e+00  error = 2.6845e-01  l1 = 0.985773  l2 = 0.004847\n",
      "nt_epoch =   1950  elapsed = 08:46  loss = 3.1335e+00  error = 2.6739e-01  l1 = 0.986278  l2 = 0.004842\n",
      "nt_epoch =   1960  elapsed = 08:47  loss = 3.1186e+00  error = 2.6812e-01  l1 = 0.985736  l2 = 0.004845\n",
      "nt_epoch =   1970  elapsed = 08:48  loss = 3.1182e+00  error = 2.6813e-01  l1 = 0.985740  l2 = 0.004845\n",
      "nt_epoch =   1980  elapsed = 08:49  loss = 3.1187e+00  error = 2.6770e-01  l1 = 0.985457  l2 = 0.004841\n",
      "nt_epoch =   1990  elapsed = 08:51  loss = 3.1011e+00  error = 2.6767e-01  l1 = 0.985212  l2 = 0.004840\n",
      "==================\n",
      "Training finished (epoch 100): duration = 08:52  error = 2.6721e-01  l1 = 0.985198  l2 = 0.004837\n",
      "l1:  0.9834988\n",
      "l2:  0.0047782687\n",
      "noisy l1:  0.98519814\n",
      "noisy l2:  0.004837123\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "lb = np.array([-1.0])\n",
    "ub = np.array([1.0])\n",
    "idx_t_0 = 10\n",
    "skip = 80\n",
    "idx_t_1 = idx_t_0 + skip\n",
    "\n",
    "# Getting the data\n",
    "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
    "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
    "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
    "  lb=lb, ub=ub, noise=0.0, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "lambdas_star = (1.0, 0.01/np.pi)\n",
    "\n",
    "# Setting the output layer dynamically\n",
    "layers[-1] = q\n",
    " \n",
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
    "def error():\n",
    "  l1, l2 = pinn.get_params(numpy=True)\n",
    "  l1_star, l2_star = lambdas_star\n",
    "  error_lambda_1 = np.abs(l1 - l1_star) / l1_star\n",
    "  error_lambda_2 = np.abs(l2 - l2_star) / l2_star\n",
    "  return (error_lambda_1 + error_lambda_2) / 2\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
    "\n",
    "# Getting the model predictions\n",
    "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
    "lambda_1_pred, lambda_2_pred = pinn.get_params(numpy=True)\n",
    "\n",
    "# Noisy case (same as before with a different noise)\n",
    "x_0, u_0, x_1, u_1, x_star, t_star, dt, q, \\\n",
    "  Exact_u, IRK_alpha, IRK_beta = prep_data(path, N_0=N_0, N_1=N_1,\n",
    "  lb=lb, ub=ub, noise=0.01, idx_t_0=idx_t_0, idx_t_1=idx_t_1)\n",
    "layers[-1] = q\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, dt, lb, ub, q, IRK_alpha, IRK_beta)\n",
    "pinn.fit(x_0, u_0, x_1, u_1, tf_epochs)\n",
    "U_0_pred, U_1_pred = pinn.predict(x_star)\n",
    "lambda_1_pred_noisy, lambda_2_pred_noisy = pinn.get_params(numpy=True)\n",
    "\n",
    "print(\"l1: \", lambda_1_pred)\n",
    "print(\"l2: \", lambda_2_pred)\n",
    "print(\"noisy l1: \", lambda_1_pred_noisy)\n",
    "print(\"noisy l2: \", lambda_2_pred_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79caa23-a49c-4bb1-8a42-0f0fd44fb8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2117/3589832593.py:10: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
      "  ax = plt.subplot(gs0[:, :])\n"
     ]
    }
   ],
   "source": [
    "plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
    "  ub, lb, U_1_pred, Exact_u, lambda_1_pred, lambda_1_pred_noisy, lambda_2_pred, lambda_2_pred_noisy, x_star, t_star, file=\"disc_example2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f8451-d57b-4e23-9595-4cfde0d71700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
