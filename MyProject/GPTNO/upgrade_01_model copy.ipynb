{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.nn.pytorch import SumPooling, AvgPooling\n",
    "\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingWarmRestarts, _LRScheduler\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "from utils import get_seed, get_num_params, plot_heatmap\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sparsemax import Sparsemax\n",
    "from performer_pytorch import FastAttention\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    A simple MLP class, includes at least 2 layers and n hidden layers\n",
    "'''\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act=nn.GELU()):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.n_layers = n_layers\n",
    "        self.linear_pre = nn.Linear(n_input, n_hidden)\n",
    "        self.linear_post = nn.Linear(n_hidden, n_output)\n",
    "        self.linears = nn.ModuleList([nn.Linear(n_hidden, n_hidden) for _ in range(n_layers)])\n",
    "        self.act = act\n",
    "        # self.bns = nn.ModuleList([nn.BatchNorm1d(n_hidden) for _ in range(n_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.linear_pre(x))\n",
    "        for i in range(self.n_layers):\n",
    "            x = x + self.act(self.linears[i](x))\n",
    "            # x = x + self.act(self.bns[i](self.linears[i](x)))\n",
    "\n",
    "        x = self.linear_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig():\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    def __init__(self,\n",
    "                 attn_type      ='linear',\n",
    "                 embd_pdrop     =0.0,\n",
    "                 resid_pdrop    =0.0,\n",
    "                 attn_pdrop     =0.0,\n",
    "                 n_embd         =128,\n",
    "                 n_head         =2,\n",
    "                 n_layer        =3,\n",
    "                 n_inner        =4,\n",
    "                 act            =nn.GELU(),\n",
    "                 branch_sizes   =1,\n",
    "                 n_inputs       =1,\n",
    "                 n_experts      =2,\n",
    "                 space_dim      =1,\n",
    "                 ):\n",
    "        \n",
    "        self.attn_type      = attn_type  \n",
    "        self.embd_pdrop     = embd_pdrop\n",
    "        self.resid_pdrop    = resid_pdrop\n",
    "        self.attn_pdrop     = attn_pdrop\n",
    "        self.n_embd         = n_embd\n",
    "        self.n_head         = n_head\n",
    "        self.n_layer        = n_layer\n",
    "        self.n_inner        = n_inner * self.n_embd\n",
    "        self.act            = act\n",
    "        self.n_experts      = n_experts\n",
    "        self.branch_sizes   = branch_sizes\n",
    "        self.n_inputs       = n_inputs\n",
    "        self.space_dim      = space_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### x: list of tensors\n",
    "class MultipleTensors():\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def to(self, device):\n",
    "        self.x = [x_.to(device) for x_ in self.x]\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data Shape: torch.Size([2, 6, 4, 5])\n",
      "Cosine Similarity between the two samples in the batch: tensor([[1.1432e-01, 2.0537e-01, 1.9943e-01, 7.2265e-01, 6.0720e-01],\n",
      "        [7.9586e-04, 9.6508e-01, 3.8921e-01, 3.7827e-01, 5.8848e-03],\n",
      "        [5.3685e-01, 4.9554e-01, 1.6798e-02, 4.4424e-01, 3.3485e-01],\n",
      "        [2.6637e-01, 6.1263e-02, 8.9437e-01, 6.3563e-02, 9.2302e-01]])\n"
     ]
    }
   ],
   "source": [
    "def generalized_kernel(data, *, projection_matrix, kernel_fn = nn.ReLU(), kernel_epsilon = 0.001, normalize_data = True, device = None):\n",
    "    b, h, *_ = data.shape\n",
    "\n",
    "    data_normalizer = (data.shape[-1] ** -0.25) if normalize_data else 1.\n",
    "\n",
    "    if projection_matrix is None:\n",
    "        return kernel_fn(data_normalizer * data) + kernel_epsilon\n",
    "\n",
    "    projection = repeat(projection_matrix, 'j d -> b h j d', b = b, h = h)\n",
    "    projection = projection.type_as(data)\n",
    "\n",
    "    data_dash = torch.einsum('...id,...jd->...ij', (data_normalizer * data), projection)\n",
    "\n",
    "    data_prime = kernel_fn(data_dash) + kernel_epsilon\n",
    "    return data_prime.type_as(data)\n",
    "\n",
    "### 사용예시 ###\n",
    "\n",
    "# 데이터셋 생성\n",
    "data = torch.randn(2, 6, 4, 5)  # 배치 크기: 2, 특징 차원: 1, 나머지 차원: (4, 5)\n",
    "\n",
    "# 프로젝션 매트릭스 생성 (임의의 고차원 투영)\n",
    "projection_matrix = torch.randn(5, 5)\n",
    "\n",
    "# 함수 호출\n",
    "result = generalized_kernel(data, projection_matrix=projection_matrix)\n",
    "print(\"Transformed Data Shape:\", result.shape)\n",
    "\n",
    "# 유사성 측정\n",
    "similarity = F.cosine_similarity(result[0], result[1], dim=0)\n",
    "print(\"Cosine Similarity between the two samples in the batch:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthogonal Matrix:\n",
      "tensor([[-0.4214,  0.0138,  0.0593,  0.4377,  0.7919],\n",
      "        [-0.0737, -0.9135, -0.3893,  0.0835, -0.0404],\n",
      "        [ 0.6683,  0.2133, -0.5544,  0.4170,  0.1629],\n",
      "        [-0.4699,  0.3138, -0.7307, -0.3830,  0.0109],\n",
      "        [ 0.3866, -0.1464,  0.0607, -0.6934,  0.5870]])\n",
      "Identity Matrix Approximation:\n",
      "tensor([[ 1.0000e+00,  1.2673e-08, -9.2740e-08,  4.0851e-08, -8.4541e-08],\n",
      "        [ 1.2673e-08,  1.0000e+00,  1.7955e-08,  2.8630e-08, -1.0657e-08],\n",
      "        [-9.2740e-08,  1.7955e-08,  1.0000e+00, -5.9889e-08,  4.1029e-08],\n",
      "        [ 4.0851e-08,  2.8630e-08, -5.9889e-08,  1.0000e+00, -8.9457e-08],\n",
      "        [-8.4541e-08, -1.0657e-08,  4.1029e-08, -8.9457e-08,  1.0000e+00]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enomazosii\\AppData\\Local\\Temp\\ipykernel_19320\\2552538928.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  TORCH_GE_1_8_0 = LooseVersion(torch.__version__) >= LooseVersion('1.8.0')\n"
     ]
    }
   ],
   "source": [
    "def orthogonal_matrix_chunk(cols, device = None):\n",
    "    from distutils.version import LooseVersion\n",
    "    TORCH_GE_1_8_0 = LooseVersion(torch.__version__) >= LooseVersion('1.8.0')\n",
    "    unstructured_block = torch.randn((cols, cols), device = device)\n",
    "    if TORCH_GE_1_8_0:\n",
    "        q, r = torch.linalg.qr(unstructured_block.cpu(), mode = 'reduced')\n",
    "    else:\n",
    "        q, r = torch.qr(unstructured_block.cpu(), some = True)\n",
    "    q, r = map(lambda t: t.to(device), (q, r))\n",
    "    return q.t()\n",
    "\n",
    "orthogonal_matrix = orthogonal_matrix_chunk(5, device='cpu')\n",
    "print(\"Orthogonal Matrix:\")\n",
    "print(orthogonal_matrix)\n",
    "\n",
    "# 생성된 직교 행렬의 특성 검증\n",
    "# 직교 행렬과 그 전치의 곱은 단위 행렬(identity matrix)이어야 합니다.\n",
    "identity_approx = torch.mm(orthogonal_matrix, orthogonal_matrix.t())\n",
    "\n",
    "print(\"Identity Matrix Approximation:\")\n",
    "print(identity_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    X: N*T*C --> N*(4*n + 3)*C \n",
    "'''\n",
    "def horizontal_fourier_embedding(X, n=3):\n",
    "    freqs = 2**torch.linspace(-n, n, 2*n+1).to(X.device)\n",
    "    freqs = freqs[None,None,None,...]\n",
    "    X_ = X.unsqueeze(-1).repeat([1,1,1,2*n+1])\n",
    "    X_cos = torch.cos(freqs * X_)\n",
    "    X_sin = torch.sin(freqs * X_)\n",
    "    X = torch.cat([X.unsqueeze(-1), X_cos, X_sin],dim=-1).view(X.shape[0],X.shape[1],-1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.fast_attention = FastAttention(dim_heads=config.n_embd // config.n_head, nb_features=256, causal=False)\n",
    "    '''\n",
    "        Linear Attention and Linear Cross Attention (if y is provided)\n",
    "    '''\n",
    "    def forward(self, x, y=None, layer_past=None):\n",
    "        y = x if y is None else y\n",
    "        B, T1, C = x.size()\n",
    "        _, T2, _ = y.size()\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        k = self.key(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = self.value(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        y = self.fast_attention(q, k, v)\n",
    "        y = rearrange(y, 'b h n d -> b n (h d)')\n",
    "\n",
    "        # Output projection\n",
    "        y = self.proj(y)\n",
    "        return x + y\n",
    "    \n",
    "class LinearCrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LinearCrossAttention, self).__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.keys = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n",
    "        self.values = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_inputs = config.n_inputs\n",
    "        self.fast_attention = FastAttention(dim_heads=config.n_embd // config.n_head, nb_features=256, causal=False)\n",
    "        \n",
    "\n",
    "    '''\n",
    "        Linear Attention and Linear Cross Attention (if y is provided)\n",
    "    '''\n",
    "    def forward(self, x, ys=None, layer_past=None):\n",
    "        if ys is None:\n",
    "            ys = [x] * self.n_inputs\n",
    "        B, T1, C = x.size()\n",
    "        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # Using FastAttention for each pair of keys and values\n",
    "        out = torch.zeros(B, self.n_head, T1, C // self.n_head, device=x.device)\n",
    "        for i, y in enumerate(ys):\n",
    "            _, T2, _ = y.size()\n",
    "            k = self.keys[i](y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "            v = self.values[i](y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "            # Apply FastAttention\n",
    "            out += self.fast_attention(q, k, v)\n",
    "\n",
    "        # Apply dropout to the combined attention output before projection\n",
    "        out = self.attn_drop(out)\n",
    "\n",
    "        # Output projection\n",
    "        projected_output = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        projected_output = self.proj(projected_output)\n",
    "\n",
    "        # Residual connection\n",
    "        output = x + projected_output  # Add original input x to the output of the projection\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Self and Cross Attention block for CGPT, contains  a cross attention block and a self attention block\n",
    "'''\n",
    "class MIOECrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MIOECrossAttentionBlock, self).__init__()\n",
    "        self.ln1            = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2_branch     = nn.ModuleList([\n",
    "            nn.LayerNorm(config.n_embd) for _ in range(config.n_inputs)\n",
    "            ])\n",
    "        self.ln3            = nn.LayerNorm(config.n_embd)\n",
    "        self.ln4            = nn.LayerNorm(config.n_embd)\n",
    "        self.ln5            = nn.LayerNorm(config.n_embd)\n",
    "        \n",
    "        self.selfattn       = LinearAttention(config)\n",
    "        self.crossattn      = LinearCrossAttention(config)\n",
    "        # self.selfattn_branch= LinearAttention(config)\n",
    "        \n",
    "        self.resid_drop1    = nn.Dropout(config.resid_pdrop)\n",
    "        self.resid_drop2    = nn.Dropout(config.resid_pdrop)\n",
    "        self.n_experts      = config.n_experts\n",
    "        self.n_inputs       = config.n_inputs\n",
    "        self.act            = config.act\n",
    "        self.sparsemax      = Sparsemax(dim=-1)\n",
    "        \n",
    "        self.moe_mlp1 = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_inner),\n",
    "            self.act,\n",
    "            nn.Linear(config.n_inner, config.n_embd),\n",
    "        ) for _ in range(self.n_experts)])\n",
    "\n",
    "        self.moe_mlp2 = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_inner),\n",
    "            self.act,\n",
    "            nn.Linear(config.n_inner, config.n_embd),\n",
    "        ) for _ in range(self.n_experts)])\n",
    "\n",
    "        self.gatenet = nn.Sequential(\n",
    "            nn.Linear(config.space_dim, config.n_inner),\n",
    "            self.act,\n",
    "            nn.Linear(config.n_inner, config.n_inner),\n",
    "            self.act,\n",
    "            nn.Linear(config.n_inner, self.n_experts)\n",
    "        )\n",
    "\n",
    "    def ln_branchs(self, y):\n",
    "        return MultipleTensors([self.ln2_branch[i](y[i]) for i in range(self.n_inputs)])\n",
    "    '''\n",
    "        x: [B, T1, C], y:[B, T2, C], pos:[B, T1, n]\n",
    "    '''\n",
    "    def forward(self, x, y, pos):\n",
    "        gate_score = F.softmax(self.gatenet(pos), dim=-1).unsqueeze(2)    # B, T1, 1, m\n",
    "        x = x + self.resid_drop1(self.crossattn(self.ln1(x), self.ln_branchs(y)))\n",
    "        x_moe1 = torch.stack([self.moe_mlp1[i](x) for i in range(self.n_experts)],dim=-1) # B, T1, C, m\n",
    "        x_moe1 = (gate_score*x_moe1).sum(dim=-1,keepdim=False)\n",
    "        x = x + self.ln3(x_moe1)\n",
    "        x = x + self.resid_drop2(self.selfattn(self.ln4(x)))\n",
    "        x_moe2 = torch.stack([self.moe_mlp2[i](x) for i in range(self.n_experts)],dim=-1) # B, T1, C, m\n",
    "        x_moe2 = (gate_score*x_moe2).sum(dim=-1,keepdim=False)\n",
    "        x = x + self.ln5(x_moe2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNOT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 trunk_size         = 2,\n",
    "                 branch_sizes       = None,\n",
    "                 space_dim          = 2,\n",
    "                 output_size        = 3,\n",
    "                 n_layers           = 2,\n",
    "                 n_hidden           = 64,\n",
    "                 n_head             = 1,\n",
    "                 n_experts          = 2,\n",
    "                 n_inner            = 4,\n",
    "                 mlp_layers         = 2,\n",
    "                 attn_type          = 'linear',\n",
    "                 act                = 'gelu',\n",
    "                 ffn_dropout        = 0.0,\n",
    "                 attn_dropout       = 0.0,\n",
    "                 horiz_fourier_dim  = 0,\n",
    "                 ):\n",
    "        super(GNOT, self).__init__()\n",
    "        \n",
    "        self.__name__ = 'GNOT'\n",
    "        self.gpt_config = GPTConfig(\n",
    "            attn_type   = attn_type,\n",
    "            embd_pdrop  = ffn_dropout,\n",
    "            resid_pdrop = ffn_dropout,\n",
    "            attn_pdrop  = attn_dropout,\n",
    "            n_embd      = n_hidden,\n",
    "            n_head      = n_head,\n",
    "            n_layer     = n_layers,\n",
    "            act         = act,\n",
    "            n_experts   = n_experts,\n",
    "            space_dim   = space_dim, \n",
    "            branch_sizes= branch_sizes,\n",
    "            n_inputs    = len(branch_sizes),\n",
    "            n_inner     = n_inner\n",
    "            )\n",
    "\n",
    "        self.horiz_fourier_dim  = horiz_fourier_dim\n",
    "        self.trunk_size         = trunk_size * (4*horiz_fourier_dim + 3) if horiz_fourier_dim>0 else trunk_size\n",
    "        self.branch_sizes       = [bsize * (4*horiz_fourier_dim + 3) for bsize in branch_sizes] if horiz_fourier_dim > 0 else branch_sizes\n",
    "        self.n_inputs           = len(self.branch_sizes)\n",
    "        self.output_size        = output_size\n",
    "        self.space_dim          = space_dim\n",
    "\n",
    "        self.trunk_mlp      = MLP(self.trunk_size, n_hidden, n_hidden, n_layers=mlp_layers,act=act)\n",
    "        self.branch_mlps    = nn.ModuleList([MLP(bsize, n_hidden, n_hidden, n_layers=mlp_layers,act=act) for bsize in self.branch_sizes])\n",
    "        self.blocks         = nn.Sequential(*[MIOECrossAttentionBlock(self.gpt_config) for _ in range(self.gpt_config.n_layer)])\n",
    "        self.out_mlp        = MLP(n_hidden, n_hidden, output_size, n_layers=mlp_layers)\n",
    "\n",
    "        # self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.0002)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, g, u_p, inputs):\n",
    "        gs = dgl.unbatch(g)\n",
    "        x = pad_sequence([_g.ndata['x'] for _g in gs]).permute(1, 0, 2)  # B, T1, F\n",
    "\n",
    "        pos = x[:,:,0:self.space_dim]\n",
    "\n",
    "\n",
    "        x = torch.cat([x, u_p.unsqueeze(1).repeat([1, x.shape[1], 1])], dim=-1)\n",
    "\n",
    "        # if self.horiz_fourier_dim > 0:\n",
    "        #     x = horizontal_fourier_embedding(x, self.horiz_fourier_dim)\n",
    "        #     z = horizontal_fourier_embedding(z, self.horiz_fourier_dim)\n",
    "\n",
    "        x = self.trunk_mlp(x)\n",
    "        z = MultipleTensors([self.branch_mlps[i](inputs[i]) for i in range(self.n_inputs)])\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, z, pos)\n",
    "        x = self.out_mlp(x)\n",
    "\n",
    "        x_out = torch.cat([x[i, :num] for i, num in enumerate(g.batch_num_nodes())],dim=0)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Simple normalization layer\n",
    "'''\n",
    "class UnitTransformer():\n",
    "    def __init__(self, X):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, keepdim=True) + 1e-8\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        self.mean = self.mean.to(device)\n",
    "        self.std = self.std.to(device)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, inverse=True,component='all'):\n",
    "        if component == 'all' or 'all-reduce':\n",
    "            if inverse:\n",
    "                orig_shape = X.shape\n",
    "                return (X*(self.std - 1e-8) + self.mean).view(orig_shape)\n",
    "            else:\n",
    "                return (X-self.mean)/self.std\n",
    "        else:\n",
    "            if inverse:\n",
    "                orig_shape = X.shape\n",
    "                return (X*(self.std[:,component] - 1e-8)+ self.mean[:,component]).view(orig_shape)\n",
    "            else:\n",
    "                return (X - self.mean[:,component])/self.std[:,component]\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "    Dataset format:\n",
    "    [X, Y, theta, (f1, f2, ...)], input functions could be None\n",
    "'''\n",
    "class MIODataset(DGLDataset):\n",
    "    def __init__(self, data_path, name=' ', train=True, test=False, train_num=None, test_num=None,normalize_y=\"unit\", y_normalizer=None, x_normalizer=None, up_normalizer=None, normalize_x=\"unit\",sort_data=False):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.cached_path = self.data_path[:-4] + '_' + 'train' + '_cached' +self.data_path[-4:] if train else  self.data_path[:-4] + '_' + 'test' + '_cached' +self.data_path[-4:]\n",
    "        self.normalize_y = normalize_y\n",
    "        self.normalize_x = normalize_x\n",
    "        self.y_normalizer = y_normalizer\n",
    "        self.x_normalizer = x_normalizer\n",
    "        self.up_normalizer = up_normalizer\n",
    "        self.sort_data = sort_data\n",
    "        self.num_inputs = 0\n",
    "\n",
    "        ####  debug timing\n",
    "        time0 = time.time()\n",
    "        if not os.path.exists(self.cached_path):\n",
    "            data_all = pickle.load(open(self.data_path, \"rb\"))\n",
    "            print('{} second, Load dataset finished'.format(time.time()-time0))\n",
    "            #### initialize dataset\n",
    "            self.train = train\n",
    "            if ((train_num == 'none') and (test_num == 'none')):\n",
    "                self.train_num = int(0.8 * len(data_all))\n",
    "                self.test_num = len(data_all) - self.train_num\n",
    "            else:\n",
    "                self.train_num = train_num\n",
    "                self.test_num = test_num\n",
    "\n",
    "            if self.train:\n",
    "                if train_num == 'all':   # use all to train\n",
    "                    self.train_num = len(data_all)\n",
    "                else:\n",
    "                    train_num = int(train_num)\n",
    "                    self.train_num = min(train_num, len(data_all))\n",
    "                    if train_num > len(data_all):\n",
    "                        print('Warnings: there is no enough train data {} / {}'.format(train_num, len(data_all)))\n",
    "                self.data_list = data_all[:self.train_num]\n",
    "                print('Training with {} samples'.format(self.train_num))\n",
    "\n",
    "            else:\n",
    "                if test_num == \"all\":\n",
    "                    self.test_num = len(data_all)\n",
    "                else:\n",
    "                    test_num = int(test_num)\n",
    "                    self.test_num = min(test_num, len(data_all))\n",
    "                    if test_num > len(data_all):\n",
    "                        print('Warnings: there is no enough test data {} / {}'.format(test_num, len(data_all)))\n",
    "\n",
    "                self.data_list = data_all[-self.test_num:]\n",
    "                print('Testing with {} samples'.format(self.test_num))\n",
    "\n",
    "        super(MIODataset, self).__init__(name)   #### invoke super method after read data\n",
    "\n",
    "        # self.__initialize_tensor_dataset()\n",
    "\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "\n",
    "        self.data_len = len(self.data_list)\n",
    "        self.graphs = []\n",
    "        self.inputs_f = []\n",
    "        self.u_p = []\n",
    "        for i in range(len(self)):\n",
    "            x, y, u_p, input_f = self.data_list[i]\n",
    "            g = dgl.DGLGraph()\n",
    "            g.add_nodes(x.shape[0])\n",
    "            g.ndata['x'] = torch.from_numpy(x).float()\n",
    "            g.ndata['y'] = torch.from_numpy(y).float()\n",
    "            up = torch.from_numpy(u_p).float()\n",
    "            self.graphs.append(g)\n",
    "            self.u_p.append(up) # global input parameters\n",
    "            if input_f is not None:\n",
    "                input_f = MultipleTensors([torch.from_numpy(f).float() for f in input_f])\n",
    "                self.inputs_f.append(input_f)\n",
    "                self.num_inputs = len(input_f)\n",
    "\n",
    "        if len(self.inputs_f) == 0:\n",
    "            self.inputs_f = torch.zeros([len(self)])  # pad values, tensor of 0, not list\n",
    "\n",
    "            # print('processing {}'.format(i))d\n",
    "\n",
    "        #### sort data if necessary\n",
    "        if self.sort_data:\n",
    "            self.__sort_dataset()\n",
    "\n",
    "        self.u_p = torch.stack(self.u_p)\n",
    "\n",
    "\n",
    "        #### normalize_y\n",
    "        if self.normalize_y != 'none':\n",
    "            self.__normalize_y()\n",
    "        if self.normalize_x != 'none':\n",
    "            self.__normalize_x()\n",
    "\n",
    "        self.__update_dataset_config()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __sort_dataset(self):\n",
    "        zipped_lists = list(zip(self.graphs, self.u_p, self.inputs_f))\n",
    "        sorted_lists = sorted(zipped_lists, key=lambda x: x[0].number_of_nodes(),reverse=True)\n",
    "\n",
    "        self.graphs, self.u_p, self.inputs_f = zip(*sorted_lists)\n",
    "        self.graphs, self.inputs_f = list(self.graphs), list(self.inputs_f)\n",
    "\n",
    "        print('Dataset sorted by number of nodes')\n",
    "        return\n",
    "\n",
    "\n",
    "    def __normalize_y(self):\n",
    "        if self.y_normalizer is None:\n",
    "            y_feats_all = torch.cat([g.ndata['y'] for g in self.graphs],dim=0)\n",
    "            if self.normalize_y == 'unit':\n",
    "                self.y_normalizer = UnitTransformer(y_feats_all)\n",
    "                print('Target features are normalized using unit transformer')\n",
    "                print(self.y_normalizer.mean, self.y_normalizer.std)\n",
    "\n",
    "\n",
    "            # elif self.normalize_y == 'minmax':\n",
    "            #     self.y_normalizer = MinMaxTransformer(y_feats_all)\n",
    "            #     print('Target features are normalized using unit transformer')\n",
    "            #     print(self.y_normalizer.max, self.y_normalizer.min)\n",
    "\n",
    "            # elif self.normalize_y == 'quantile':\n",
    "            #     self.y_normalizer = QuantileTransformer(output_distribution='normal')\n",
    "            #     self.y_normalizer = self.y_normalizer.fit(y_feats_all)\n",
    "            #     self.y_normalizer = TorchQuantileTransformer(self.y_normalizer.output_distribution, self.y_normalizer.references_,self.y_normalizer.quantiles_)\n",
    "            #     print('Target features are normalized using quantile transformer')\n",
    "\n",
    "\n",
    "        for g in self.graphs:\n",
    "            g.ndata['y'] = self.y_normalizer.transform(g.ndata[\"y\"], inverse=False)  # a torch quantile transformer\n",
    "\n",
    "        # print('Target features are normalized using quantile transformer')\n",
    "        print('Target features are normalized using unit transformer')\n",
    "\n",
    "\n",
    "    def __normalize_x(self):\n",
    "        if self.x_normalizer is None:\n",
    "            x_feats_all = torch.cat([g.ndata[\"x\"] for g in self.graphs],dim=0)\n",
    "            if self.normalize_x == 'unit':\n",
    "                self.x_normalizer = UnitTransformer(x_feats_all)\n",
    "                self.up_normalizer = UnitTransformer(self.u_p)\n",
    "\n",
    "            # elif self.normalize_x == 'minmax':\n",
    "            #     self.x_normalizer = MinMaxTransformer(x_feats_all)\n",
    "            #     self.up_normalizer = MinMaxTransformer(self.u_p)\n",
    "\n",
    "            # else:\n",
    "            #     raise NotImplementedError\n",
    "\n",
    "\n",
    "        for g in self.graphs:\n",
    "            g.ndata['x'] = self.x_normalizer.transform(g.ndata['x'], inverse=False)\n",
    "        self.u_p = self.up_normalizer.transform(self.u_p, inverse=False)\n",
    "\n",
    "\n",
    "        print('Input features are normalized using unit transformer')\n",
    "\n",
    "\n",
    "    def __update_dataset_config(self):\n",
    "        self.config = {\n",
    "            'input_dim': self.graphs[0].ndata['x'].shape[1],\n",
    "            'theta_dim': self.u_p.shape[1],\n",
    "            'output_dim': self.graphs[0].ndata['y'].shape[1],\n",
    "            'branch_sizes': [x.shape[1] for x in self.inputs_f[0]] if isinstance(self.inputs_f, list) else 0\n",
    "        }\n",
    "        return \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.u_p[idx], self.inputs_f[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MIODataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,sort_data=True, shuffle=False, sampler=None,\n",
    "                 batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                 pin_memory=False, drop_last=False, timeout=0,\n",
    "                 worker_init_fn=None):\n",
    "        super(MIODataLoader, self).__init__(dataset=dataset, batch_size=batch_size,\n",
    "                                           shuffle=shuffle, sampler=sampler,\n",
    "                                           batch_sampler=batch_sampler,\n",
    "                                           num_workers=num_workers,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           pin_memory=pin_memory,\n",
    "                                           drop_last=drop_last, timeout=timeout,\n",
    "                                           worker_init_fn=worker_init_fn)\n",
    "\n",
    "        self.sort_data = sort_data\n",
    "        if sort_data:\n",
    "            self.batch_indices = [list(range(i, min(i+batch_size, len(dataset)))) for i in range(0, len(dataset), batch_size)]\n",
    "            if drop_last:\n",
    "                self.batch_indices = self.batch_indices[:-1]\n",
    "        else:\n",
    "            self.batch_indices = list(range(0, (len(dataset) // batch_size)*batch_size)) if drop_last else list(range(0, len(dataset)))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.batch_indices)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for indices in self.batch_indices:\n",
    "            transposed = zip(*[self.dataset[idx] for idx in indices])\n",
    "            batched = []\n",
    "            for sample in transposed:\n",
    "                if isinstance(sample[0], dgl.DGLGraph):\n",
    "                    batched.append(dgl.batch(list(sample)))\n",
    "                elif isinstance(sample[0], torch.Tensor):\n",
    "                    batched.append(torch.stack(sample))\n",
    "                elif isinstance(sample[0], MultipleTensors):\n",
    "                    sample_ = MultipleTensors(\n",
    "                        [pad_sequence([sample[i][j] for i in range(len(sample))]).permute(1, 0, 2) for j in range(len(sample[0]))])\n",
    "                    batched.append(sample_)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            yield batched\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLpRelLoss(_WeightedLoss):\n",
    "    def __init__(self, d=2, p=2, component=0,regularizer=False, normalizer=None):\n",
    "        super(WeightedLpRelLoss, self).__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.component = component if component == 'all' or 'all-reduce' else int(component)\n",
    "        self.regularizer = regularizer\n",
    "        self.normalizer = normalizer\n",
    "        self.sum_pool = SumPooling()\n",
    "\n",
    "    ### all reduce is used in temporal cases, use only one metric for all components\n",
    "    def _lp_losses(self, g, pred, target):\n",
    "        if (self.component == 'all') or (self.component == 'all-reduce'):\n",
    "            err_pool = (self.sum_pool(g, (pred - target).abs() ** self.p))\n",
    "            target_pool = (self.sum_pool(g, target.abs() ** self.p))\n",
    "            losses = (err_pool / target_pool)**(1/ self.p)\n",
    "            if self.component == 'all':\n",
    "                metrics = losses.mean(dim=0).clone().detach().cpu().numpy()\n",
    "            else:\n",
    "                metrics = losses.mean().clone().detach().cpu().numpy()\n",
    "\n",
    "        else:\n",
    "            assert self.component <= target.shape[1]\n",
    "            err_pool = (self.sum_pool(g, (pred - target[:,self.component]).abs() ** self.p))\n",
    "            target_pool = (self.sum_pool(g, target[:,self.component].abs() ** self.p))\n",
    "            losses = (err_pool / target_pool)**(1/ self.p)\n",
    "            metrics = losses.mean().clone().detach().cpu().numpy()\n",
    "\n",
    "        loss = losses.mean()\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def forward(self, g,  pred, target):\n",
    "\n",
    "        #### only for computing metrics\n",
    "\n",
    "\n",
    "        loss, metrics = self._lp_losses(g, pred, target)\n",
    "\n",
    "        if self.normalizer is not None:\n",
    "            ori_pred, ori_target = self.normalizer.transform(pred,component=self.component,inverse=True), self.normalizer.transform(target, inverse=True)\n",
    "            _, metrics = self._lp_losses(g, ori_pred, ori_target)\n",
    "\n",
    "        if self.regularizer:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            reg = torch.zeros_like(loss)\n",
    "\n",
    "\n",
    "        return loss, reg, metrics\n",
    "\n",
    "\n",
    "class WeightedLpLoss(_WeightedLoss):\n",
    "    def __init__(self, d=2, p=2, component=0, regularizer=False, normalizer=None):\n",
    "        super(WeightedLpLoss, self).__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.component = component if component == 'all' else int(component)\n",
    "        self.regularizer = regularizer\n",
    "        self.normalizer = normalizer\n",
    "        self.avg_pool = AvgPooling()\n",
    "\n",
    "    def _lp_losses(self, g, pred, target):\n",
    "        if self.component == 'all':\n",
    "            losses = self.avg_pool(g, ((pred - target).abs() ** self.p)) ** (1 / self.p)\n",
    "            metrics = losses.mean(dim=0).clone().detach().cpu().numpy()\n",
    "\n",
    "        else:\n",
    "            assert self.component <= target.shape[1]\n",
    "            losses = self.avg_pool(g, (pred - target[:, self.component]).abs() ** self.p) ** (1 / self.p)\n",
    "            metrics = losses.mean().clone().detach().cpu().numpy()\n",
    "\n",
    "        loss = losses.mean()\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def forward(self, g, pred, target):\n",
    "\n",
    "        #### only for computing metrics\n",
    "\n",
    "        loss, metrics = self._lp_losses(g, pred, target)\n",
    "\n",
    "        if self.normalizer is not None:\n",
    "            ori_pred, ori_target = self.normalizer.transform(pred,component=self.component, inverse=True), self.normalizer.transform(target, inverse=True)\n",
    "            _, metrics = self._lp_losses(g, ori_pred, ori_target)\n",
    "\n",
    "        if self.regularizer:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            reg = torch.zeros_like(loss)\n",
    "            \n",
    "        return loss, reg, metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "\n",
      "\n",
      "The following code snippets have been run.\n",
      "==================================================\n",
      "\n",
      "    os.environ['PYTHONHASHSEED'] = str(1010)\n",
      "    numpy.random.seed(1010)\n",
      "    torch.manual_seed(1010)\n",
      "    torch.cuda.manual_seed(1010)\n",
      "    torch.backends.cudnn.deterministic = True\n",
      "    torch.backends.cudnn.benchmark = False\n",
      "    if torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed_all(1010)\n",
      "    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print('Using device:', device)\n",
    "get_seed(1010, printout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/heat2d_1100_train.pkl\"\n",
    "test_path = \"./data/heat2d_1100_test.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03583884239196777 second, Load dataset finished\n",
      "Training with 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\enomazosii\\miniconda3\\envs\\graph\\Lib\\site-packages\\dgl\\heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target features are normalized using unit transformer\n",
      "tensor([[0.0184]]) tensor([[0.9740]])\n",
      "Target features are normalized using unit transformer\n",
      "Input features are normalized using unit transformer\n",
      "0.0036592483520507812 second, Load dataset finished\n",
      "Testing with 100 samples\n",
      "Target features are normalized using unit transformer\n",
      "Input features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MIODataset(train_path,\n",
    "                           name=\"heat2d\",\n",
    "                           train=True,\n",
    "                           train_num=\"all\",\n",
    "                           sort_data=False,\n",
    "                           normalize_y=\"unit\",\n",
    "                           normalize_x=\"unit\")\n",
    "test_dataset = MIODataset(test_path,\n",
    "                          name=\"heat2d\",\n",
    "                          train=False,\n",
    "                          test_num=\"all\",\n",
    "                          sort_data=False,\n",
    "                          normalize_y=\"unit\",\n",
    "                          normalize_x=\"unit\",\n",
    "                          y_normalizer=train_dataset.y_normalizer,\n",
    "                          x_normalizer=train_dataset.x_normalizer,\n",
    "                          up_normalizer=train_dataset.up_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = MIODataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=False)\n",
    "test_loader = MIODataLoader(test_dataset, batch_size=16, shuffle=False, drop_last=False)\n",
    "normalizer = train_dataset.y_normalizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The following code snippets have been run.\n",
      "==================================================\n",
      "\n",
      "    os.environ['PYTHONHASHSEED'] = str(1010)\n",
      "    numpy.random.seed(1010)\n",
      "    torch.manual_seed(1010)\n",
      "    torch.cuda.manual_seed(1010)\n",
      "    torch.backends.cudnn.deterministic = True\n",
      "    torch.backends.cudnn.benchmark = False\n",
      "    if torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed_all(1010)\n",
      "    \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_seed(1010)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = WeightedLpRelLoss(p=2,component='all', regularizer=False, normalizer=normalizer)\n",
    "metric_func = WeightedLpRelLoss(p=2,component='all', regularizer=False, normalizer=normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: GNOT\t Number of params: 2848900\n"
     ]
    }
   ],
   "source": [
    "trunk_size = train_dataset.config['input_dim']\n",
    "theta_size = train_dataset.config['theta_dim']\n",
    "branch_sizes = train_dataset.config['branch_sizes']\n",
    "output_size = train_dataset.config['output_dim']\n",
    "\n",
    "\n",
    "model = GNOT(\n",
    "    trunk_size =trunk_size + theta_size,\n",
    "    branch_sizes=branch_sizes, \n",
    "    output_size=output_size,\n",
    "    n_layers= 3,\n",
    "    n_hidden= 128, \n",
    "    n_head=2,\n",
    "    attn_type=\"l1\",\n",
    "    ffn_dropout=0.1,\n",
    "    attn_dropout=0.1, \n",
    "    mlp_layers=3,\n",
    "    act=nn.GELU(),\n",
    "    horiz_fourier_dim=0,\n",
    "    space_dim=2,\n",
    "    n_experts=1,\n",
    "    n_inner=4,\n",
    "    )\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and result in ./checkpoints/heat2d_all_GNOT_0423_19_00_55.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"heat2d\"  + '_{}_'.format(\"all\") + model.__name__  + time.strftime('_%m%d_%H_%M_%S')\n",
    "model_path, result_path = path_prefix + '.pt', path_prefix + '.pkl'\n",
    "\n",
    "print(f\"Saving model and result in ./checkpoints/{model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_stdout = sys.stdout\n",
    "writer_path =  './logs/' + path_prefix\n",
    "log_path = writer_path + '/params.txt'\n",
    "writer = SummaryWriter(log_dir=writer_path)\n",
    "fp = open(log_path, \"w+\")\n",
    "sys.stdout = fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "base_lr = 0.0000001  \n",
    "max_lr = 0.001\n",
    "warmup_epochs = 5\n",
    "total_epochs = 100\n",
    "steps_per_epoch = len(train_loader)\n",
    "warmup_steps = warmup_epochs * steps_per_epoch\n",
    "total_steps = total_epochs * steps_per_epoch\n",
    "total_cycles = 1\n",
    "\n",
    "class CombinedScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, base_lr, max_lr, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        super(CombinedScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Warmup phase: increase learning rate logarithmically\n",
    "            progress = self.last_epoch / self.warmup_steps\n",
    "            lr = torch.log10(torch.tensor(self.base_lr)) + progress * (torch.log10(torch.tensor(self.max_lr)) - torch.log10(torch.tensor(self.base_lr)))\n",
    "            lr = torch.pow(10, lr).item()\n",
    "        else:\n",
    "            # Cosine annealing phase\n",
    "            progress = (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = self.base_lr + (self.max_lr - self.base_lr) * 0.5 * (1 + torch.cos(torch.tensor(torch.pi * progress)))\n",
    "        return [lr for group in self.optimizer.param_groups]\n",
    "\n",
    "# 스케줄러 적용\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=5e-6)\n",
    "scheduler = CombinedScheduler(optimizer, warmup_steps, total_steps, base_lr, max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "############################\n",
    "\n",
    "patience=10\n",
    "grad_clip=0.999\n",
    "start_epoch: int = 0\n",
    "print_freq: int = 20\n",
    "model_save_path='./checkpoints/'\n",
    "save_mode='state_dict'  # 'state_dict' or 'entire'\n",
    "model_name='cgpt_model.pt'\n",
    "result_name='cgpt_result.pt'\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "loss_epoch = []\n",
    "lr_history = []\n",
    "it = 0\n",
    "\n",
    "result = None\n",
    "start_epoch = 0\n",
    "end_epoch = start_epoch + total_epochs\n",
    "best_val_metric = np.inf\n",
    "best_val_epoch = None\n",
    "stop_counter = 0\n",
    "\n",
    "def train_batch(model,\n",
    "                loss_func,\n",
    "                data,\n",
    "                optimizer,\n",
    "                device,\n",
    "                grad_clip):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    g, u_p, g_u = data\n",
    "    g = g.to(device)\n",
    "    u_p = u_p.to(device)\n",
    "    g_u = g_u.to(device)\n",
    "    \n",
    "    out = model(g, u_p, g_u)\n",
    "    y_pred, y = out.squeeze(), g.ndata['y'].squeeze()\n",
    "    loss, reg, _ = loss_func(g, y_pred, y)\n",
    "    loss = loss + reg\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    return (loss.item(), reg.item())\n",
    "    \n",
    "def validate_epoch(model, metric_func, valid_loader, device):\n",
    "    model.eval()\n",
    "    metric_val = []\n",
    "    for _, data in enumerate(valid_loader):\n",
    "        with torch.no_grad():\n",
    "            g, u_p, g_u = data\n",
    "            g, g_u, u_p = g.to(device), g_u.to(device), u_p.to(device)\n",
    "\n",
    "            out = model(g, u_p, g_u)\n",
    "\n",
    "            y_pred, y = out.squeeze(), g.ndata['y'].squeeze()\n",
    "            _, _, metric = metric_func(g, y_pred, y)\n",
    "\n",
    "            metric_val.append(metric)\n",
    "    return dict(metric=np.mean(metric_val, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 63/63 [00:13<00:00,  4.63batch/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_iterator = tqdm(train_loader, desc=f'Epoch {start_epoch+1}/{end_epoch}', unit='batch')\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    print(\"-----------------\")\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        loss = train_batch(model,\n",
    "                           loss_func,\n",
    "                           batch,\n",
    "                           optimizer,\n",
    "                           device,\n",
    "                           grad_clip,\n",
    "                           )\n",
    "        loss = np.array(loss)\n",
    "        loss_epoch.append(loss)\n",
    "        it += 1\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        lr_history.append(lr)\n",
    "        log = f\"Epoch: {epoch+1}/{end_epoch}\"\n",
    "        if loss.ndim == 0:\n",
    "            _loss_mean = np.mean(loss_epoch)\n",
    "            log += f\" | loss: {_loss_mean:.6f}\"\n",
    "        else:\n",
    "            _loss_mean = np.mean(loss_epoch, axis=0)\n",
    "            for j in range(len(_loss_mean)):\n",
    "                log += f\" | loss {j}: {_loss_mean[j]:.6f}\"\n",
    "        log += f\" | current lr: {lr:.3e}\"\n",
    "        \n",
    "        if it % print_freq == 0:\n",
    "            print(log)\n",
    "        \n",
    "        if writer is not None:\n",
    "            for j in range(len(_loss_mean)):\n",
    "                writer.add_scalar(f\"train_loss_{j}\", _loss_mean[j], it)\n",
    "            writer.add_scalar(\"learning_rate\", lr, it)\n",
    "        scheduler.step()\n",
    "        \n",
    "    loss_train.append(_loss_mean)\n",
    "    loss_epoch = []\n",
    "\n",
    "    val_result = validate_epoch(model, metric_func, test_loader, device)\n",
    "\n",
    "    loss_val.append(val_result[\"metric\"])\n",
    "    val_metric = val_result[\"metric\"].sum()\n",
    "\n",
    "\n",
    "    if val_metric < best_val_metric:\n",
    "        best_val_epoch = epoch\n",
    "        best_val_metric = val_metric\n",
    "\n",
    "\n",
    "\n",
    "    if val_result[\"metric\"].size == 1:\n",
    "        log = \"| val metric 0: {:.6f} \".format(val_metric)\n",
    "\n",
    "    else:\n",
    "        log = ''\n",
    "        for i, metric_i in enumerate(val_result['metric']):\n",
    "            log += '| val metric {} : {:.6f} '.format(i, metric_i)\n",
    "\n",
    "    if writer is not None:\n",
    "        if val_result[\"metric\"].size == 1:\n",
    "            writer.add_scalar('val loss {}'.format(metric_func.component),val_metric, epoch)\n",
    "        else:\n",
    "            for i, metric_i in enumerate(val_result['metric']):\n",
    "                writer.add_scalar('val loss {}'.format(i), metric_i, epoch)\n",
    "\n",
    "    \n",
    "    log += f\"| best val: {best_val_metric:.6f} at epoch {best_val_epoch+1} | current lr: {lr:.3e}\"\n",
    "    \n",
    "    desc_ep = \"\"\n",
    "    if _loss_mean.ndim == 0:  # 1 target loss\n",
    "        desc_ep += f\"| loss: {_loss_mean:.6f}\"\n",
    "    else:\n",
    "        for j in range(len(_loss_mean)):\n",
    "            if _loss_mean[j] > 0:\n",
    "                desc_ep += \"| loss {}: {:.3e}\".format(j, _loss_mean[j])\n",
    "    \n",
    "    desc_ep += log\n",
    "    print(desc_ep)\n",
    "    \n",
    "    result = dict(\n",
    "        best_val_epoch=best_val_epoch,\n",
    "        best_val_metric=best_val_metric,\n",
    "        loss_train=np.asarray(loss_train),\n",
    "        loss_val=np.asarray(loss_val),\n",
    "        lr_history=np.asarray(lr_history),\n",
    "        # best_model=best_model_state_dict,\n",
    "        optimizer_state=optimizer.state_dict()\n",
    "    )\n",
    "    \n",
    "    pickle.dump(result, open(os.path.join(model_save_path, result_name),'wb'))\n",
    "############################\n",
    "execute_time = time.time() - time_start\n",
    "print(f'Training takes {execute_time} seconds.')\n",
    "\n",
    "checkpoint = {'model':model.state_dict(),'optimizer':optimizer.state_dict()}\n",
    "torch.save(checkpoint, os.path.join('./checkpoints/{}'.format(model_path)))\n",
    "model.eval()\n",
    "val_metric = validate_epoch(model, metric_func, test_loader, device)\n",
    "print(f\"\\nBest model's validation metric in this run: {val_metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#### test single case\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     g, u_p, g_u \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))[idx]\n\u001b[0;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# u_p = u_p.unsqueeze(0)      ### test if necessary\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #### test single case\n",
    "    idx = 7\n",
    "    g, u_p, g_u =  list(iter(test_loader))[idx]\n",
    "    model = model.to(\"cpu\")\n",
    "    # u_p = u_p.unsqueeze(0)      ### test if necessary\n",
    "    out = model(g, u_p, g_u)\n",
    "\n",
    "    x, y = g.ndata['x'][:,0].cpu().numpy(), g.ndata['x'][:,1].cpu().numpy()\n",
    "    pred = out[:,0].squeeze().cpu().numpy()\n",
    "    target =g.ndata['y'][:,0].squeeze().cpu().numpy()\n",
    "    err = pred - target\n",
    "    print(pred)\n",
    "    print(target)\n",
    "    print(err)\n",
    "    print(np.linalg.norm(err)/np.linalg.norm(target))\n",
    "    \n",
    "    #### choose one to visualize\n",
    "    cm = plt.cm.get_cmap('rainbow')\n",
    "\n",
    "    plot_heatmap(x, y, pred,cmap=cm,show=True)\n",
    "    plot_heatmap(x, y, target,cmap=cm,show=True)\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, c=pred, cmap=cm,s=2)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, c=err, cmap=cm,s=2)\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "    plt.scatter(x, y, c=target, s=2,cmap=cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
